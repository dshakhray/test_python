From 9a8b617d3a8bd6cc29077bafe2e09616a6261c62 Mon Sep 17 00:00:00 2001
From: Herton R. Krzesinski <herton@redhat.com>
Date: Wed, 18 May 2016 06:41:07 +0200
Subject: [kernel] sched/fair: Fix unlocked reads of some cfs_b->quota/period

Message-id: <1463553686-8136-7-git-send-email-herton@redhat.com>
Patchwork-id: 145571
O-Subject: [RHEL7 PATCH 06/25] sched/fair: Fix unlocked reads of some cfs_b->quota/period
Bugzilla: 1336863
Z-Bugzilla: 1370157
RH-Acked-by: Jiri Olsa <jolsa@redhat.com>
RH-Acked-by: Rafael Aquini <aquini@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1336863
Upstream Status: commit 51f2176
Build Info: https://brewweb.engineering.redhat.com/brew/taskinfo?taskID=11035443
Tested: kt1 (looking for consistent regressions) and with test case on the bug

commit 51f2176d74ace4c3f58579a605ef5a9720befb00
Author: Ben Segall <bsegall@google.com>
Date:   Mon May 19 15:49:45 2014 -0700

    sched/fair: Fix unlocked reads of some cfs_b->quota/period

    sched_cfs_period_timer() reads cfs_b->period without locks before calling
    do_sched_cfs_period_timer(), and similarly unthrottle_offline_cfs_rqs()
    would read cfs_b->period without the right lock. Thus a simultaneous
    change of bandwidth could cause corruption on any platform where ktime_t
    or u64 writes/reads are not atomic.

    Extend cfs_b->lock from do_sched_cfs_period_timer() to include the read of
    cfs_b->period to solve that issue; unthrottle_offline_cfs_rqs() can just
    use 1 rather than the exact quota, much like distribute_cfs_runtime()
    does.

    There is also an unlocked read of cfs_b->runtime_expires, but a race
    there would only delay runtime expiry by a tick. Still, the comparison
    should just be != anyway, which clarifies even that problem.

    Signed-off-by: Ben Segall <bsegall@google.com>
    Tested-by: Roman Gushchin <klamm@yandex-team.ru>
    [peterz: Fix compile warn]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140519224945.20303.93530.stgit@sword-of-the-dawn.mtv.corp.google.com
    Cc: pjt@google.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

Signed-off-by: Herton R. Krzesinski <herton@redhat.com>
Signed-off-by: Alexander Gordeev <agordeev@redhat.com>

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index d9ef561..f335a06 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -3146,10 +3146,12 @@ static void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)
   * has not truly expired.
   *
   * Fortunately we can check determine whether this the case by checking
-  * whether the global deadline has advanced.
+  * whether the global deadline has advanced. It is valid to compare
+  * cfs_b->runtime_expires without any locks since we only care about
+  * exact equality, so a partial write will still work.
   */
 
- if ((s64)(cfs_rq->runtime_expires - cfs_b->runtime_expires) >= 0) {
+ if (cfs_rq->runtime_expires != cfs_b->runtime_expires) {
   /* extend local deadline, drift is bounded above by 2 ticks */
   cfs_rq->runtime_expires += TICK_NSEC;
  } else {
@@ -3376,21 +3378,21 @@ next:
 static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)
 {
  u64 runtime, runtime_expires;
- int idle = 1, throttled;
+ int throttled;
 
- raw_spin_lock(&cfs_b->lock);
  /* no need to continue the timer with no bandwidth constraint */
  if (cfs_b->quota == RUNTIME_INF)
-  goto out_unlock;
+  goto out_deactivate;
 
  throttled = !list_empty(&cfs_b->throttled_cfs_rq);
- /* idle depends on !throttled (for the case of a large deficit) */
- idle = cfs_b->idle && !throttled;
  cfs_b->nr_periods += overrun;
 
- /* if we're going inactive then everything else can be deferred */
- if (idle)
-  goto out_unlock;
+ /*
+  * idle depends on !throttled (for the case of a large deficit), and if
+  * we're going inactive then everything else can be deferred
+  */
+ if (cfs_b->idle && !throttled)
+  goto out_deactivate;
 
  /*
   * if we have relooped after returning idle once, we need to update our
@@ -3404,7 +3406,7 @@ static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)
  if (!throttled) {
   /* mark as potentially idle for the upcoming period */
   cfs_b->idle = 1;
-  goto out_unlock;
+  return 0;
  }
 
  /* account preceding periods in which throttling occurred */
@@ -3444,12 +3446,12 @@ static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)
   * timer to remain active while there are any throttled entities.)
   */
  cfs_b->idle = 0;
-out_unlock:
- if (idle)
-  cfs_b->timer_active = 0;
- raw_spin_unlock(&cfs_b->lock);
 
- return idle;
+ return 0;
+
+out_deactivate:
+ cfs_b->timer_active = 0;
+ return 1;
 }
 
 /* a cfs_rq won't donate quota below this amount */
@@ -3646,6 +3648,7 @@ static enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)
  int overrun;
  int idle = 0;
 
+ raw_spin_lock(&cfs_b->lock);
  for (;;) {
   now = hrtimer_cb_get_time(timer);
   overrun = hrtimer_forward(timer, now, cfs_b->period);
@@ -3655,6 +3658,7 @@ static enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)
 
   idle = do_sched_cfs_period_timer(cfs_b, overrun);
  }
+ raw_spin_unlock(&cfs_b->lock);
 
  return idle ? HRTIMER_NORESTART : HRTIMER_RESTART;
 }
@@ -3714,8 +3718,6 @@ static void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)
  struct cfs_rq *cfs_rq;
 
  for_each_leaf_cfs_rq(rq, cfs_rq) {
-  struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);
-
   if (!cfs_rq->runtime_enabled)
    continue;
 
@@ -3723,7 +3725,7 @@ static void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)
    * clock_task is not advancing so we just need to make sure
    * there's some valid quota amount
    */
-  cfs_rq->runtime_remaining = cfs_b->quota;
+  cfs_rq->runtime_remaining = 1;
   if (cfs_rq_throttled(cfs_rq))
    unthrottle_cfs_rq(cfs_rq);
  }
-- 
1.7.1