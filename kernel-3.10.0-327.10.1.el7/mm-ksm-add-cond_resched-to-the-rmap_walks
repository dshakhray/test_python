From 6b49a6440f0288ac3a2de660329f54a1b6a79adf Mon Sep 17 00:00:00 2001
From: Andrea Arcangeli <aarcange@redhat.com>
Date: Mon, 2 Nov 2015 17:24:50 +0100
Subject: [mm] ksm: add cond_resched() to the rmap_walks

Message-id: <1446485094-9312-2-git-send-email-aarcange@redhat.com>
Patchwork-id: 126116
O-Subject: [RHEL7.2 PATCH 1/5] mm: add cond_resched() to the rmap walks
Bugzilla: 1274871
Z-Bugzilla: 1281422
RH-Acked-by: Andrew Jones <drjones@redhat.com>
RH-Acked-by: Petr Holasek <pholasek@redhat.com>

The rmap walk must reach every possible mapping of the page, so if a
page is heavily shared (no matter if it's KSM, anon, pagecache) there
will be tons of entries to walk through. All optimizations with
prio_tree, anon_vma chains, interval tree, helps to find the right
virtual mapping faster, but if there are lots of virtual mappings, all
mapping must still be walked through.

The biggest cost is for the IPIs, but regardless of the IPIs, it's
generally safer to keep these cond_resched() in all cases, as even if
we massively reduce the number of IPIs, the number of entries to walk
IPI-less may still be large and no entry can be possibly skipped in
the page migration case.

Acked-by: Hugh Dickins <hughd@google.com>
Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Signed-off-by: Alexander Gordeev <agordeev@redhat.com>

diff --git a/mm/ksm.c b/mm/ksm.c
index 7ee5b1c..5a1c194 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -1917,9 +1917,11 @@ again:
   struct anon_vma_chain *vmac;
   struct vm_area_struct *vma;
 
+  cond_resched();
   anon_vma_lock_read(anon_vma);
   anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
             0, ULONG_MAX) {
+   cond_resched();
    vma = vmac->vma;
    if (rmap_item->address < vma->vm_start ||
        rmap_item->address >= vma->vm_end)
-- 
1.7.1