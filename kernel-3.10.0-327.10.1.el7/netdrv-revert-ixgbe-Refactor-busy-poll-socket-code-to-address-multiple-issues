From 228c7adbceeef83f23b5b90eb7eb4cda13f6e2f3 Mon Sep 17 00:00:00 2001
From: John Greene <jogreene@redhat.com>
Date: Fri, 18 Sep 2015 17:51:59 -0400
Subject: [netdrv] revert "ixgbe: Refactor busy poll socket code to address multiple issues"

Message-id: <1442598719-11454-1-git-send-email-jogreene@redhat.com>
Patchwork-id: 124963
O-Subject: [RHEL7.2 bz1261275] Revert "[netdrv] ixgbe: Refactor busy poll socket code to address multiple issues"
Bugzilla: 1261275
RH-Acked-by: Alexander Duyck <alexander.h.duyck@redhat.com>
RH-Acked-by: Stefan Assmann <sassmann@redhat.com>
RH-Acked-by: John Linville <linville@redhat.com>

BZ: 1261275, 1261109
Brew: 9850658
Tested: by QA, RT team

Reverts commit 695045c1f452c027012588be2ab72fc99ef9299d to address QA
observed rx performance regression in BZ1261275 when using fq_codel
scheduling and hard lockup downloading a file in RT kernel.

Test kvm performance in the guest and host is using ixgbe driver.
Patch intoduces around 10% rx performance drop.

The same patch causes realtime splats in file downloads.  Apparently,
we need spinlocks rather atomics the original patch adds.

BZ1261109  kernel 3.10.0-314.rt56.188.el7.x86_64 cause host call trace
when use wget tool to download file

In summary, we get a host hardlock that produces the backtrace at bottom
of this email. I've bisected the issue and git bisect points to a merge
commit in the RT kernel bringing in the ixgbe driver update that happened
in the -309 kernel. We're very suspicious of this change:

commit 695045c1f452c027012588be2ab72fc99ef9299d
Author: John Greene <jogreene@redhat.com>
Date:   Thu Aug 20 19:56:15 2015 -0400

      [netdrv] ixgbe: Refactor busy poll socket code to address multiple issues

As it replaces a spin_lock by a atomic_ops. However, that spin_lock had
special case handling in the RT kernel (which was dropped in the merge)
and the RT kernel uses irq handlers as threads. So, I suspect the spin_lock
was preventing race conditions the atomic_ops doesn't.

Signed-off-by: John Greene <jogreene@redhat.com>
Signed-off-by: Rafael Aquini <aquini@redhat.com>

diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe.h b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
index 9a2f881..86d789a 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe.h
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
@@ -379,87 +379,119 @@ struct ixgbe_q_vector {
  char name[IFNAMSIZ + 9];
 
 #ifdef CONFIG_NET_RX_BUSY_POLL
- atomic_t state;
+ unsigned int state;
+#define IXGBE_QV_STATE_IDLE        0
+#define IXGBE_QV_STATE_NAPI    1     /* NAPI owns this QV */
+#define IXGBE_QV_STATE_POLL    2     /* poll owns this QV */
+#define IXGBE_QV_STATE_DISABLED    4     /* QV is disabled */
+#define IXGBE_QV_OWNED (IXGBE_QV_STATE_NAPI | IXGBE_QV_STATE_POLL)
+#define IXGBE_QV_LOCKED (IXGBE_QV_OWNED | IXGBE_QV_STATE_DISABLED)
+#define IXGBE_QV_STATE_NAPI_YIELD  8     /* NAPI yielded this QV */
+#define IXGBE_QV_STATE_POLL_YIELD  16    /* poll yielded this QV */
+#define IXGBE_QV_YIELD (IXGBE_QV_STATE_NAPI_YIELD | IXGBE_QV_STATE_POLL_YIELD)
+#define IXGBE_QV_USER_PEND (IXGBE_QV_STATE_POLL | IXGBE_QV_STATE_POLL_YIELD)
+ spinlock_t lock;
 #endif  /* CONFIG_NET_RX_BUSY_POLL */
 
  /* for dynamic allocation of rings associated with this q_vector */
  struct ixgbe_ring ring[0] ____cacheline_internodealigned_in_smp;
 };
-
 #ifdef CONFIG_NET_RX_BUSY_POLL
-enum ixgbe_qv_state_t {
- IXGBE_QV_STATE_IDLE = 0,
- IXGBE_QV_STATE_NAPI,
- IXGBE_QV_STATE_POLL,
- IXGBE_QV_STATE_DISABLE
-};
-
 static inline void ixgbe_qv_init_lock(struct ixgbe_q_vector *q_vector)
 {
- /* reset state to idle */
- atomic_set(&q_vector->state, IXGBE_QV_STATE_IDLE);
+
+ spin_lock_init(&q_vector->lock);
+ q_vector->state = IXGBE_QV_STATE_IDLE;
 }
 
 /* called from the device poll routine to get ownership of a q_vector */
 static inline bool ixgbe_qv_lock_napi(struct ixgbe_q_vector *q_vector)
 {
- int rc = atomic_cmpxchg(&q_vector->state, IXGBE_QV_STATE_IDLE,
-    IXGBE_QV_STATE_NAPI);
+ int rc = true;
+ spin_lock_bh(&q_vector->lock);
+ if (q_vector->state & IXGBE_QV_LOCKED) {
+  WARN_ON(q_vector->state & IXGBE_QV_STATE_NAPI);
+  q_vector->state |= IXGBE_QV_STATE_NAPI_YIELD;
+  rc = false;
 #ifdef BP_EXTENDED_STATS
- if (rc != IXGBE_QV_STATE_IDLE)
   q_vector->tx.ring->stats.yields++;
 #endif
-
- return rc == IXGBE_QV_STATE_IDLE;
+ } else {
+  /* we don't care if someone yielded */
+  q_vector->state = IXGBE_QV_STATE_NAPI;
+ }
+ spin_unlock_bh(&q_vector->lock);
+ return rc;
 }
 
 /* returns true is someone tried to get the qv while napi had it */
-static inline void ixgbe_qv_unlock_napi(struct ixgbe_q_vector *q_vector)
+static inline bool ixgbe_qv_unlock_napi(struct ixgbe_q_vector *q_vector)
 {
- WARN_ON(atomic_read(&q_vector->state) != IXGBE_QV_STATE_NAPI);
-
- /* flush any outstanding Rx frames */
- if (q_vector->napi.gro_list)
-  napi_gro_flush(&q_vector->napi, false);
-
- /* reset state to idle */
- atomic_set(&q_vector->state, IXGBE_QV_STATE_IDLE);
+ int rc = false;
+ spin_lock_bh(&q_vector->lock);
+ WARN_ON(q_vector->state & (IXGBE_QV_STATE_POLL |
+          IXGBE_QV_STATE_NAPI_YIELD));
+
+ if (q_vector->state & IXGBE_QV_STATE_POLL_YIELD)
+  rc = true;
+ /* will reset state to idle, unless QV is disabled */
+ q_vector->state &= IXGBE_QV_STATE_DISABLED;
+ spin_unlock_bh(&q_vector->lock);
+ return rc;
 }
 
 /* called from ixgbe_low_latency_poll() */
 static inline bool ixgbe_qv_lock_poll(struct ixgbe_q_vector *q_vector)
 {
- int rc = atomic_cmpxchg(&q_vector->state, IXGBE_QV_STATE_IDLE,
-    IXGBE_QV_STATE_POLL);
+ int rc = true;
+ spin_lock_bh(&q_vector->lock);
+ if ((q_vector->state & IXGBE_QV_LOCKED)) {
+  q_vector->state |= IXGBE_QV_STATE_POLL_YIELD;
+  rc = false;
 #ifdef BP_EXTENDED_STATS
- if (rc != IXGBE_QV_STATE_IDLE)
-  q_vector->tx.ring->stats.yields++;
+  q_vector->rx.ring->stats.yields++;
 #endif
- return rc == IXGBE_QV_STATE_IDLE;
+ } else {
+  /* preserve yield marks */
+  q_vector->state |= IXGBE_QV_STATE_POLL;
+ }
+ spin_unlock_bh(&q_vector->lock);
+ return rc;
 }
 
 /* returns true if someone tried to get the qv while it was locked */
-static inline void ixgbe_qv_unlock_poll(struct ixgbe_q_vector *q_vector)
+static inline bool ixgbe_qv_unlock_poll(struct ixgbe_q_vector *q_vector)
 {
- WARN_ON(atomic_read(&q_vector->state) != IXGBE_QV_STATE_POLL);
-
- /* reset state to idle */
- atomic_set(&q_vector->state, IXGBE_QV_STATE_IDLE);
+ int rc = false;
+ spin_lock_bh(&q_vector->lock);
+ WARN_ON(q_vector->state & (IXGBE_QV_STATE_NAPI));
+
+ if (q_vector->state & IXGBE_QV_STATE_POLL_YIELD)
+  rc = true;
+ /* will reset state to idle, unless QV is disabled */
+ q_vector->state &= IXGBE_QV_STATE_DISABLED;
+ spin_unlock_bh(&q_vector->lock);
+ return rc;
 }
 
 /* true if a socket is polling, even if it did not get the lock */
 static inline bool ixgbe_qv_busy_polling(struct ixgbe_q_vector *q_vector)
 {
- return atomic_read(&q_vector->state) == IXGBE_QV_STATE_POLL;
+ WARN_ON(!(q_vector->state & IXGBE_QV_OWNED));
+ return q_vector->state & IXGBE_QV_USER_PEND;
 }
 
 /* false if QV is currently owned */
 static inline bool ixgbe_qv_disable(struct ixgbe_q_vector *q_vector)
 {
- int rc = atomic_cmpxchg(&q_vector->state, IXGBE_QV_STATE_IDLE,
-    IXGBE_QV_STATE_DISABLE);
-
- return rc == IXGBE_QV_STATE_IDLE;
+ int rc = true;
+ spin_lock_bh(&q_vector->lock);
+ if (q_vector->state & IXGBE_QV_OWNED)
+  rc = false;
+ q_vector->state |= IXGBE_QV_STATE_DISABLED;
+ spin_unlock_bh(&q_vector->lock);
+
+ return rc;
 }
 
 #else /* CONFIG_NET_RX_BUSY_POLL */
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_lib.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_lib.c
index 1be0293..14a2895 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_lib.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_lib.c
@@ -845,11 +845,6 @@ static int ixgbe_alloc_q_vector(struct ixgbe_adapter *adapter,
          ixgbe_poll, 64);
  napi_hash_add(&q_vector->napi);
 
-#ifdef CONFIG_NET_RX_BUSY_POLL
- /* initialize busy poll */
- atomic_set(&q_vector->state, IXGBE_QV_STATE_DISABLE);
-
-#endif
  /* tie q_vector and adapter together */
  adapter->q_vector[v_idx] = q_vector;
  q_vector->adapter = adapter;
-- 
1.7.1