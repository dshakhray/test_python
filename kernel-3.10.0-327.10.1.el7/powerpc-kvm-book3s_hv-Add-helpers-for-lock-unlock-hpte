From 00aaa0b851e61e0adf0db8bb8825921fd885e30f Mon Sep 17 00:00:00 2001
From: David Gibson <dgibson@redhat.com>
Date: Tue, 27 Oct 2015 04:01:26 +0100
Subject: [powerpc] kvm: book3s_hv: Add helpers for lock/unlock hpte

Message-id: <1445918487-8660-2-git-send-email-dgibson@redhat.com>
Patchwork-id: 125908
O-Subject: [RHEL-7.3 kernel PATCH 1/2] KVM: PPC: Book3S HV: Add helpers for lock/unlock hpte
Bugzilla: 1273692
Z-Bugzilla: 1281420
RH-Acked-by: Laurent Vivier <lvivier@redhat.com>
RH-Acked-by: Thomas Huth <thuth@redhat.com>
RH-Acked-by: Don Zickus <dzickus@redhat.com>

This adds helper routines for locking and unlocking HPTEs, and uses
them in the rest of the code.  We don't change any locking rules in
this patch.

Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Signed-off-by: Paul Mackerras <paulus@samba.org>
Signed-off-by: Alexander Graf <agraf@suse.de>
(cherry picked from commit a4bd6eb07ca72d21a7a34499ad34cfef6f527d4e)

Conflicts:
 arch/powerpc/kvm/book3s_hv_rm_mmu.c

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1273692

Imported as prerequisite.

Signed-off-by: David Gibson <dgibson@redhat.com>
Signed-off-by: Alexander Gordeev <agordeev@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_book3s_64.h b/arch/powerpc/include/asm/kvm_book3s_64.h
index 73896e8..3b56c4c 100644
--- a/arch/powerpc/include/asm/kvm_book3s_64.h
+++ b/arch/powerpc/include/asm/kvm_book3s_64.h
@@ -86,6 +86,20 @@ static inline long try_lock_hpte(__be64 *hpte, unsigned long bits)
  return old == 0;
 }
 
+static inline void unlock_hpte(__be64 *hpte, unsigned long hpte_v)
+{
+ hpte_v &= ~HPTE_V_HVLOCK;
+ asm volatile(PPC_RELEASE_BARRIER "" : : : "memory");
+ hpte[0] = cpu_to_be64(hpte_v);
+}
+
+/* Without barrier */
+static inline void __unlock_hpte(__be64 *hpte, unsigned long hpte_v)
+{
+ hpte_v &= ~HPTE_V_HVLOCK;
+ hpte[0] = cpu_to_be64(hpte_v);
+}
+
 static inline int __hpte_actual_psize(unsigned int lp, int psize)
 {
  int i, shift;
diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c
index 8afd9d6..e0fcd8f 100644
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@ -478,9 +478,7 @@ static int kvmppc_mmu_book3s_64_hv_xlate(struct kvm_vcpu *vcpu, gva_t eaddr,
  v = be64_to_cpu(hptep[0]) & ~HPTE_V_HVLOCK;
  gr = kvm->arch.revmap[index].guest_rpte;
 
- /* Unlock the HPTE */
- asm volatile("lwsync" : : : "memory");
- hptep[0] = cpu_to_be64(v);
+ unlock_hpte(hptep, v);
  preempt_enable();
 
  gpte->eaddr = eaddr;
@@ -609,8 +607,7 @@ int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,
  hpte[0] = be64_to_cpu(hptep[0]) & ~HPTE_V_HVLOCK;
  hpte[1] = be64_to_cpu(hptep[1]);
  hpte[2] = r = rev->guest_rpte;
- asm volatile("lwsync" : : : "memory");
- hptep[0] = cpu_to_be64(hpte[0]);
+ unlock_hpte(hptep, hpte[0]);
  preempt_enable();
 
  if (hpte[0] != vcpu->arch.pgfault_hpte[0] ||
@@ -762,7 +759,7 @@ int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
  hptep[1] = cpu_to_be64(r);
  eieio();
- hptep[0] = cpu_to_be64(hpte[0]);
+ __unlock_hpte(hptep, hpte[0]);
  asm volatile("ptesync" : : : "memory");
  preempt_enable();
  if (page && hpte_is_writable(r))
@@ -783,7 +780,7 @@ int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,
  return ret;
 
  out_unlock:
- hptep[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);
+ __unlock_hpte(hptep, be64_to_cpu(hptep[0]));
  preempt_enable();
  goto out_put;
 }
@@ -913,7 +910,7 @@ static int kvm_unmap_rmapp(struct kvm *kvm, unsigned long *rmapp,
    }
   }
   unlock_rmap(rmapp);
-  hptep[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);
+  __unlock_hpte(hptep, be64_to_cpu(hptep[0]));
  }
  return 0;
 }
@@ -1001,7 +998,7 @@ static int kvm_age_rmapp(struct kvm *kvm, unsigned long *rmapp,
    }
    ret = 1;
   }
-  hptep[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);
+  __unlock_hpte(hptep, be64_to_cpu(hptep[0]));
  } while ((i = j) != head);
 
  unlock_rmap(rmapp);
@@ -1124,8 +1121,7 @@ static int kvm_test_clear_dirty_npages(struct kvm *kvm, unsigned long *rmapp)
 
   /* Now check and modify the HPTE */
   if (!(hptep[0] & cpu_to_be64(HPTE_V_VALID))) {
-   /* unlock and continue */
-   hptep[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);
+   __unlock_hpte(hptep, be64_to_cpu(hptep[0]));
    continue;
   }
 
@@ -1146,9 +1142,9 @@ static int kvm_test_clear_dirty_npages(struct kvm *kvm, unsigned long *rmapp)
     npages_dirty = n;
    eieio();
   }
-  v &= ~(HPTE_V_ABSENT | HPTE_V_HVLOCK);
+  v &= ~HPTE_V_ABSENT;
   v |= HPTE_V_VALID;
-  hptep[0] = cpu_to_be64(v);
+  __unlock_hpte(hptep, v);
  } while ((i = j) != head);
 
  unlock_rmap(rmapp);
@@ -1386,8 +1382,7 @@ static long record_hpte(unsigned long flags, __be64 *hptp,
    r &= ~HPTE_GR_MODIFIED;
    revp->guest_rpte = r;
   }
-  asm volatile(PPC_RELEASE_BARRIER "" : : : "memory");
-  hptp[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);
+  unlock_hpte(hptp, be64_to_cpu(hptp[0]));
   preempt_enable();
   if (!(valid == want_valid && (first_pass || dirty)))
    ok = 0;
diff --git a/arch/powerpc/kvm/book3s_hv_rm_mmu.c b/arch/powerpc/kvm/book3s_hv_rm_mmu.c
index a2ab277..e6848ba 100644
--- a/arch/powerpc/kvm/book3s_hv_rm_mmu.c
+++ b/arch/powerpc/kvm/book3s_hv_rm_mmu.c
@@ -138,12 +138,6 @@ static void remove_revmap_chain(struct kvm *kvm, long pte_index,
  unlock_rmap(rmap);
 }
 
-static inline void unlock_hpte(__be64 *hpte, unsigned long hpte_v)
-{
- asm volatile(PPC_RELEASE_BARRIER "" : : : "memory");
- hpte[0] = cpu_to_be64(hpte_v);
-}
-
 long kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,
          long pte_index, unsigned long pteh, unsigned long ptel,
          pgd_t *pgdir, bool realmode, unsigned long *pte_idx_ret)
@@ -308,10 +302,10 @@ long kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,
     u64 pte;
     while (!try_lock_hpte(hpte, HPTE_V_HVLOCK))
      cpu_relax();
-    pte = be64_to_cpu(*hpte);
+    pte = be64_to_cpu(hpte[0]);
     if (!(pte & (HPTE_V_VALID | HPTE_V_ABSENT)))
      break;
-    *hpte &= ~cpu_to_be64(HPTE_V_HVLOCK);
+    __unlock_hpte(hpte, pte);
     hpte += 2;
    }
    if (i == 8)
@@ -327,9 +321,9 @@ long kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,
 
    while (!try_lock_hpte(hpte, HPTE_V_HVLOCK))
     cpu_relax();
-   pte = be64_to_cpu(*hpte);
+   pte = be64_to_cpu(hpte[0]);
    if (pte & (HPTE_V_VALID | HPTE_V_ABSENT)) {
-    *hpte &= ~cpu_to_be64(HPTE_V_HVLOCK);
+    __unlock_hpte(hpte, pte);
     return H_PTEG_FULL;
    }
   }
@@ -369,7 +363,7 @@ long kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,
 
  /* Write the first HPTE dword, unlocking the HPTE and making it valid */
  eieio();
- hpte[0] = cpu_to_be64(pteh);
+ __unlock_hpte(hpte, pteh);
  asm volatile("ptesync" : : : "memory");
 
  *pte_idx_ret = pte_index;
@@ -500,7 +494,7 @@ long kvmppc_do_h_remove(struct kvm *kvm, unsigned long flags,
  if ((pte & (HPTE_V_ABSENT | HPTE_V_VALID)) == 0 ||
      ((flags & H_AVPN) && (pte & ~0x7fUL) != avpn) ||
      ((flags & H_ANDCOND) && (pte & avpn) != 0)) {
-  hpte[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);
+  __unlock_hpte(hpte, pte);
   return H_NOT_FOUND;
  }
 
@@ -636,7 +630,7 @@ long kvmppc_h_bulk_remove(struct kvm_vcpu *vcpu)
     be64_to_cpu(hp[0]), be64_to_cpu(hp[1]));
    rcbits = rev->guest_rpte & (HPTE_R_R|HPTE_R_C);
    args[j] |= rcbits << (56 - 5);
-   hp[0] = 0;
+   __unlock_hpte(hp, 0);
   }
  }
 
@@ -662,7 +656,7 @@ long kvmppc_h_protect(struct kvm_vcpu *vcpu, unsigned long flags,
  pte = be64_to_cpu(hpte[0]);
  if ((pte & (HPTE_V_ABSENT | HPTE_V_VALID)) == 0 ||
      ((flags & H_AVPN) && (pte & ~0x7fUL) != avpn)) {
-  hpte[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);
+  __unlock_hpte(hpte, pte);
   return H_NOT_FOUND;
  }
 
@@ -844,8 +838,7 @@ long kvmppc_hv_find_lock_hpte(struct kvm *kvm, gva_t eaddr, unsigned long slb_v,
     /* Return with the HPTE still locked */
     return (hash << 3) + (i >> 1);
 
-   /* Unlock and move on */
-   hpte[i] = cpu_to_be64(v);
+   __unlock_hpte(&hpte[i], v);
   }
 
   if (val & HPTE_V_SECONDARY)
-- 
1.7.1