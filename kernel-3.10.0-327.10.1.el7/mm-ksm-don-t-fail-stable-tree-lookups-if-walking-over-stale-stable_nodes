From c5614ecfbe71743a76ca08bcb93f3fb334f51726 Mon Sep 17 00:00:00 2001
From: Andrea Arcangeli <aarcange@redhat.com>
Date: Mon, 2 Nov 2015 17:24:51 +0100
Subject: [mm] ksm: don't fail stable tree lookups if walking over stale stable_nodes

Message-id: <1446485094-9312-3-git-send-email-aarcange@redhat.com>
Patchwork-id: 126114
O-Subject: [RHEL7.2 PATCH 2/5] ksm: don't fail stable tree lookups if walking over stale stable_nodes
Bugzilla: 1274871
Z-Bugzilla: 1281422
RH-Acked-by: Andrew Jones <drjones@redhat.com>
RH-Acked-by: Petr Holasek <pholasek@redhat.com>

The stable_nodes can become stale at any time if the underlying pages
gets freed. The stable_node gets collected and removed from the stable
rbtree if that is detected during the rbtree lookups.

Don't fail the lookup if running into stale stable_nodes, just restart
the lookup after collecting the stale stable_nodes. Otherwise the CPU
spent in the preparation stage is wasted and the lookup must be
repeated at the next loop potentially failing a second time in a
second stale stable_node.

If we don't prune aggressively we delay the merging of the unstable
node candidates and at the same time we delay the freeing of the stale
stable_nodes. Keeping stale stable_nodes around wastes memory and it
can't provide any benefit.

Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Acked-by: Hugh Dickins <hughd@google.com>
Signed-off-by: Alexander Gordeev <agordeev@redhat.com>

diff --git a/mm/ksm.c b/mm/ksm.c
index 5a1c194..4e147fc 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -1178,8 +1178,18 @@ again:
   cond_resched();
   stable_node = rb_entry(*new, struct stable_node, node);
   tree_page = get_ksm_page(stable_node, false);
-  if (!tree_page)
-   return NULL;
+  if (!tree_page) {
+   /*
+    * If we walked over a stale stable_node,
+    * get_ksm_page() will call rb_erase() and it
+    * may rebalance the tree from under us. So
+    * restart the search from scratch. Returning
+    * NULL would be safe too, but we'd generate
+    * false negative insertions just because some
+    * stable_node was stale.
+    */
+   goto again;
+  }
 
   ret = memcmp_pages(page, tree_page);
   put_page(tree_page);
@@ -1255,12 +1265,14 @@ static struct stable_node *stable_tree_insert(struct page *kpage)
  unsigned long kpfn;
  struct rb_root *root;
  struct rb_node **new;
- struct rb_node *parent = NULL;
+ struct rb_node *parent;
  struct stable_node *stable_node;
 
  kpfn = page_to_pfn(kpage);
  nid = get_kpfn_nid(kpfn);
  root = root_stable_tree + nid;
+again:
+ parent = NULL;
  new = &root->rb_node;
 
  while (*new) {
@@ -1270,8 +1282,18 @@ static struct stable_node *stable_tree_insert(struct page *kpage)
   cond_resched();
   stable_node = rb_entry(*new, struct stable_node, node);
   tree_page = get_ksm_page(stable_node, false);
-  if (!tree_page)
-   return NULL;
+  if (!tree_page) {
+   /*
+    * If we walked over a stale stable_node,
+    * get_ksm_page() will call rb_erase() and it
+    * may rebalance the tree from under us. So
+    * restart the search from scratch. Returning
+    * NULL would be safe too, but we'd generate
+    * false negative insertions just because some
+    * stable_node was stale.
+    */
+   goto again;
+  }
 
   ret = memcmp_pages(kpage, tree_page);
   put_page(tree_page);
-- 
1.7.1