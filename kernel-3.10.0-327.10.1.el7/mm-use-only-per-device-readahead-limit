From 1e322f7c27662dc467953c0e4151a244db2df9da Mon Sep 17 00:00:00 2001
From: Eric Sandeen <sandeen@redhat.com>
Date: Wed, 25 Nov 2015 21:06:09 +0100
Subject: [mm] use only per-device readahead limit

Message-id: <565622C1.4090806@redhat.com>
Patchwork-id: 127638
O-Subject: [PATCH RHEL7] mm: use only per-device readahead limit
Bugzilla: 1280355
Z-Bugzilla: 1287550
RH-Acked-by: Rik van Riel <riel@redhat.com>
RH-Acked-by: Mike Snitzer <snitzer@redhat.com>
RH-Acked-by: Rafael Aquini <aquini@redhat.com>
RH-Acked-by: Mikulas Patocka <mpatocka@redhat.com>

Backport of the following upstream commit, with one additional change
to move inode_to_bdi() from a static in fs-writeback.c to a static
inline function in backing-dev.h.  (This was done upstream a while ago,
as part of a slightly more involved patch).

  From 600e19afc5f8a6c18ea49cee9511c5797db02391 Mon Sep 17 00:00:00 2001
  From: Roman Gushchin <klamm@yandex-team.ru>
  Date: Thu, 5 Nov 2015 18:47:08 -0800
  Subject: mm: use only per-device readahead limit

  Maximal readahead size is limited now by two values:
   1) by global 2Mb constant (MAX_READAHEAD in max_sane_readahead())
   2) by configurable per-device value* (bdi->ra_pages)

  There are devices, which require custom readahead limit.
  For instance, for RAIDs it's calculated as number of devices
  multiplied by chunk size times 2.

  Readahead size can never be larger than bdi->ra_pages * 2 value
  (POSIX_FADV_SEQUNTIAL doubles readahead size).

  If so, why do we need two limits?
  I suggest to completely remove this max_sane_readahead() stuff and
  use per-device readahead limit everywhere.

  Also, using right readahead size for RAID disks can significantly
  increase i/o performance:

  before:
    dd if=/dev/md2 of=/dev/null bs=100M count=100
    100+0 records in
    100+0 records out
    10485760000 bytes (10 GB) copied, 12.9741 s, 808 MB/s

  after:
    $ dd if=/dev/md2 of=/dev/null bs=100M count=100
    100+0 records in
    100+0 records out
    10485760000 bytes (10 GB) copied, 8.91317 s, 1.2 GB/s

  (It's an 8-disks RAID5 storage).

  This patch doesn't change sys_readahead and madvise(MADV_WILLNEED)
  behavior introduced by 6d2be915e589b58 ("mm/readahead.c: fix readahead
  failure for memoryless NUMA nodes and limit readahead pages").

  Signed-off-by: Roman Gushchin <klamm@yandex-team.ru>
  Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
  Cc: Jan Kara <jack@suse.cz>
  Cc: Wu Fengguang <fengguang.wu@intel.com>
  Cc: David Rientjes <rientjes@google.com>
  Cc: onstantin Khlebnikov <khlebnikov@yandex-team.ru>
  Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
  Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Signed-off-by: Alexander Gordeev <agordeev@redhat.com>

diff --git a/fs/fs-writeback.c b/fs/fs-writeback.c
index 16b7614..d0deb83 100644
--- a/fs/fs-writeback.c
+++ b/fs/fs-writeback.c
@@ -65,16 +65,6 @@ int writeback_in_progress(struct backing_dev_info *bdi)
 }
 EXPORT_SYMBOL(writeback_in_progress);
 
-static inline struct backing_dev_info *inode_to_bdi(struct inode *inode)
-{
- struct super_block *sb = inode->i_sb;
-
- if (sb_is_blkdev_sb(sb))
-  return inode->i_mapping->backing_dev_info;
-
- return sb->s_bdi;
-}
-
 static inline struct inode *wb_inode(struct list_head *head)
 {
  return list_entry(head, struct inode, i_wb_list);
diff --git a/include/linux/backing-dev.h b/include/linux/backing-dev.h
index 6915d33..df1d4b3 100644
--- a/include/linux/backing-dev.h
+++ b/include/linux/backing-dev.h
@@ -275,6 +275,16 @@ extern struct backing_dev_info noop_backing_dev_info;
 
 int writeback_in_progress(struct backing_dev_info *bdi);
 
+static inline struct backing_dev_info *inode_to_bdi(struct inode *inode)
+{
+ struct super_block *sb = inode->i_sb;
+
+ if (sb_is_blkdev_sb(sb))
+  return inode->i_mapping->backing_dev_info;
+
+ return sb->s_bdi;
+}
+
 static inline int bdi_congested(struct backing_dev_info *bdi, int bdi_bits)
 {
  if (bdi->congested_fn)
diff --git a/include/linux/mm.h b/include/linux/mm.h
index b2e1abb..ca85034 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1873,7 +1873,6 @@ void page_cache_async_readahead(struct address_space *mapping,
     pgoff_t offset,
     unsigned long size);
 
-unsigned long max_sane_readahead(unsigned long nr);
 unsigned long ra_submit(struct file_ra_state *ra,
    struct address_space *mapping,
    struct file *filp);
diff --git a/mm/filemap.c b/mm/filemap.c
index c892a9b..44cf34f 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -2032,7 +2032,6 @@ static void do_sync_mmap_readahead(struct vm_area_struct *vma,
        struct file *file,
        pgoff_t offset)
 {
- unsigned long ra_pages;
  struct address_space *mapping = file->f_mapping;
 
  /* If we don't want any read-ahead, don't bother */
@@ -2061,10 +2060,9 @@ static void do_sync_mmap_readahead(struct vm_area_struct *vma,
  /*
   * mmap read-around
   */
- ra_pages = max_sane_readahead(ra->ra_pages);
- ra->start = max_t(long, 0, offset - ra_pages / 2);
- ra->size = ra_pages;
- ra->async_size = ra_pages / 4;
+ ra->start = max_t(long, 0, offset - ra->ra_pages / 2);
+ ra->size = ra->ra_pages;
+ ra->async_size = ra->ra_pages / 4;
  ra_submit(ra, mapping, file);
 }
 
diff --git a/mm/readahead.c b/mm/readahead.c
index 6c16741..e9322db 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -216,7 +216,7 @@ int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
  if (unlikely(!mapping->a_ops->readpage && !mapping->a_ops->readpages))
   return -EINVAL;
 
- nr_to_read = max_sane_readahead(nr_to_read);
+ nr_to_read = min(nr_to_read, inode_to_bdi(mapping->host)->ra_pages);
  while (nr_to_read) {
   int err;
 
@@ -237,16 +237,6 @@ int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
  return ret;
 }
 
-#define MAX_READAHEAD   ((512*4096)/PAGE_CACHE_SIZE)
-/*
- * Given a desired number of PAGE_CACHE_SIZE readahead pages, return a
- * sensible upper limit.
- */
-unsigned long max_sane_readahead(unsigned long nr)
-{
- return min(nr, MAX_READAHEAD);
-}
-
 /*
  * Submit IO for the read-ahead request in file_ra_state.
  */
@@ -400,7 +390,7 @@ ondemand_readahead(struct address_space *mapping,
      bool hit_readahead_marker, pgoff_t offset,
      unsigned long req_size)
 {
- unsigned long max = max_sane_readahead(ra->ra_pages);
+ unsigned long max = ra->ra_pages;
  pgoff_t prev_offset;
 
  /*
-- 
1.7.1