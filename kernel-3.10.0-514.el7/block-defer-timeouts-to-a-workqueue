From a98e6990599cb1058519084e962e1ed77a4e914f Mon Sep 17 00:00:00 2001
From: Gustavo Duarte <gduarte@redhat.com>
Date: Thu, 15 Sep 2016 16:39:32 -0400
Subject: [block] defer timeouts to a workqueue

Message-id: <1473957573-2285-2-git-send-email-gduarte@redhat.com>
Patchwork-id: 157876
O-Subject: [RHEL7.3 PATCH BZ 1372483 1/2] block: defer timeouts to a workqueue
Bugzilla: 1372483
RH-Acked-by: Jeff Moyer <jmoyer@redhat.com>
RH-Acked-by: David Milburn <dmilburn@redhat.com>
RH-Acked-by: Mike Snitzer <snitzer@redhat.com>

BZ 1372483
Upstream Status: 287922eb0b186e2a5bf54fdd04b734c25c90035c

Backport: A minor KABI workaround was required.

commit 287922eb0b186e2a5bf54fdd04b734c25c90035c
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Oct 30 20:57:30 2015 +0800

    block: defer timeouts to a workqueue

    Timer context is not very useful for drivers to perform any meaningful abort
    action from.  So instead of calling the driver from this useless context
    defer it to a workqueue as soon as possible.

    Note that while a delayed_work item would seem the right thing here I didn't
    dare to use it due to the magic in blk_add_timer that pokes deep into timer
    internals.  But maybe this encourages Tejun to add a sensible API for that to
    the workqueue API and we'll all be fine in the end :)

    Contains a major update from Keith Bush:

    "This patch removes synchronizing the timeout work so that the timer can
     start a freeze on its own queue. The timer enters the queue, so timer
     context can only start a freeze, but not wait for frozen."

    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

Signed-off-by: Rafael Aquini <aquini@redhat.com>

diff --git a/block/blk-core.c b/block/blk-core.c
index bde92b0..defb773 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -634,6 +634,13 @@ static void blk_queue_usage_counter_release(struct percpu_ref *ref)
  wake_up_all(&q->mq_freeze_wq);
 }
 
+static void blk_rq_timed_out_timer(unsigned long data)
+{
+ struct request_queue *q = (struct request_queue *)data;
+
+ kblockd_schedule_work(&q->timeout_work);
+}
+
 struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 {
  struct request_queue *q;
@@ -793,6 +800,7 @@ blk_init_allocated_queue(struct request_queue *q, request_fn_proc *rfn,
  if (blk_init_rl(&q->root_rl, q, GFP_KERNEL))
   goto fail;
 
+ INIT_WORK(&q->timeout_work, blk_timeout_work);
  q->request_fn  = rfn;
  q->prep_rq_fn  = NULL;
  q->unprep_rq_fn  = NULL;
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 862c046..9d6b843 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -616,15 +616,19 @@ static void blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
  }
 }
 
-static void blk_mq_rq_timer(unsigned long priv)
+static void blk_mq_timeout_work(struct work_struct *work)
 {
- struct request_queue *q = (struct request_queue *)priv;
+ struct request_queue *q =
+  container_of(work, struct request_queue, timeout_work);
  struct blk_mq_timeout_data data = {
   .next  = 0,
   .next_set = 0,
  };
  int i;
 
+ if (blk_queue_enter(q, true))
+  return;
+
  blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &data);
 
  if (data.next_set) {
@@ -639,6 +643,7 @@ static void blk_mq_rq_timer(unsigned long priv)
     blk_mq_tag_idle(hctx);
   }
  }
+ blk_queue_exit(q);
 }
 
 /*
@@ -1990,7 +1995,7 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
  if (!q->nr_hw_queues)
   goto err_hctxs;
 
- setup_timer(&q->timeout, blk_mq_rq_timer, (unsigned long) q);
+ INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
  blk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);
 
  q->nr_queues = nr_cpu_ids;
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index b73d485..d2d0f02 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -126,13 +126,16 @@ static void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout
  }
 }
 
-void blk_rq_timed_out_timer(unsigned long data)
+void blk_timeout_work(struct work_struct *work)
 {
- struct request_queue *q = (struct request_queue *) data;
+ struct request_queue *q =
+  container_of(work, struct request_queue, timeout_work);
  unsigned long flags, next = 0;
  struct request *rq, *tmp;
  int next_set = 0;
 
+ if (blk_queue_enter(q, true))
+  return;
  spin_lock_irqsave(q->queue_lock, flags);
 
  list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list)
@@ -142,6 +145,7 @@ void blk_rq_timed_out_timer(unsigned long data)
   mod_timer(&q->timeout, round_jiffies_up(next));
 
  spin_unlock_irqrestore(q->queue_lock, flags);
+ blk_queue_exit(q);
 }
 
 /**
diff --git a/block/blk.h b/block/blk.h
index a611885..4d1760d 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -95,7 +95,7 @@ static inline void blk_flush_integrity(void)
 }
 #endif
 
-void blk_rq_timed_out_timer(unsigned long data);
+void blk_timeout_work(struct work_struct *work);
 unsigned long blk_rq_timeout(unsigned long timeout);
 void blk_add_timer(struct request *req);
 void blk_delete_timer(struct request *);
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 52ebdfb..04d39eb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -530,6 +530,7 @@ struct request_queue {
  RH_KABI_EXTEND(struct blk_flush_queue   *fq)
  RH_KABI_EXTEND(struct percpu_ref q_usage_counter)
  RH_KABI_EXTEND(bool   mq_sysfs_init_done)
+ RH_KABI_EXTEND(struct work_struct timeout_work)
 };
 
 #define QUEUE_FLAG_QUEUED 1 /* uses generic tag queueing */
-- 
1.7.1