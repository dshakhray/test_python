From f95ad361679d926245c9f8ab2fdff6daba8bd3c1 Mon Sep 17 00:00:00 2001
From: Andrea Arcangeli <aarcange@redhat.com>
Date: Tue, 13 Sep 2016 13:55:39 -0400
Subject: [mm] thp: harden the debug kernel with a strict check for thp_mmu_gather

Message-id: <1473774939-18100-4-git-send-email-aarcange@redhat.com>
Patchwork-id: 157784
O-Subject: [RHEL7.3 PATCH 3/3] mm: thp: harden the debug kernel with a strict check for thp_mmu_gather
Bugzilla: 1369365
RH-Acked-by: Larry Woodman <lwoodman@redhat.com>
RH-Acked-by: Rik van Riel <riel@redhat.com>
RH-Acked-by: Pankaj Gupta <pagupta@redhat.com>
RH-Acked-by: Koki Sanagi <ksanagi@redhat.com>

Ensure there's no thp_mmu_gather leak, caused by uninitialized
thp_mmu_gather when the THP page is first allocated.

Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Signed-off-by: Rafael Aquini <aquini@redhat.com>

diff --git a/mm/swap.c b/mm/swap.c
index 6284df6..710acc9 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -893,6 +893,7 @@ void release_pages(struct page **pages, int nr, bool cold)
  for (i = 0; i < nr; i++) {
   struct page *page = pages[i];
   const bool was_thp = is_trans_huge_page_release(page);
+  bool check_mmu_gather = false;
 
   if (unlikely(!was_thp && PageCompound(page))) {
    zone = zone_lru_unlock(zone, flags);
@@ -951,7 +952,7 @@ void release_pages(struct page **pages, int nr, bool cold)
      put_page(page);
     }
     continue;
-   } else
+   } else {
     /*
      * __split_huge_page_refcount() cannot
      * run from under us, so we can
@@ -964,11 +965,16 @@ void release_pages(struct page **pages, int nr, bool cold)
      * tails to be freed anymore.
      */
     dec_trans_huge_mmu_gather_count(page);
+    check_mmu_gather = true;
+   }
   }
 
   if (!put_page_testzero(page))
    continue;
 
+  VM_BUG_ON_PAGE(check_mmu_gather &&
+          trans_huge_mmu_gather_count(page), page);
+
   if (PageLRU(page)) {
    if (!was_thp)
     zone = zone_lru_lock(zone, page, &lock_batch,
-- 
1.7.1