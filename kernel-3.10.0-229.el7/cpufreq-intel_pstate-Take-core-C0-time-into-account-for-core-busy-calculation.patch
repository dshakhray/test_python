From 302d28528667fe2f1f3536cc259d9a548b85a81f Mon Sep 17 00:00:00 2001
From: Prarit Bhargava <prarit@redhat.com>
Date: Thu, 28 Aug 2014 14:23:14 -0400
Subject: [cpufreq] intel_pstate: Take core C0 time into account for core busy calculation

Message-id: <1409235879-1283-101-git-send-email-prarit@redhat.com>
Patchwork-id: 89449
O-Subject: [RHEL7.1 PATCH BZ 1134639 100/185] intel_pstate: Take core C0 time into account for core busy calculation
Bugzilla: 1134369
RH-Acked-by: Stefan Assmann <sassmann@redhat.com>
RH-Acked-by: Mark Langsdorf <mlangsdo@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1134639

commit fcb6a15c2e7e76d493e6f91ea889ab40e1c643a4
Author: Dirk Brandewie <dirk.j.brandewie@intel.com>
Date:   Mon Feb 3 08:55:31 2014 -0800

    intel_pstate: Take core C0 time into account for core busy calculation

    Take non-idle time into account when calculating core busy time.
    This ensures that intel_pstate will notice a decrease in load.

    References: https://bugzilla.kernel.org/show_bug.cgi?id=66581
    Cc: 3.10+ <stable@vger.kernel.org> # 3.10+
    Signed-off-by: Dirk Brandewie <dirk.j.brandewie@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

Cc: Lenny Szubowicz <lszubowi@redhat.com>
Cc: Tony Camuso <tcamuso@redhat.com>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/cpufreq/intel_pstate.c b/drivers/cpufreq/intel_pstate.c
index 2c11482..a8275ae 100644
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@ -57,6 +57,7 @@ struct sample {
  int32_t core_pct_busy;
  u64 aperf;
  u64 mperf;
+ unsigned long long tsc;
  int freq;
 };
 
@@ -96,6 +97,7 @@ struct cpudata {
 
  u64 prev_aperf;
  u64 prev_mperf;
+ unsigned long long prev_tsc;
  int sample_ptr;
  struct sample samples[SAMPLE_COUNT];
 };
@@ -548,30 +550,41 @@ static inline void intel_pstate_calc_busy(struct cpudata *cpu,
      struct sample *sample)
 {
  u64 core_pct;
- core_pct = div64_u64(int_tofp(sample->aperf * 100),
-        sample->mperf);
- sample->freq = fp_toint(cpu->pstate.max_pstate * core_pct * 1000);
+ u64 c0_pct;
 
- sample->core_pct_busy = core_pct;
+ core_pct = div64_u64(sample->aperf * 100, sample->mperf);
+
+ c0_pct = div64_u64(sample->mperf * 100, sample->tsc);
+ sample->freq = fp_toint(
+  mul_fp(int_tofp(cpu->pstate.max_pstate),
+   int_tofp(core_pct * 1000)));
+
+ sample->core_pct_busy = mul_fp(int_tofp(core_pct),
+    div_fp(int_tofp(c0_pct + 1), int_tofp(100)));
 }
 
 static inline void intel_pstate_sample(struct cpudata *cpu)
 {
  u64 aperf, mperf;
+ unsigned long long tsc;
 
  rdmsrl(MSR_IA32_APERF, aperf);
  rdmsrl(MSR_IA32_MPERF, mperf);
+ tsc = native_read_tsc();
 
  cpu->sample_ptr = (cpu->sample_ptr + 1) % SAMPLE_COUNT;
  cpu->samples[cpu->sample_ptr].aperf = aperf;
  cpu->samples[cpu->sample_ptr].mperf = mperf;
+ cpu->samples[cpu->sample_ptr].tsc = tsc;
  cpu->samples[cpu->sample_ptr].aperf -= cpu->prev_aperf;
  cpu->samples[cpu->sample_ptr].mperf -= cpu->prev_mperf;
+ cpu->samples[cpu->sample_ptr].tsc -= cpu->prev_tsc;
 
  intel_pstate_calc_busy(cpu, &cpu->samples[cpu->sample_ptr]);
 
  cpu->prev_aperf = aperf;
  cpu->prev_mperf = mperf;
+ cpu->prev_tsc = tsc;
 }
 
 static inline void intel_pstate_set_sample_time(struct cpudata *cpu)
-- 
1.7.1