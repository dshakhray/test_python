From c343f05b7b7b12c1d83ab415c4bcf8cf6ec90e23 Mon Sep 17 00:00:00 2001
From: Larry Woodman <lwoodman@redhat.com>
Date: Thu, 25 Sep 2014 17:38:22 -0400
Subject: [kernel] sched/fair: Stop searching for tasks in idle_balance if there are runnable tasks

Message-id: <1411666702-16932-3-git-send-email-lwoodman@redhat.com>
Patchwork-id: 96728
O-Subject: [RHEL7.1 PATCH 2/2] sched/fair: Stop searching for tasks in idle_balance if there are runnable tasks
Bugzilla: 1103828
RH-Acked-by: Rik van Riel <riel@redhat.com>

commit 39a4d9ca77a31503c6317e49742341d0859d5cb2
 Author: Jason Low <jason.low2@hp.com>
 Date:   Wed Apr 23 18:30:35 2014 -0700

    sched/fair: Stop searching for tasks in newidle balance if there are runnable tasks

    It was found that when running some workloads (such as AIM7) on large
    systems with many cores, CPUs do not remain idle for long. Thus, tasks
    can wake/get enqueued while doing idle balancing.

    In this patch, while traversing the domains in idle balance, in
    addition to checking for pulled_task, we add an extra check for
    this_rq->nr_running for determining if we should stop searching for
    tasks to pull. If there are runnable tasks on this rq, then we will
    stop traversing the domains. This reduces the chance that idle balance
    delays a task from running.

    This patch resulted in approximately a 6% performance improvement when
    running a Java Server workload on an 8 socket machine.

    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: daniel.lezcano@linaro.org
    Cc: alex.shi@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: efault@gmx.de
    Cc: vincent.guittot@linaro.org
    Cc: morten.rasmussen@arm.com
    Cc: aswin@hp.com
    Cc: chegu_vinod@hp.com
    Link: http://lkml.kernel.org/r/1398303035-18255-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 351dbd8..19bfaa0 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6473,7 +6473,6 @@ void idle_balance(int this_cpu, struct rq *this_rq)
   if (sd->flags & SD_BALANCE_NEWIDLE) {
    t0 = sched_clock_cpu(this_cpu);
 
-   /* If we've pulled tasks over stop searching: */
    pulled_task = load_balance(this_cpu, this_rq,
          sd, CPU_NEWLY_IDLE,
          &should_balance);
@@ -6488,7 +6487,12 @@ void idle_balance(int this_cpu, struct rq *this_rq)
   interval = msecs_to_jiffies(sd->balance_interval);
   if (time_after(next_balance, sd->last_balance + interval))
    next_balance = sd->last_balance + interval;
-  if (pulled_task) {
+
+  /*
+   * Stop searching for tasks to pull if there are
+   * now runnable tasks on this rq.
+   */
+  if (pulled_task || this_rq->nr_running > 0) {
    this_rq->idle_stamp = 0;
    break;
   }
-- 
1.7.1