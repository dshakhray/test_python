From 8659b9ee5c69c0824d2c312b9ea3c4f97fb17f1f Mon Sep 17 00:00:00 2001
From: Jiri Olsa <jolsa@redhat.com>
Date: Mon, 25 Aug 2014 07:48:14 -0400
Subject: [kernel] perf: Fix PERF_EVENT_IOC_PERIOD to force-reset the period

Message-id: <1408953210-26343-7-git-send-email-jolsa@redhat.com>
Patchwork-id: 88729
O-Subject: [PATCH RHEL7.1 BZ1133083 006/322] perf: Fix PERF_EVENT_IOC_PERIOD to force-reset the period
Bugzilla: 1133083
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

From: Jiri Olsa <jolsa@kernel.org>

Bugzilla: 1133083
https://bugzilla.redhat.com/show_bug.cgi?id=1133083

upstream
========
commit bad7192b842c83e580747ca57104dd51fe08c223
Author: Peter Zijlstra <peterz@infradead.org>
Date: Wed Nov 27 13:54:38 2013 +0000

description
===========
Vince Weaver reports that, on all architectures apart from ARM,
PERF_EVENT_IOC_PERIOD doesn't actually update the period until the next
event fires. This is counter-intuitive behaviour and is better dealt
with in the core code.

This patch ensures that the period is forcefully reset when dealing with
such a request in the core code. A subsequent patch removes the
equivalent hack from the ARM back-end.
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/events/core.c b/kernel/events/core.c
index ab1798f..ad7e0b3 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -3539,7 +3539,7 @@ static void perf_event_for_each(struct perf_event *event,
 static int perf_event_period(struct perf_event *event, u64 __user *arg)
 {
  struct perf_event_context *ctx = event->ctx;
- int ret = 0;
+ int ret = 0, active;
  u64 value;
 
  if (!is_sampling_event(event))
@@ -3563,6 +3563,20 @@ static int perf_event_period(struct perf_event *event, u64 __user *arg)
   event->attr.sample_period = value;
   event->hw.sample_period = value;
  }
+
+ active = (event->state == PERF_EVENT_STATE_ACTIVE);
+ if (active) {
+  perf_pmu_disable(ctx->pmu);
+  event->pmu->stop(event, PERF_EF_UPDATE);
+ }
+
+ local64_set(&event->hw.period_left, 0);
+
+ if (active) {
+  event->pmu->start(event, PERF_EF_RELOAD);
+  perf_pmu_enable(ctx->pmu);
+ }
+
 unlock:
  raw_spin_unlock_irq(&ctx->lock);
 
-- 
1.7.1