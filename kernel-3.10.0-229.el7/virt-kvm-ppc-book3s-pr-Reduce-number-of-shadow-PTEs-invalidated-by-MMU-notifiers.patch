From 537e5eeca6b500a15a8b050e0e9fb21aa73b3ee7 Mon Sep 17 00:00:00 2001
From: Don Zickus <dzickus@redhat.com>
Date: Fri, 12 Sep 2014 18:06:10 -0400
Subject: [virt] kvm/ppc: book3s pr - Reduce number of shadow PTEs invalidated by MMU notifiers

Message-id: <1410545655-205645-142-git-send-email-dzickus@redhat.com>
Patchwork-id: 94168
O-Subject: [RHEL7 PATCH 141/626] KVM: PPC: Book3S PR: Reduce number of shadow PTEs invalidated by MMU notifiers
Bugzilla: 1127366
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Steve Best <sbest@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>
RH-Acked-by: David Gibson <dgibson@redhat.com>

https://bugzilla.redhat.com/show_bug.cgi?id=1127366

commit 491d6ecc17171518565358c2cfe33b59722d234c
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:54 2013 +1000

    KVM: PPC: Book3S PR: Reduce number of shadow PTEs invalidated by MMU notifiers

    Currently, whenever any of the MMU notifier callbacks get called, we
    invalidate all the shadow PTEs.  This is inefficient because it means
    that we typically then get a lot of DSIs and ISIs in the guest to fault
    the shadow PTEs back in.  We do this even if the address range being
    notified doesn't correspond to guest memory.

    This commit adds code to scan the memslot array to find out what range(s)
    of guest physical addresses corresponds to the host virtual address range
    being affected.  For each such range we flush only the shadow PTEs
    for the range, on all cpus.

    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/kvm/book3s_pr.c b/arch/powerpc/kvm/book3s_pr.c
index 85b96a2..d10e118 100644
--- a/arch/powerpc/kvm/book3s_pr.c
+++ b/arch/powerpc/kvm/book3s_pr.c
@@ -150,24 +150,48 @@ int kvmppc_core_check_requests(struct kvm_vcpu *vcpu)
 }
 
 /************* MMU Notifiers *************/
+static void do_kvm_unmap_hva(struct kvm *kvm, unsigned long start,
+        unsigned long end)
+{
+ long i;
+ struct kvm_vcpu *vcpu;
+ struct kvm_memslots *slots;
+ struct kvm_memory_slot *memslot;
+
+ slots = kvm_memslots(kvm);
+ kvm_for_each_memslot(memslot, slots) {
+  unsigned long hva_start, hva_end;
+  gfn_t gfn, gfn_end;
+
+  hva_start = max(start, memslot->userspace_addr);
+  hva_end = min(end, memslot->userspace_addr +
+     (memslot->npages << PAGE_SHIFT));
+  if (hva_start >= hva_end)
+   continue;
+  /*
+   * {gfn(page) | page intersects with [hva_start, hva_end)} =
+   * {gfn, gfn+1, ..., gfn_end-1}.
+   */
+  gfn = hva_to_gfn_memslot(hva_start, memslot);
+  gfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);
+  kvm_for_each_vcpu(i, vcpu, kvm)
+   kvmppc_mmu_pte_pflush(vcpu, gfn << PAGE_SHIFT,
+           gfn_end << PAGE_SHIFT);
+ }
+}
 
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva)
 {
  trace_kvm_unmap_hva(hva);
 
- /*
-  * Flush all shadow tlb entries everywhere. This is slow, but
-  * we are 100% sure that we catch the to be unmapped page
-  */
- kvm_flush_remote_tlbs(kvm);
+ do_kvm_unmap_hva(kvm, hva, hva + PAGE_SIZE);
 
  return 0;
 }
 
 int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end)
 {
- /* kvm_unmap_hva flushes everything anyways */
- kvm_unmap_hva(kvm, start);
+ do_kvm_unmap_hva(kvm, start, end);
 
  return 0;
 }
@@ -187,7 +211,7 @@ int kvm_test_age_hva(struct kvm *kvm, unsigned long hva)
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)
 {
  /* The page will get remapped properly on its next fault */
- kvm_unmap_hva(kvm, hva);
+ do_kvm_unmap_hva(kvm, hva, hva + PAGE_SIZE);
 }
 
 /*****************************************/
-- 
1.7.1