From 322e91b57621843f49a4f981b8f077e2f7d742b1 Mon Sep 17 00:00:00 2001
From: Gustavo Duarte <gduarte@redhat.com>
Date: Tue, 9 Dec 2014 20:01:37 -0500
Subject: [mm] topology: add support for node_to_mem_node() to determine the fallback node

Message-id: <1418155299-17945-4-git-send-email-gduarte@redhat.com>
Patchwork-id: 101210
O-Subject: [RHEL7.1 PATCH BZ 953583 3/5] topology: add support for node_to_mem_node() to determine the fallback node
Bugzilla: 953583
RH-Acked-by: David Gibson <dgibson@redhat.com>
RH-Acked-by: Jes Sorensen <Jes.Sorensen@redhat.com>

BZ 953583
Upstream Status: ad2c8144418c6a81cefe65379fd47bbe8344cef2

commit ad2c8144418c6a81cefe65379fd47bbe8344cef2
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Oct 9 15:26:13 2014 -0700

    topology: add support for node_to_mem_node() to determine the fallback node

    Anton noticed (http://www.spinics.net/lists/linux-mm/msg67489.html) that
    on ppc LPARs with memoryless nodes, a large amount of memory was consumed
    by slabs and was marked unreclaimable.  He tracked it down to slab
    deactivations in the SLUB core when we allocate remotely, leading to poor
    efficiency always when memoryless nodes are present.

    After much discussion, Joonsoo provided a few patches that help
    significantly.  They don't resolve the problem altogether:

     - memory hotplug still needs testing, that is when a memoryless node
       becomes memory-ful, we want to dtrt
     - there are other reasons for going off-node than memoryless nodes,
       e.g., fully exhausted local nodes

    Neither case is resolved with this series, but I don't think that should
    block their acceptance, as they can be explored/resolved with follow-on
    patches.

    The series consists of:

    [1/3] topology: add support for node_to_mem_node() to determine the
          fallback node

    [2/3] slub: fallback to node_to_mem_node() node if allocating on
          memoryless node

          - Joonsoo's patches to cache the nearest node with memory for each
            NUMA node

    [3/3] Partial revert of 81c98869faa5 (""kthread: ensure locality of
          task_struct allocations")

     - At Tejun's request, keep the knowledge of memoryless node fallback
       to the allocator core.

    This patch (of 3):

    We need to determine the fallback node in slub allocator if the allocation
    target node is memoryless node.  Without it, the SLUB wrongly select the
    node which has no memory and can't use a partial slab, because of node
    mismatch.  Introduced function, node_to_mem_node(X), will return a node Y
    with memory that has the nearest distance.  If X is memoryless node, it
    will return nearest distance node, but, if X is normal node, it will
    return itself.

    We will use this function in following patch to determine the fallback
    node.

    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Han Pingtian <hanpt@linux.vnet.ibm.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 610ec39..4e2bd57 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -119,11 +119,20 @@ static inline int numa_node_id(void)
  * Use the accessor functions set_numa_mem(), numa_mem_id() and cpu_to_mem().
  */
 DECLARE_PER_CPU(int, _numa_mem_);
+extern int _node_numa_mem_[MAX_NUMNODES];
 
 #ifndef set_numa_mem
 static inline void set_numa_mem(int node)
 {
  this_cpu_write(_numa_mem_, node);
+ _node_numa_mem_[numa_node_id()] = node;
+}
+#endif
+
+#ifndef node_to_mem_node
+static inline int node_to_mem_node(int node)
+{
+ return _node_numa_mem_[node];
 }
 #endif
 
@@ -146,6 +155,7 @@ static inline int cpu_to_mem(int cpu)
 static inline void set_cpu_numa_mem(int cpu, int node)
 {
  per_cpu(_numa_mem_, cpu) = node;
+ _node_numa_mem_[cpu_to_node(cpu)] = node;
 }
 #endif
 
@@ -159,6 +169,13 @@ static inline int numa_mem_id(void)
 }
 #endif
 
+#ifndef node_to_mem_node
+static inline int node_to_mem_node(int node)
+{
+ return node;
+}
+#endif
+
 #ifndef cpu_to_mem
 static inline int cpu_to_mem(int cpu)
 {
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index ff81683..59b8842 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -80,6 +80,7 @@ EXPORT_PER_CPU_SYMBOL(numa_node);
  */
 DEFINE_PER_CPU(int, _numa_mem_);  /* Kernel "local memory" node */
 EXPORT_PER_CPU_SYMBOL(_numa_mem_);
+int _node_numa_mem_[MAX_NUMNODES];
 #endif
 
 /*
-- 
1.7.1