From 11fadd36527821b8ad9100d41bfb45b849651e77 Mon Sep 17 00:00:00 2001
From: Jeff Moyer <jmoyer@redhat.com>
Date: Mon, 8 Sep 2014 22:52:52 -0400
Subject: [fs] aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration

Message-id: <1410216777-18522-4-git-send-email-jmoyer@redhat.com>
Patchwork-id: 93349
O-Subject: [RHEL7 PATCH 3/8] aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration
Bugzilla: 1122092
RH-Acked-by: Zach Brown <zab@redhat.com>

This is a backport of the following commit.  This patch required
significant rework, since we can't backport the change to per-cpu
reference counting.  I've preserved the locking changes made upstream.
This patch could use a fair amount of review.

This was tested using the libaio test harness, aio-stress, and
xfstests aio tests.

This addresses bug 1122092.

  commit fa8a53c39f3fdde98c9eace6a9b412143f0f6ed6
  Author: Benjamin LaHaise <bcrl@kvack.org>
  Date:   Fri Mar 28 10:14:45 2014 -0400

    aio: v4 ensure access to ctx->ring_pages is correctly serialised for migration

    As reported by Tang Chen, Gu Zheng and Yasuaki Isimatsu, the following issues
    exist in the aio ring page migration support.

    As a result, for example, we have the following problem:

                thread 1                      |              thread 2
                                              |
    aio_migratepage()                         |
     |-> take ctx->completion_lock            |
     |-> migrate_page_copy(new, old)          |
     |   *NOW*, ctx->ring_pages[idx] == old   |
                                              |
                                              |    *NOW*, ctx->ring_pages[idx] == old
                                              |    aio_read_events_ring()
                                              |     |-> ring = kmap_atomic(ctx->ring_pages[0])
                                              |     |-> ring->head = head;          *HERE, write to the old ring page*
                                              |     |-> kunmap_atomic(ring);
                                              |
     |-> ctx->ring_pages[idx] = new           |
     |   *BUT NOW*, the content of            |
     |    ring_pages[idx] is old.             |
     |-> release ctx->completion_lock         |

    As above, the new ring page will not be updated.

    Fix this issue, as well as prevent races in aio_ring_setup() by holding
    the ring_lock mutex during kioctx setup and page migration.  This avoids
    the overhead of taking another spinlock in aio_read_events_ring() as Tang's
    and Gu's original fix did, pushing the overhead into the migration code.

    Note that to handle the nesting of ring_lock inside of mmap_sem, the
    migratepage operation uses mutex_trylock().  Page migration is not a 100%
    critical operation in this case, so the ocassional failure can be
    tolerated.  This issue was reported by Sasha Levin.

    Based on feedback from Linus, avoid the extra taking of ctx->completion_lock.
    Instead, make page migration fully serialised by mapping->private_lock, and
    have aio_free_ring() simply disconnect the kioctx from the mapping by calling
    put_aio_ring_file() before touching ctx->ring_pages[].  This simplifies the
    error handling logic in aio_migratepage(), and should improve robustness.

    v4: always do mutex_unlock() in cases when kioctx setup fails.

    Reported-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Benjamin LaHaise <bcrl@kvack.org>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Cc: stable@vger.kernel.org

Signed-off-by: Jeff Moyer <jmoyer@redhat.com>
Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/fs/aio.c b/fs/aio.c
index d9d22a4..7dc7565 100644
--- a/fs/aio.c
+++ b/fs/aio.c
@@ -48,7 +48,8 @@
 struct aio_ring {
  unsigned id; /* kernel internal index number */
  unsigned nr; /* number of io_events */
- unsigned head;
+ unsigned head; /* Written to by userland or under ring_lock
+     * mutex by aio_read_events_ring(). */
  unsigned tail;
 
  unsigned magic;
@@ -213,6 +214,11 @@ static void aio_free_ring(struct kioctx *ctx)
 {
  int i;
 
+ /* Disconnect the kiotx from the ring file.  This prevents future
+  * accesses to the kioctx from page migration.
+  */
+ put_aio_ring_file(ctx);
+
  for (i = 0; i < ctx->nr_pages; i++) {
   struct page *page;
   pr_debug("pid(%d) [%d] page->count=%d\n", current->pid, i,
@@ -224,8 +230,6 @@ static void aio_free_ring(struct kioctx *ctx)
   put_page(page);
  }
 
- put_aio_ring_file(ctx);
-
  if (ctx->ring_pages && ctx->ring_pages != ctx->internal_pages)
   kfree(ctx->ring_pages);
 }
@@ -251,29 +255,38 @@ static int aio_migratepage(struct address_space *mapping, struct page *new,
 {
  struct kioctx *ctx;
  unsigned long flags;
+ pgoff_t idx;
  int rc;
 
  rc = 0;
 
- /* Make sure the old page hasn't already been changed */
+ /* mapping->private_lock here protects against the kioctx teardown.  */
  spin_lock(&mapping->private_lock);
  ctx = mapping->private_data;
- if (ctx) {
-  pgoff_t idx;
-  spin_lock_irqsave(&ctx->completion_lock, flags);
-  idx = old->index;
-  if (idx < (pgoff_t)ctx->nr_pages) {
-   if (ctx->ring_pages[idx] != old)
-    rc = -EAGAIN;
-  } else
-   rc = -EINVAL;
-  spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ if (!ctx) {
+  rc = -EINVAL;
+  goto out;
+ }
+
+ /* The ring_lock mutex.  The prevents aio_read_events() from writing
+  * to the ring's head, and prevents page migration from mucking in
+  * a partially initialized kiotx.
+  */
+ if (!mutex_trylock(&ctx->ring_lock)) {
+  rc = -EAGAIN;
+  goto out;
+ }
+
+ idx = old->index;
+ if (idx < (pgoff_t)ctx->nr_pages) {
+  /* Make sure the old page hasn't already been changed */
+  if (ctx->ring_pages[idx] != old)
+   rc = -EAGAIN;
  } else
   rc = -EINVAL;
- spin_unlock(&mapping->private_lock);
 
  if (rc != 0)
-  return rc;
+  goto out_unlock;
 
  /* Writeback must be complete */
  BUG_ON(PageWriteback(old));
@@ -282,38 +295,26 @@ static int aio_migratepage(struct address_space *mapping, struct page *new,
  rc = migrate_page_move_mapping(mapping, new, old, NULL, mode, 1);
  if (rc != MIGRATEPAGE_SUCCESS) {
   put_page(new);
-  return rc;
+  goto out_unlock;
  }
 
- /* We can potentially race against kioctx teardown here.  Use the
-  * address_space's private data lock to protect the mapping's
-  * private_data.
+ /* Take completion_lock to prevent other writes to the ring buffer
+  * while the old page is copied to the new.  This prevents new
+  * events from being lost.
   */
- spin_lock(&mapping->private_lock);
- ctx = mapping->private_data;
- if (ctx) {
-  pgoff_t idx;
-  spin_lock_irqsave(&ctx->completion_lock, flags);
-  migrate_page_copy(new, old);
-  idx = old->index;
-  if (idx < (pgoff_t)ctx->nr_pages) {
-   /* And only do the move if things haven't changed */
-   if (ctx->ring_pages[idx] == old)
-    ctx->ring_pages[idx] = new;
-   else
-    rc = -EAGAIN;
-  } else
-   rc = -EINVAL;
-  spin_unlock_irqrestore(&ctx->completion_lock, flags);
- } else
-  rc = -EBUSY;
- spin_unlock(&mapping->private_lock);
+ spin_lock_irqsave(&ctx->completion_lock, flags);
+ migrate_page_copy(new, old);
+ BUG_ON(ctx->ring_pages[idx] != old);
+ ctx->ring_pages[idx] = new;
+ spin_unlock_irqrestore(&ctx->completion_lock, flags);
 
- if (rc == MIGRATEPAGE_SUCCESS)
-  put_page(old);
- else
-  put_page(new);
+ /* The old page is no longer accessible. */
+ put_page(old);
 
+out_unlock:
+ mutex_unlock(&ctx->ring_lock);
+out:
+ spin_unlock(&mapping->private_lock);
  return rc;
 }
 #endif
@@ -334,7 +335,6 @@ static int aio_setup_ring(struct kioctx *ctx)
  int nr_pages;
  int i;
  struct file *file;
- unsigned long flags;
 
  /* Compensate for the ring buffer's head/tail overlap entry */
  nr_events += 2; /* 1 is required, 2 for good luck */
@@ -349,7 +349,7 @@ static int aio_setup_ring(struct kioctx *ctx)
  file = aio_private_file(ctx, nr_pages);
  if (IS_ERR(file)) {
   ctx->aio_ring_file = NULL;
-  return -EAGAIN;
+  return -ENOMEM;
  }
 
  ctx->aio_ring_file = file;
@@ -384,7 +384,7 @@ static int aio_setup_ring(struct kioctx *ctx)
 
  if (unlikely(i != nr_pages)) {
   aio_free_ring(ctx);
-  return -EAGAIN;
+  return -ENOMEM;
  }
 
  ctx->mmap_size = nr_pages * PAGE_SIZE;
@@ -398,7 +398,7 @@ static int aio_setup_ring(struct kioctx *ctx)
  if (IS_ERR((void *)ctx->mmap_base)) {
   ctx->mmap_size = 0;
   aio_free_ring(ctx);
-  return -EAGAIN;
+  return -ENOMEM;
  }
 
  pr_debug("mmap address: 0x%08lx\n", ctx->mmap_base);
@@ -406,14 +406,6 @@ static int aio_setup_ring(struct kioctx *ctx)
  ctx->user_id = ctx->mmap_base;
  ctx->nr_events = nr_events; /* trusted copy */
 
- /*
-  * The aio ring pages are user space pages, so they can be migrated.
-  * When writing to an aio ring page, we should ensure the page is not
-  * being migrated. Aio page migration procedure is protected by
-  * ctx->completion_lock, so we add this lock here.
-  */
- spin_lock_irqsave(&ctx->completion_lock, flags);
-
  ring = kmap_atomic(ctx->ring_pages[0]);
  ring->nr = nr_events; /* user copy */
  ring->id = ctx->user_id;
@@ -425,8 +417,6 @@ static int aio_setup_ring(struct kioctx *ctx)
  kunmap_atomic(ring);
  flush_dcache_page(ctx->ring_pages[0]);
 
- spin_unlock_irqrestore(&ctx->completion_lock, flags);
-
  return 0;
 }
 
@@ -574,16 +564,22 @@ static struct kioctx *ioctx_alloc(unsigned nr_events)
 
  ctx->max_reqs = nr_events;
 
- atomic_set(&ctx->users, 2);
- atomic_set(&ctx->dead, 0);
  spin_lock_init(&ctx->ctx_lock);
  spin_lock_init(&ctx->completion_lock);
  mutex_init(&ctx->ring_lock);
+ /* Protect against page migration throughout kioctx setup by keeping
+  * the ring_lock mutex held until setup is complete. */
+ mutex_lock(&ctx->ring_lock);
+
  init_waitqueue_head(&ctx->wait);
 
  INIT_LIST_HEAD(&ctx->active_reqs);
 
- if (aio_setup_ring(ctx) < 0)
+ atomic_set(&ctx->users, 2);
+ atomic_set(&ctx->dead, 0);
+
+ err = aio_setup_ring(ctx);
+ if (err < 0)
   goto out_freectx;
 
  /* limit the number of system wide aios */
@@ -601,6 +597,9 @@ static struct kioctx *ioctx_alloc(unsigned nr_events)
  hlist_add_head_rcu(&ctx->list, &mm->ioctx_list);
  spin_unlock(&mm->ioctx_lock);
 
+ /* Release the ring_lock mutex now that all setup is complete. */
+ mutex_unlock(&ctx->ring_lock);
+
  pr_debug("allocated ioctx %p[%ld]: mm=%p mask=0x%x\n",
    ctx, ctx->user_id, mm, ctx->nr_events);
  return ctx;
@@ -609,6 +608,7 @@ out_cleanup:
  err = -EAGAIN;
  aio_free_ring(ctx);
 out_freectx:
+ mutex_unlock(&ctx->ring_lock);
  put_aio_ring_file(ctx);
  kmem_cache_free(kioctx_cachep, ctx);
  pr_debug("error allocating ioctx %d\n", err);
@@ -923,6 +923,7 @@ static long aio_read_events_ring(struct kioctx *ctx,
 
  mutex_lock(&ctx->ring_lock);
 
+ /* Access to ->ring_pages here is protected by ctx->ring_lock. */
  ring = kmap_atomic(ctx->ring_pages[0]);
  head = ring->head;
  kunmap_atomic(ring);
-- 
1.7.1