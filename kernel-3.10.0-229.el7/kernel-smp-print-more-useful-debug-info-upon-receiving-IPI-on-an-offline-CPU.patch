From 897835823c851cac89d00ec5d8bdbbbba6ee4cbd Mon Sep 17 00:00:00 2001
From: Gustavo Duarte <gduarte@redhat.com>
Date: Thu, 23 Oct 2014 22:42:52 -0400
Subject: [kernel] smp: print more useful debug info upon receiving IPI on an offline CPU

Message-id: <1414104173-20882-2-git-send-email-gduarte@redhat.com>
Patchwork-id: 98365
O-Subject: [RHEL7.1 PATCH BZ 1100093 1/2] smp: print more useful debug info upon receiving IPI on an offline CPU
Bugzilla: 1100093
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Don Zickus <dzickus@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1100093
Upstream Status: a219ccf4637396a2392bfbec7c12acbfe2b06b46

commit a219ccf4637396a2392bfbec7c12acbfe2b06b46
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Fri Jun 6 14:37:05 2014 -0700

    smp: print more useful debug info upon receiving IPI on an offline CPU

    There is a longstanding problem related to CPU hotplug which causes IPIs
    to be delivered to offline CPUs, and the smp-call-function IPI handler
    code prints out a warning whenever this is detected.  Every once in a
    while this (usually harmless) warning gets reported on LKML, but so far
    it has not been completely fixed.  Usually the solution involves finding
    out the IPI sender and fixing it by adding appropriate synchronization
    with CPU hotplug.

    However, while going through one such internal bug reports, I found that
    there is a significant bug in the receiver side itself (more
    specifically, in stop-machine) that can lead to this problem even when
    the sender code is perfectly fine.  This patchset fixes that
    synchronization problem in the CPU hotplug stop-machine code.

    Patch 1 adds some additional debug code to the smp-call-function
    framework, to help debug such issues easily.

    Patch 2 modifies the stop-machine code to ensure that any IPIs that were
    sent while the target CPU was online, would be noticed and handled by
    that CPU without fail before it goes offline.  Thus, this avoids
    scenarios where IPIs are received on offline CPUs (as long as the sender
    uses proper hotplug synchronization).

    In fact, I debugged the problem by using Patch 1, and found that the
    payload of the IPI was always the block layer's trigger_softirq()
    function.  But I was not able to find anything wrong with the block
    layer code.  That's when I started looking at the stop-machine code and
    realized that there is a race-window which makes the IPI _receiver_ the
    culprit, not the sender.  Patch 2 fixes that race and hence this should
    put an end to most of the hard-to-debug IPI-to-offline-CPU issues.

    This patch (of 2):

    Today the smp-call-function code just prints a warning if we get an IPI
    on an offline CPU.  This info is sufficient to let us know that
    something went wrong, but often it is very hard to debug exactly who
    sent the IPI and why, from this info alone.

    In most cases, we get the warning about the IPI to an offline CPU,
    immediately after the CPU going offline comes out of the stop-machine
    phase and reenables interrupts.  Since all online CPUs participate in
    stop-machine, the information regarding the sender of the IPI is already
    lost by the time we exit the stop-machine loop.  So even if we dump the
    stack on each CPU at this point, we won't find anything useful since all
    of them will show the stack-trace of the stopper thread.  So we need a
    better way to figure out who sent the IPI and why.

    To achieve this, when we detect an IPI targeted to an offline CPU, loop
    through the call-single-data linked list and print out the payload
    (i.e., the name of the function which was supposed to be executed by the
    target CPU).  This would give us an insight as to who might have sent
    the IPI and help us debug this further.

    [akpm@linux-foundation.org: correctly suppress warning output on second and later occurrences]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mike Galbraith <mgalbraith@suse.de>
    Cc: Gautham R Shenoy <ego@linux.vnet.ibm.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/smp.c b/kernel/smp.c
index 4187c6d..f3f6fa8 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -186,14 +186,26 @@ void generic_smp_call_function_single_interrupt(void)
 {
  struct llist_node *entry;
  struct call_single_data *csd, *csd_next;
+ static bool warned;
+
+ entry = llist_del_all(&__get_cpu_var(call_single_queue));
+ entry = llist_reverse_order(entry);
 
  /*
   * Shouldn't receive this interrupt on a cpu that is not yet online.
   */
- WARN_ON_ONCE(!cpu_online(smp_processor_id()));
+ if (unlikely(!cpu_online(smp_processor_id()) && !warned)) {
+  warned = true;
+  WARN(1, "IPI on offline CPU %d\n", smp_processor_id());
 
- entry = llist_del_all(&__get_cpu_var(call_single_queue));
- entry = llist_reverse_order(entry);
+  /*
+   * We don't have to use the _safe() variant here
+   * because we are not invoking the IPI handlers yet.
+   */
+  llist_for_each_entry(csd, entry, llist)
+   pr_warn("IPI callback %pS sent to offline CPU\n",
+    csd->func);
+ }
 
  llist_for_each_entry_safe(csd, csd_next, entry, llist) {
   csd->func(csd->info);
-- 
1.7.1