From 3ed243b92a5c62a6f343675bb3ad51e44032b094 Mon Sep 17 00:00:00 2001
From: Larry Woodman <lwoodman@redhat.com>
Date: Wed, 9 Apr 2014 16:39:43 -0400
Subject: [kernel] futex: Fix pthread_cond_broadcast() to wake up all threads

Message-id: <1397061583-17255-1-git-send-email-lwoodman@redhat.com>
Patchwork-id: 78842
O-Subject: [RHEL7 PATCH] Fix pthread_cond_broadcast() to wake up all threads.
Bugzilla: 1084757
RH-Acked-by: Rik van Riel <riel@redhat.com>
RH-Acked-by: Rafael Aquini <aquini@redhat.com>

Jan Stancek noticed that pthread_cond_broadcast() does not wake up all threads
after my "revert back to the explicit waiter counting code" upstream backport
went into the -119 RHEL7 kernel.  The problem was a race between requeue and wake
in the upstream futex code.  The attached upstream backport fixes this problem.

Brew: https://brewweb.devel.redhat.com/taskinfo?taskID=7321344

Testing: Tested by Jan Stancek(Thanks Jan!) and Linus.

BZ: Fixes BZ1084757.

--------------------------------------------------------------------------------
 commit 69cd9eba38867a493a043bb13eb9b33cad5f1a9a
 Author: Linus Torvalds <torvalds@linux-foundation.org>
 Date:   Tue Apr 8 15:30:07 2014 -0700

    futex: avoid race between requeue and wake

    Jan Stancek reported:
     "pthread_cond_broadcast/4-1.c testcase from openposix testsuite (LTP)
      occasionally fails, because some threads fail to wake up.

      Testcase creates 5 threads, which are all waiting on same condition.
      Main thread then calls pthread_cond_broadcast() without holding mutex,
      which calls:

          futex(uaddr1, FUTEX_CMP_REQUEUE_PRIVATE, 1, 2147483647, uaddr2, ..)

      This immediately wakes up single thread A, which unlocks mutex and
      tries to wake up another thread:

          futex(uaddr2, FUTEX_WAKE_PRIVATE, 1)

      If thread A manages to call futex_wake() before any waiters are
      requeued for uaddr2, no other thread is woken up"

    The ordering constraints for the hash bucket waiter counting are that
    the waiter counts have to be incremented _before_ getting the spinlock
    (because the spinlock acts as part of the memory barrier), but the
    "requeue" operation didn't honor those rules, and nobody had even
    thought about that case.

    This fairly simple patch just increments the waiter count for the target
    hash bucket (hb2) when requeing a futex before taking the locks.  It
    then decrements them again after releasing the lock - the code that
    actually moves the futex(es) between hash buckets will do the additional
    required waiter count housekeeping.

    Reported-and-tested-by: Jan Stancek <jstancek@redhat.com>
    Acked-by: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org # 3.14
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/futex.c b/kernel/futex.c
index 5ff12f5..baece19 100644
--- a/kernel/futex.c
+++ b/kernel/futex.c
@@ -1450,6 +1450,7 @@ retry:
  hb2 = hash_futex(&key2);
 
 retry_private:
+ hb_waiters_inc(hb2);
  double_lock_hb(hb1, hb2);
 
  if (likely(cmpval != NULL)) {
@@ -1459,6 +1460,7 @@ retry_private:
 
   if (unlikely(ret)) {
    double_unlock_hb(hb1, hb2);
+   hb_waiters_dec(hb2);
 
    ret = get_user(curval, uaddr1);
    if (ret)
@@ -1508,6 +1510,7 @@ retry_private:
    break;
   case -EFAULT:
    double_unlock_hb(hb1, hb2);
+   hb_waiters_dec(hb2);
    put_futex_key(&key2);
    put_futex_key(&key1);
    ret = fault_in_user_writeable(uaddr2);
@@ -1517,6 +1520,7 @@ retry_private:
   case -EAGAIN:
    /* The owner was exiting, try again. */
    double_unlock_hb(hb1, hb2);
+   hb_waiters_dec(hb2);
    put_futex_key(&key2);
    put_futex_key(&key1);
    cond_resched();
@@ -1592,6 +1596,7 @@ retry_private:
 
 out_unlock:
  double_unlock_hb(hb1, hb2);
+ hb_waiters_dec(hb2);
 
  /*
   * drop_futex_key_refs() must be called outside the spinlocks. During
-- 
1.7.1