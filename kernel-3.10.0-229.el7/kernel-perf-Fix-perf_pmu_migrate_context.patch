From 856e1ba7d9de46e6049b3a3afd7d5b5fa22b59ea Mon Sep 17 00:00:00 2001
From: Jiri Olsa <jolsa@redhat.com>
Date: Mon, 25 Aug 2014 07:48:10 -0400
Subject: [kernel] perf: Fix perf_pmu_migrate_context

Message-id: <1408953210-26343-3-git-send-email-jolsa@redhat.com>
Patchwork-id: 88722
O-Subject: [PATCH RHEL7.1 BZ1133083 002/322] perf: Fix perf_pmu_migrate_context
Bugzilla: 1133083
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

From: Jiri Olsa <jolsa@kernel.org>

Bugzilla: 1133083
https://bugzilla.redhat.com/show_bug.cgi?id=1133083

upstream
========
commit 9886167d20c0720dcfb01e62cdff4d906b226f43
Author: Peter Zijlstra <peterz@infradead.org>
Date: Thu Oct 3 16:02:23 2013 +0200

KABI
====
The 'struct perf_event*' pointer is used within 'struct thread_struct',
making perf_event struct KABI-broken any time we change it. At the moment
we dont have any perf related function on KABI list, so masking the change
with __GENKSYMS__ macro.

description
===========
While auditing the list_entry usage due to a trinity bug I found that
perf_pmu_migrate_context violates the rules for
perf_event::event_entry.

The problem is that perf_event::event_entry is a RCU list element, and
hence we must wait for a full RCU grace period before re-using the
element after deletion.

Therefore the usage in perf_pmu_migrate_context() which re-uses the
entry immediately is broken. For now introduce another list_head into
perf_event for this specific usage.

This doesn't actually fix the trinity report because that never goes
through this code.
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3efbb37..5c036e0 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -314,7 +314,20 @@ struct ring_buffer;
  */
 struct perf_event {
 #ifdef CONFIG_PERF_EVENTS
+ /*
+  * XXX: group_entry and sibling_list should be mutually exclusive;
+  * either you're a sibling on a group, or you're the group leader.
+  * Rework the code to always use the same list element.
+  *
+  * Locked for modification by both ctx->mutex and ctx->lock; holding
+  * either sufficies for read.
+  */
  struct list_head  group_entry;
+ /*
+  * entry onto perf_event_context::event_list;
+  *   modifications require ctx->lock
+  *   RCU safe iterations.
+  */
  struct list_head  event_entry;
  struct list_head  sibling_list;
  struct hlist_node  hlist_entry;
@@ -434,6 +447,14 @@ struct perf_event {
  int    cgrp_defer_enabled;
 #endif
 
+#ifndef __GENKSYMS__
+ /*
+  * We need storage to track the entries in perf_pmu_migrate_context; we
+  * cannot use the event_entry because of RCU and we want to keep the
+  * group in tact which avoids us using the other two entries.
+  */
+ struct list_head  migrate_entry;
+#endif
 #endif /* CONFIG_PERF_EVENTS */
 };
 
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 1026f3a..42948f9 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -7273,15 +7273,15 @@ void perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)
   perf_remove_from_context(event);
   unaccount_event_cpu(event, src_cpu);
   put_ctx(src_ctx);
-  list_add(&event->event_entry, &events);
+  list_add(&event->migrate_entry, &events);
  }
  mutex_unlock(&src_ctx->mutex);
 
  synchronize_rcu();
 
  mutex_lock(&dst_ctx->mutex);
- list_for_each_entry_safe(event, tmp, &events, event_entry) {
-  list_del(&event->event_entry);
+ list_for_each_entry_safe(event, tmp, &events, migrate_entry) {
+  list_del(&event->migrate_entry);
   if (event->state >= PERF_EVENT_STATE_OFF)
    event->state = PERF_EVENT_STATE_INACTIVE;
   account_event_cpu(event, dst_cpu);
-- 
1.7.1