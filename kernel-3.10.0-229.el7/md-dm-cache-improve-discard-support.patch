From 7133e74564115630e326a1badd948dc7392cb306 Mon Sep 17 00:00:00 2001
From: Mike Snitzer <snitzer@redhat.com>
Date: Thu, 13 Nov 2014 01:53:41 -0500
Subject: [md] dm-cache: improve discard support

Message-id: <1415843623-26132-8-git-send-email-snitzer@redhat.com>
Patchwork-id: 99886
O-Subject: [RHEL7.1 PATCH 7/9] dm cache: improve discard support
Bugzilla: 1159001
RH-Acked-by: Joe Thornber <thornber@redhat.com>

BZ: 1159001

Upstream linux-dm.git commit 7ae34e7778966d39f66397491eb114b613202c20
Author: Joe Thornber <ejt@redhat.com>
Date:   Thu Nov 6 10:18:04 2014 +0000

    dm cache: improve discard support

    Safely allow the discard blocksize to be larger than the cache blocksize
    by using the bio prison's range locking support.  This also improves
    discard performance considerly because larger discards are issued to the
    dm-cache device.  The discard blocksize was always intended to be
    greater than the cache blocksize.  But until now it wasn't implemented
    safely.

    Also, by safely restoring the ability to have discard blocksize larger
    than cache blocksize we're able to significantly reduce the memory used
    for the cache's discard bitset.  Before, with a small discard blocksize,
    the discard bitset could get quite large because its size is a function
    of the discard blocksize and the origin device's size.  For example,
    previously, using a 32KB cache blocksize with a 40TB origin resulted in
    1280MB of incore memory use for the discard bitset!  Now, the discard
    blocksize is scaled up accordingly to ensure the discard bitset is
    capped at 2**14 bits, or 16KB.

    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/md/dm-cache-target.c b/drivers/md/dm-cache-target.c
index 75103ab..e336c72 100644
--- a/drivers/md/dm-cache-target.c
+++ b/drivers/md/dm-cache-target.c
@@ -304,6 +304,7 @@ struct dm_cache_migration {
  dm_cblock_t cblock;
 
  bool err:1;
+ bool discard:1;
  bool writeback:1;
  bool demote:1;
  bool promote:1;
@@ -427,12 +428,12 @@ static void prealloc_put_cell(struct prealloc *p, struct dm_bio_prison_cell *cel
 
 /*----------------------------------------------------------------*/
 
-static void build_key(dm_oblock_t oblock, struct dm_cell_key *key)
+static void build_key(dm_oblock_t begin, dm_oblock_t end, struct dm_cell_key *key)
 {
  key->virtual = 0;
  key->dev = 0;
- key->block_begin = from_oblock(oblock);
- key->block_end = key->block_begin + 1ULL;
+ key->block_begin = from_oblock(begin);
+ key->block_end = from_oblock(end);
 }
 
 /*
@@ -442,15 +443,15 @@ static void build_key(dm_oblock_t oblock, struct dm_cell_key *key)
  */
 typedef void (*cell_free_fn)(void *context, struct dm_bio_prison_cell *cell);
 
-static int bio_detain(struct cache *cache, dm_oblock_t oblock,
-        struct bio *bio, struct dm_bio_prison_cell *cell_prealloc,
-        cell_free_fn free_fn, void *free_context,
-        struct dm_bio_prison_cell **cell_result)
+static int bio_detain_range(struct cache *cache, dm_oblock_t oblock_begin, dm_oblock_t oblock_end,
+       struct bio *bio, struct dm_bio_prison_cell *cell_prealloc,
+       cell_free_fn free_fn, void *free_context,
+       struct dm_bio_prison_cell **cell_result)
 {
  int r;
  struct dm_cell_key key;
 
- build_key(oblock, &key);
+ build_key(oblock_begin, oblock_end, &key);
  r = dm_bio_detain(cache->prison, &key, bio, cell_prealloc, cell_result);
  if (r)
   free_fn(free_context, cell_prealloc);
@@ -458,6 +459,16 @@ static int bio_detain(struct cache *cache, dm_oblock_t oblock,
  return r;
 }
 
+static int bio_detain(struct cache *cache, dm_oblock_t oblock,
+        struct bio *bio, struct dm_bio_prison_cell *cell_prealloc,
+        cell_free_fn free_fn, void *free_context,
+        struct dm_bio_prison_cell **cell_result)
+{
+ dm_oblock_t end = to_oblock(from_oblock(oblock) + 1ULL);
+ return bio_detain_range(cache, oblock, end, bio,
+    cell_prealloc, free_fn, free_context, cell_result);
+}
+
 static int get_cell(struct cache *cache,
       dm_oblock_t oblock,
       struct prealloc *structs,
@@ -469,7 +480,7 @@ static int get_cell(struct cache *cache,
 
  cell_prealloc = prealloc_get_cell(structs);
 
- build_key(oblock, &key);
+ build_key(oblock, to_oblock(from_oblock(oblock) + 1ULL), &key);
  r = dm_get_cell(cache->prison, &key, cell_prealloc, cell_result);
  if (r)
   prealloc_put_cell(structs, cell_prealloc);
@@ -519,25 +530,34 @@ static dm_block_t block_div(dm_block_t b, uint32_t n)
  return b;
 }
 
-static dm_dblock_t oblock_to_dblock(struct cache *cache, dm_oblock_t oblock)
+static dm_block_t oblocks_per_dblock(struct cache *cache)
 {
- uint32_t discard_blocks = cache->discard_block_size;
- dm_block_t b = from_oblock(oblock);
+ dm_block_t oblocks = cache->discard_block_size;
 
- if (!block_size_is_power_of_two(cache))
-  discard_blocks = discard_blocks / cache->sectors_per_block;
+ if (block_size_is_power_of_two(cache))
+  oblocks >>= cache->sectors_per_block_shift;
  else
-  discard_blocks >>= cache->sectors_per_block_shift;
+  oblocks = block_div(oblocks, cache->sectors_per_block);
 
- b = block_div(b, discard_blocks);
+ return oblocks;
+}
+
+static dm_dblock_t oblock_to_dblock(struct cache *cache, dm_oblock_t oblock)
+{
+ return to_dblock(block_div(from_oblock(oblock),
+       oblocks_per_dblock(cache)));
+}
 
- return to_dblock(b);
+static dm_oblock_t dblock_to_oblock(struct cache *cache, dm_dblock_t dblock)
+{
+ return to_oblock(from_dblock(dblock) * oblocks_per_dblock(cache));
 }
 
 static void set_discard(struct cache *cache, dm_dblock_t b)
 {
  unsigned long flags;
 
+ BUG_ON(from_dblock(b) >= from_dblock(cache->discard_nr_blocks));
  atomic_inc(&cache->stats.discard_count);
 
  spin_lock_irqsave(&cache->lock, flags);
@@ -989,7 +1009,7 @@ static void copy_complete(int read_err, unsigned long write_err, void *context)
  wake_worker(cache);
 }
 
-static void issue_copy_real(struct dm_cache_migration *mg)
+static void issue_copy(struct dm_cache_migration *mg)
 {
  int r;
  struct dm_io_region o_region, c_region;
@@ -1068,11 +1088,46 @@ static void avoid_copy(struct dm_cache_migration *mg)
  migration_success_pre_commit(mg);
 }
 
-static void issue_copy(struct dm_cache_migration *mg)
+static void calc_discard_block_range(struct cache *cache, struct bio *bio,
+         dm_dblock_t *b, dm_dblock_t *e)
+{
+ sector_t sb = bio->bi_sector;
+ sector_t se = bio_end_sector(bio);
+
+ *b = to_dblock(dm_sector_div_up(sb, cache->discard_block_size));
+
+ if (se - sb < cache->discard_block_size)
+  *e = *b;
+ else
+  *e = to_dblock(block_div(se, cache->discard_block_size));
+}
+
+static void issue_discard(struct dm_cache_migration *mg)
+{
+ dm_dblock_t b, e;
+ struct bio *bio = mg->new_ocell->holder;
+
+ calc_discard_block_range(mg->cache, bio, &b, &e);
+ while (b != e) {
+  set_discard(mg->cache, b);
+  b = to_dblock(from_dblock(b) + 1);
+ }
+
+ bio_endio(bio, 0);
+ cell_defer(mg->cache, mg->new_ocell, false);
+ free_migration(mg);
+}
+
+static void issue_copy_or_discard(struct dm_cache_migration *mg)
 {
  bool avoid;
  struct cache *cache = mg->cache;
 
+ if (mg->discard) {
+  issue_discard(mg);
+  return;
+ }
+
  if (mg->writeback || mg->demote)
   avoid = !is_dirty(cache, mg->cblock) ||
    is_discarded_oblock(cache, mg->old_oblock);
@@ -1087,7 +1142,7 @@ static void issue_copy(struct dm_cache_migration *mg)
   }
  }
 
- avoid ? avoid_copy(mg) : issue_copy_real(mg);
+ avoid ? avoid_copy(mg) : issue_copy(mg);
 }
 
 static void complete_migration(struct dm_cache_migration *mg)
@@ -1172,6 +1227,7 @@ static void promote(struct cache *cache, struct prealloc *structs,
  struct dm_cache_migration *mg = prealloc_get_migration(structs);
 
  mg->err = false;
+ mg->discard = false;
  mg->writeback = false;
  mg->demote = false;
  mg->promote = true;
@@ -1195,6 +1251,7 @@ static void writeback(struct cache *cache, struct prealloc *structs,
  struct dm_cache_migration *mg = prealloc_get_migration(structs);
 
  mg->err = false;
+ mg->discard = false;
  mg->writeback = true;
  mg->demote = false;
  mg->promote = false;
@@ -1220,6 +1277,7 @@ static void demote_then_promote(struct cache *cache, struct prealloc *structs,
  struct dm_cache_migration *mg = prealloc_get_migration(structs);
 
  mg->err = false;
+ mg->discard = false;
  mg->writeback = false;
  mg->demote = true;
  mg->promote = true;
@@ -1248,6 +1306,7 @@ static void invalidate(struct cache *cache, struct prealloc *structs,
  struct dm_cache_migration *mg = prealloc_get_migration(structs);
 
  mg->err = false;
+ mg->discard = false;
  mg->writeback = false;
  mg->demote = true;
  mg->promote = false;
@@ -1264,6 +1323,26 @@ static void invalidate(struct cache *cache, struct prealloc *structs,
  quiesce_migration(mg);
 }
 
+static void discard(struct cache *cache, struct prealloc *structs,
+      struct dm_bio_prison_cell *cell)
+{
+ struct dm_cache_migration *mg = prealloc_get_migration(structs);
+
+ mg->err = false;
+ mg->discard = true;
+ mg->writeback = false;
+ mg->demote = false;
+ mg->promote = false;
+ mg->requeue_holder = false;
+ mg->invalidate = false;
+ mg->cache = cache;
+ mg->old_ocell = NULL;
+ mg->new_ocell = cell;
+ mg->start_jiffies = jiffies;
+
+ quiesce_migration(mg);
+}
+
 /*----------------------------------------------------------------
  * bio processing
  *--------------------------------------------------------------*/
@@ -1297,31 +1376,27 @@ static void process_flush_bio(struct cache *cache, struct bio *bio)
  issue(cache, bio);
 }
 
-/*
- * People generally discard large parts of a device, eg, the whole device
- * when formatting.  Splitting these large discards up into cache block
- * sized ios and then quiescing (always neccessary for discard) takes too
- * long.
- *
- * We keep it simple, and allow any size of discard to come in, and just
- * mark off blocks on the discard bitset.  No passdown occurs!
- *
- * To implement passdown we need to change the bio_prison such that a cell
- * can have a key that spans many blocks.
- */
-static void process_discard_bio(struct cache *cache, struct bio *bio)
+static void process_discard_bio(struct cache *cache, struct prealloc *structs,
+    struct bio *bio)
 {
- dm_block_t start_block = dm_sector_div_up(bio->bi_sector,
-        cache->discard_block_size);
- dm_block_t end_block = bio->bi_sector + bio_sectors(bio);
- dm_block_t b;
+ int r;
+ dm_dblock_t b, e;
+ struct dm_bio_prison_cell *cell_prealloc, *new_ocell;
 
- end_block = block_div(end_block, cache->discard_block_size);
+ calc_discard_block_range(cache, bio, &b, &e);
+ if (b == e) {
+  bio_endio(bio, 0);
+  return;
+ }
 
- for (b = start_block; b < end_block; b++)
-  set_discard(cache, to_dblock(b));
+ cell_prealloc = prealloc_get_cell(structs);
+ r = bio_detain_range(cache, dblock_to_oblock(cache, b), dblock_to_oblock(cache, e), bio, cell_prealloc,
+        (cell_free_fn) prealloc_put_cell,
+        structs, &new_ocell);
+ if (r > 0)
+  return;
 
- bio_endio(bio, 0);
+ discard(cache, structs, new_ocell);
 }
 
 static bool spare_migration_bandwidth(struct cache *cache)
@@ -1511,7 +1586,7 @@ static void process_deferred_bios(struct cache *cache)
   if (bio->bi_rw & REQ_FLUSH)
    process_flush_bio(cache, bio);
   else if (bio->bi_rw & REQ_DISCARD)
-   process_discard_bio(cache, bio);
+   process_discard_bio(cache, &structs, bio);
   else
    process_bio(cache, &structs, bio);
  }
@@ -1726,7 +1801,7 @@ static void do_worker(struct work_struct *ws)
    process_invalidation_requests(cache);
   }
 
-  process_migrations(cache, &cache->quiesced_migrations, issue_copy);
+  process_migrations(cache, &cache->quiesced_migrations, issue_copy_or_discard);
   process_migrations(cache, &cache->completed_migrations, complete_migration);
 
   if (commit_if_needed(cache)) {
@@ -3129,7 +3204,8 @@ static void set_discard_limits(struct cache *cache, struct queue_limits *limits)
  /*
   * FIXME: these limits may be incompatible with the cache device
   */
- limits->max_discard_sectors = cache->discard_block_size * 1024;
+ limits->max_discard_sectors = min_t(sector_t, cache->discard_block_size * 1024,
+         cache->origin_sectors);
  limits->discard_granularity = cache->discard_block_size << SECTOR_SHIFT;
 }
 
@@ -3154,7 +3230,7 @@ static void cache_io_hints(struct dm_target *ti, struct queue_limits *limits)
 
 static struct target_type cache_target = {
  .name = "cache",
- .version = {1, 5, 0},
+ .version = {1, 6, 0},
  .module = THIS_MODULE,
  .ctr = cache_ctr,
  .dtr = cache_dtr,
-- 
1.7.1