From 4df51de60a18b1a6856d3f82f3916a101fa2a89f Mon Sep 17 00:00:00 2001
From: Aaron Tomlin <atomlin@redhat.com>
Date: Wed, 5 Nov 2014 09:42:32 -0500
Subject: [kernel] cpuset: PF_SPREAD_PAGE and PF_SPREAD_SLAB should be atomic flags

Message-id: <1415180552-10440-1-git-send-email-atomlin@redhat.com>
Patchwork-id: 99400
O-Subject: [RHEL7.1 PATCH] cpuset: PF_SPREAD_PAGE and PF_SPREAD_SLAB should be atomic flags
Bugzilla: 1160360
RH-Acked-by: Larry Woodman <lwoodman@redhat.com>
RH-Acked-by: Oleg Nesterov <oleg@redhat.com>
RH-Acked-by: Rik van Riel <riel@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1160360

Upstream Status:

  This is essentially a backport of linus commit 2ad654bc5e ("cpuset:
  PF_SPREAD_PAGE and PF_SPREAD_SLAB should be atomic flags") however
  this does not apply cleanly. Some of the missing dependencies are not
  actually applicable to the current RHEL7 code base.

  Most hunks applied cleanly apart from the following exceptions:

    * Adding the PFA_SPREAD_* macros to the new location
    * The change to and comment above __do_cache_alloc()

  These hunks were modified by hand since linus commit f0432d1596
  ("mm, mempolicy: remove per-process flag") has not been applied.

  PR_SET_NO_NEW_PRIVS has been introduced since
  Linux 3.5 - linus commit 259e5e6c75 ("Add PR_{GET,SET}_NO_NEW_PRIVS
  to prevent execve from granting privs"). Note this series does not
  aim to provide this functionality.

  Additional individual hunks were taken from the following commits:

    * 1d4457f99928a968767f6405b4a1f50845aa15fd
    * e0e5070b20e01f0321f97db4e4e174f3f6b49e50

Build Info: https://brewweb.devel.redhat.com/taskinfo?taskID=8195729

Tested: Compile tested on all architectures only

Commit 2ad654bc5e2b211e92f66da1d819e47d79a866f0
Author: Zefan Li <lizefan@huawei.com>
Date:   Thu Sep 25 09:41:02 2014 +0800

    When we change cpuset.memory_spread_{page,slab}, cpuset will flip
    PF_SPREAD_{PAGE,SLAB} bit of tsk->flags for each task in that cpuset.
    This should be done using atomic bitops, but currently we don't,
    which is broken.

    Tetsuo reported a hard-to-reproduce kernel crash on RHEL6, which happened
    when one thread tried to clear PF_USED_MATH while at the same time another
    thread tried to flip PF_SPREAD_PAGE/PF_SPREAD_SLAB. They both operate on
    the same task.

    Here's the full report:
    https://lkml.org/lkml/2014/9/19/230

    To fix this, we make PF_SPREAD_PAGE and PF_SPREAD_SLAB atomic flags.

    v4:
    - updated mm/slab.c. (Fengguang Wu)
    - updated Documentation.

    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: Kees Cook <keescook@chromium.org>
    Fixes: 950592f7b991 ("cpusets: update tasks' page/slab spread flags in time")
    Cc: <stable@vger.kernel.org> # 2.6.31+
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

Signed-off-by: Aaron Tomlin <atomlin@redhat.com>
Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/Documentation/cgroups/cpusets.txt b/Documentation/cgroups/cpusets.txt
index 12e01d4..09d58ea 100644
--- a/Documentation/cgroups/cpusets.txt
+++ b/Documentation/cgroups/cpusets.txt
@@ -345,14 +345,14 @@ the named feature on.
 The implementation is simple.
 
 Setting the flag 'cpuset.memory_spread_page' turns on a per-process flag
-PF_SPREAD_PAGE for each task that is in that cpuset or subsequently
+PFA_SPREAD_PAGE for each task that is in that cpuset or subsequently
 joins that cpuset.  The page allocation calls for the page cache
-is modified to perform an inline check for this PF_SPREAD_PAGE task
+is modified to perform an inline check for this PFA_SPREAD_PAGE task
 flag, and if set, a call to a new routine cpuset_mem_spread_node()
 returns the node to prefer for the allocation.
 
 Similarly, setting 'cpuset.memory_spread_slab' turns on the flag
-PF_SPREAD_SLAB, and appropriately marked slab caches will allocate
+PFA_SPREAD_SLAB, and appropriately marked slab caches will allocate
 pages from the node returned by cpuset_mem_spread_node().
 
 The cpuset_mem_spread_node() routine is also simple.  It uses the
diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index cc1b01c..f4550a9 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -72,12 +72,12 @@ extern int cpuset_slab_spread_node(void);
 
 static inline int cpuset_do_page_mem_spread(void)
 {
- return current->flags & PF_SPREAD_PAGE;
+ return task_spread_page(current);
 }
 
 static inline int cpuset_do_slab_mem_spread(void)
 {
- return current->flags & PF_SPREAD_SLAB;
+ return task_spread_slab(current);
 }
 
 extern int current_cpuset_is_being_rebound(void);
diff --git a/include/linux/sched.h b/include/linux/sched.h
index d64e720..aec7476 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1552,10 +1552,11 @@ struct task_struct {
 /* hung task detection */
  unsigned long last_switch_count;
 #endif
+ unsigned long atomic_flags;
 #else
  unsigned long rh_reserved1;
-#endif
  unsigned long rh_reserved2;
+#endif
  unsigned long rh_reserved3;
  unsigned long rh_reserved4;
  unsigned long rh_reserved5;
@@ -1821,8 +1822,6 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define PF_KTHREAD 0x00200000 /* I am a kernel thread */
 #define PF_RANDOMIZE 0x00400000 /* randomize virtual address space */
 #define PF_SWAPWRITE 0x00800000 /* Allowed to write to swap */
-#define PF_SPREAD_PAGE 0x01000000 /* Spread page cache over cpuset */
-#define PF_SPREAD_SLAB 0x02000000 /* Spread some slab caches over cpuset */
 #define PF_NO_SETAFFINITY 0x04000000 /* Userland is not allowed to meddle with cpus_allowed */
 #define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */
 #define PF_MEMPOLICY 0x10000000 /* Non-default NUMA mempolicy */
@@ -1855,6 +1854,28 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 #define used_math() tsk_used_math(current)
 
+/* Per-process atomic flags. */
+#define PFA_SPREAD_PAGE  1      /* Spread page cache over cpuset */
+#define PFA_SPREAD_SLAB  2      /* Spread some slab caches over cpuset */
+
+#define TASK_PFA_TEST(name, func)                                      \
+       static inline bool task_##func(struct task_struct *p)           \
+       { return test_bit(PFA_##name, &p->atomic_flags); }
+#define TASK_PFA_SET(name, func)                                       \
+       static inline void task_set_##func(struct task_struct *p)       \
+       { set_bit(PFA_##name, &p->atomic_flags); }
+#define TASK_PFA_CLEAR(name, func)                                     \
+       static inline void task_clear_##func(struct task_struct *p)     \
+       { clear_bit(PFA_##name, &p->atomic_flags); }
+
+TASK_PFA_TEST(SPREAD_PAGE, spread_page)
+TASK_PFA_SET(SPREAD_PAGE, spread_page)
+TASK_PFA_CLEAR(SPREAD_PAGE, spread_page)
+
+TASK_PFA_TEST(SPREAD_SLAB, spread_slab)
+TASK_PFA_SET(SPREAD_SLAB, spread_slab)
+TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
+
 /* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags */
 static inline gfp_t memalloc_noio_flags(gfp_t flags)
 {
diff --git a/kernel/cpuset.c b/kernel/cpuset.c
index d313870..37ee6fa 100644
--- a/kernel/cpuset.c
+++ b/kernel/cpuset.c
@@ -362,13 +362,13 @@ static void cpuset_update_task_spread_flag(struct cpuset *cs,
      struct task_struct *tsk)
 {
  if (is_spread_page(cs))
-  tsk->flags |= PF_SPREAD_PAGE;
+  task_set_spread_page(tsk);
  else
-  tsk->flags &= ~PF_SPREAD_PAGE;
+  task_clear_spread_page(tsk);
  if (is_spread_slab(cs))
-  tsk->flags |= PF_SPREAD_SLAB;
+  task_set_spread_slab(tsk);
  else
-  tsk->flags &= ~PF_SPREAD_SLAB;
+  task_clear_spread_slab(tsk);
 }
 
 /*
diff --git a/mm/slab.c b/mm/slab.c
index a1b22c7..5ce8374 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -3168,7 +3168,7 @@ out:
 
 #ifdef CONFIG_NUMA
 /*
- * Try allocating on another node if PF_SPREAD_SLAB|PF_MEMPOLICY.
+ * Try allocating on another node if PFA_SPREAD_SLAB|PF_MEMPOLICY.
  *
  * If we are in_interrupt, then process context, including cpusets and
  * mempolicy, may not apply and should not be used for allocation policy.
@@ -3412,7 +3412,8 @@ __do_cache_alloc(struct kmem_cache *cache, gfp_t flags)
 {
  void *objp;
 
- if (unlikely(current->flags & (PF_SPREAD_SLAB | PF_MEMPOLICY))) {
+ if (unlikely(current->flags & PF_MEMPOLICY ||
+   cpuset_do_slab_mem_spread())) {
   objp = alternate_node_alloc(cache, flags);
   if (objp)
    goto out;
-- 
1.7.1