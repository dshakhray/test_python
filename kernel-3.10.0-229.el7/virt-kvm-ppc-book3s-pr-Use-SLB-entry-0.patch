From 84944f772f1e06e4a051d378e57d820998cf8382 Mon Sep 17 00:00:00 2001
From: David Gibson <dgibson@redhat.com>
Date: Mon, 15 Sep 2014 07:13:21 -0400
Subject: [virt] kvm/ppc: book3s/pr - Use SLB entry 0

Message-id: <1410765214-16377-66-git-send-email-dgibson@redhat.com>
Patchwork-id: 94761
O-Subject: [PATCH 65/78] KVM: PPC: Book3S PR: Use SLB entry 0
Bugzilla: 1123145 1123133 1123367
RH-Acked-by: Radim Krcmar <rkrcmar@redhat.com>
RH-Acked-by: Don Zickus <dzickus@redhat.com>
RH-Acked-by: Steve Best <sbest@redhat.com>

Signed-off-by: David Gibson <dgibson@redhat.com>

commit 207438d4e21e05728a8a58b5e25b0f6553260068
Author: Alexander Graf <agraf@suse.de>
Date:   Thu May 15 14:36:05 2014 +0200

    KVM: PPC: Book3S PR: Use SLB entry 0

    We didn't make use of SLB entry 0 because ... of no good reason. SLB entry 0
    will always be used by the Linux linear SLB entry, so the fact that slbia
    does not invalidate it doesn't matter as we overwrite SLB 0 on exit anyway.

    Just enable use of SLB entry 0 for our shadow SLB code.

    Signed-off-by: Alexander Graf <agraf@suse.de>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/kvm/book3s_64_mmu_host.c b/arch/powerpc/kvm/book3s_64_mmu_host.c
index e2efb85..0ac9839 100644
--- a/arch/powerpc/kvm/book3s_64_mmu_host.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_host.c
@@ -271,11 +271,8 @@ static int kvmppc_mmu_next_segment(struct kvm_vcpu *vcpu, ulong esid)
  int found_inval = -1;
  int r;
 
- if (!svcpu->slb_max)
-  svcpu->slb_max = 1;
-
  /* Are we overwriting? */
- for (i = 1; i < svcpu->slb_max; i++) {
+ for (i = 0; i < svcpu->slb_max; i++) {
   if (!(svcpu->slb[i].esid & SLB_ESID_V))
    found_inval = i;
   else if ((svcpu->slb[i].esid & ESID_MASK) == esid) {
@@ -285,7 +282,7 @@ static int kvmppc_mmu_next_segment(struct kvm_vcpu *vcpu, ulong esid)
  }
 
  /* Found a spare entry that was invalidated before */
- if (found_inval > 0) {
+ if (found_inval >= 0) {
   r = found_inval;
   goto out;
  }
@@ -359,7 +356,7 @@ void kvmppc_mmu_flush_segment(struct kvm_vcpu *vcpu, ulong ea, ulong seg_size)
  ulong seg_mask = -seg_size;
  int i;
 
- for (i = 1; i < svcpu->slb_max; i++) {
+ for (i = 0; i < svcpu->slb_max; i++) {
   if ((svcpu->slb[i].esid & SLB_ESID_V) &&
       (svcpu->slb[i].esid & seg_mask) == ea) {
    /* Invalidate this entry */
@@ -373,7 +370,7 @@ void kvmppc_mmu_flush_segment(struct kvm_vcpu *vcpu, ulong ea, ulong seg_size)
 void kvmppc_mmu_flush_segments(struct kvm_vcpu *vcpu)
 {
  struct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);
- svcpu->slb_max = 1;
+ svcpu->slb_max = 0;
  svcpu->slb[0].esid = 0;
  svcpu_put(svcpu);
 }
diff --git a/arch/powerpc/kvm/book3s_64_slb.S b/arch/powerpc/kvm/book3s_64_slb.S
index 4f12e8f..b57d09a 100644
--- a/arch/powerpc/kvm/book3s_64_slb.S
+++ b/arch/powerpc/kvm/book3s_64_slb.S
@@ -139,7 +139,8 @@ slb_do_enter:
 
  /* Restore bolted entries from the shadow and fix it along the way */
 
- /* We don't store anything in entry 0, so we don't need to take care of it */
+ li r0, r0
+ slbmte r0, r0
  slbia
  isync
 
-- 
1.7.1