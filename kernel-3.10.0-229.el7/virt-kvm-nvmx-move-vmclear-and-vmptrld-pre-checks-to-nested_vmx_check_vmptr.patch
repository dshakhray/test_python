From b3a16a24430ff22ff2848b1e84a3f53024fd6eed Mon Sep 17 00:00:00 2001
From: Paolo Bonzini <pbonzini@redhat.com>
Date: Tue, 22 Jul 2014 14:39:51 -0400
Subject: [virt] kvm/nvmx: move vmclear and vmptrld pre-checks to nested_vmx_check_vmptr

Message-id: <1406040016-3289-197-git-send-email-pbonzini@redhat.com>
Patchwork-id: 86275
O-Subject: [RHEL7 PATCH v2 196/221] KVM: nVMX: move vmclear and vmptrld pre-checks to nested_vmx_check_vmptr
Bugzilla: 1116936
RH-Acked-by: Radim Krcmar <rkrcmar@redhat.com>
RH-Acked-by: Bandan Das <bsd@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

From: Bandan Das <bsd@redhat.com>

Some checks are common to all, and moreover,
according to the spec, the check for whether any bits
beyond the physical address width are set are also
applicable to all of them

Signed-off-by: Bandan Das <bsd@redhat.com>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 4291b58885f5af560488a5b9667ca6930b9fdc3d)
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 1d7e727..a5fd47e 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -5850,8 +5850,10 @@ static int get_vmx_mem_address(struct kvm_vcpu *vcpu,
  * - if it's 4KB aligned
  * - No bits beyond the physical address width are set
  * - Returns 0 on success or else 1
+ * (Intel SDM Section 30.3)
  */
-static int nested_vmx_check_vmptr(struct kvm_vcpu *vcpu, int exit_reason)
+static int nested_vmx_check_vmptr(struct kvm_vcpu *vcpu, int exit_reason,
+      gpa_t *vmpointer)
 {
  gva_t gva;
  gpa_t vmptr;
@@ -5899,11 +5901,42 @@ static int nested_vmx_check_vmptr(struct kvm_vcpu *vcpu, int exit_reason)
   kunmap(page);
   vmx->nested.vmxon_ptr = vmptr;
   break;
+ case EXIT_REASON_VMCLEAR:
+  if (!IS_ALIGNED(vmptr, PAGE_SIZE) || (vmptr >> maxphyaddr)) {
+   nested_vmx_failValid(vcpu,
+          VMXERR_VMCLEAR_INVALID_ADDRESS);
+   skip_emulated_instruction(vcpu);
+   return 1;
+  }
 
+  if (vmptr == vmx->nested.vmxon_ptr) {
+   nested_vmx_failValid(vcpu,
+          VMXERR_VMCLEAR_VMXON_POINTER);
+   skip_emulated_instruction(vcpu);
+   return 1;
+  }
+  break;
+ case EXIT_REASON_VMPTRLD:
+  if (!IS_ALIGNED(vmptr, PAGE_SIZE) || (vmptr >> maxphyaddr)) {
+   nested_vmx_failValid(vcpu,
+          VMXERR_VMPTRLD_INVALID_ADDRESS);
+   skip_emulated_instruction(vcpu);
+   return 1;
+  }
+
+  if (vmptr == vmx->nested.vmxon_ptr) {
+   nested_vmx_failValid(vcpu,
+          VMXERR_VMCLEAR_VMXON_POINTER);
+   skip_emulated_instruction(vcpu);
+   return 1;
+  }
+  break;
  default:
   return 1; /* shouldn't happen */
  }
 
+ if (vmpointer)
+  *vmpointer = vmptr;
  return 0;
 }
 
@@ -5946,7 +5979,7 @@ static int handle_vmon(struct kvm_vcpu *vcpu)
   return 1;
  }
 
- if (nested_vmx_check_vmptr(vcpu, EXIT_REASON_VMON))
+ if (nested_vmx_check_vmptr(vcpu, EXIT_REASON_VMON, NULL))
   return 1;
 
  if (vmx->nested.vmxon) {
@@ -6075,37 +6108,16 @@ static int handle_vmoff(struct kvm_vcpu *vcpu)
 static int handle_vmclear(struct kvm_vcpu *vcpu)
 {
  struct vcpu_vmx *vmx = to_vmx(vcpu);
- gva_t gva;
  gpa_t vmptr;
  struct vmcs12 *vmcs12;
  struct page *page;
- struct x86_exception e;
 
  if (!nested_vmx_check_permission(vcpu))
   return 1;
 
- if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
-   vmcs_read32(VMX_INSTRUCTION_INFO), &gva))
+ if (nested_vmx_check_vmptr(vcpu, EXIT_REASON_VMCLEAR, &vmptr))
   return 1;
 
- if (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &vmptr,
-    sizeof(vmptr), &e)) {
-  kvm_inject_page_fault(vcpu, &e);
-  return 1;
- }
-
- if (!IS_ALIGNED(vmptr, PAGE_SIZE)) {
-  nested_vmx_failValid(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);
-  skip_emulated_instruction(vcpu);
-  return 1;
- }
-
- if (vmptr == vmx->nested.vmxon_ptr) {
-  nested_vmx_failValid(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);
-  skip_emulated_instruction(vcpu);
-  return 1;
- }
-
  if (vmptr == vmx->nested.current_vmptr) {
   nested_release_vmcs12(vmx);
   vmx->nested.current_vmptr = -1ull;
@@ -6425,35 +6437,14 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 static int handle_vmptrld(struct kvm_vcpu *vcpu)
 {
  struct vcpu_vmx *vmx = to_vmx(vcpu);
- gva_t gva;
  gpa_t vmptr;
- struct x86_exception e;
  u32 exec_control;
 
  if (!nested_vmx_check_permission(vcpu))
   return 1;
 
- if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
-   vmcs_read32(VMX_INSTRUCTION_INFO), &gva))
-  return 1;
-
- if (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &vmptr,
-    sizeof(vmptr), &e)) {
-  kvm_inject_page_fault(vcpu, &e);
-  return 1;
- }
-
- if (!IS_ALIGNED(vmptr, PAGE_SIZE)) {
-  nested_vmx_failValid(vcpu, VMXERR_VMPTRLD_INVALID_ADDRESS);
-  skip_emulated_instruction(vcpu);
+ if (nested_vmx_check_vmptr(vcpu, EXIT_REASON_VMPTRLD, &vmptr))
   return 1;
- }
-
- if (vmptr == vmx->nested.vmxon_ptr) {
-  nested_vmx_failValid(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);
-  skip_emulated_instruction(vcpu);
-  return 1;
- }
 
  if (vmx->nested.current_vmptr != vmptr) {
   struct vmcs12 *new_vmcs12;
-- 
1.7.1