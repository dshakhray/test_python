From c8e46ae0872d55e32ec35977c8a121748c8d7feb Mon Sep 17 00:00:00 2001
From: John Greene <jogreene@redhat.com>
Date: Wed, 10 Sep 2014 18:28:23 -0400
Subject: [ethernet] ixgbe: fix qv_lock_napi call in ixgbe_napi_disable_all

Message-id: <1410373750-11341-4-git-send-email-jogreene@redhat.com>
Patchwork-id: 93513
O-Subject: [RHEL7.1 PATCH 03/50] ixgbe: fix qv_lock_napi call in ixgbe_napi_disable_all
Bugzilla: 1091123
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Stefan Assmann <sassmann@redhat.com>
RH-Acked-by: Neil Horman <nhorman@redhat.com>

BZ: 1091123
Brew: https://brewweb.devel.redhat.com/taskinfo?taskID=7919168
Tested: In beaker. See BZ for details

ixgbe_napi_disable_all calls napi_disable on each queue, however the busy
polling code introduced a local_bh_disable()d context around the napi_disable.
The original author did not realize that napi_disable might sleep, which would
cause a sleep while atomic BUG. In addition, on a single processor system, the
ixgbe_qv_lock_napi loop shouldn't have to mdelay. This patch adds an
ixgbe_qv_disable along with a new IXGBE_QV_STATE_DISABLED bit, which it uses to
indicate to the poll and napi routines that the q_vector has been disabled. Now
the ixgbe_napi_disable_all function will wait until all pending work has been
finished and prevent any future work from being started.

Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
Cc: Eliezer Tamir <eliezer.tamir@linux.intel.com>
Cc: Alexander Duyck <alexander.duyck@intel.com>
Cc: Hyong-Youb Kim <hykim@myri.com>
Cc: Amir Vadai <amirv@mellanox.com>
Cc: Dmitry Kravkov <dmitry@broadcom.com>
Tested-by: Phil Schmitt <phillip.j.schmitt@intel.com>
Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 27d9ce4fd0e2e75c2907f6d3dc0487012a3e4298)
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe.h b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
index 8d57352..d44d2df 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe.h
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
@@ -373,11 +373,13 @@ struct ixgbe_q_vector {
 #ifdef CONFIG_NET_RX_BUSY_POLL
  unsigned int state;
 #define IXGBE_QV_STATE_IDLE        0
-#define IXGBE_QV_STATE_NAPI    1    /* NAPI owns this QV */
-#define IXGBE_QV_STATE_POLL    2    /* poll owns this QV */
-#define IXGBE_QV_LOCKED (IXGBE_QV_STATE_NAPI | IXGBE_QV_STATE_POLL)
-#define IXGBE_QV_STATE_NAPI_YIELD  4    /* NAPI yielded this QV */
-#define IXGBE_QV_STATE_POLL_YIELD  8    /* poll yielded this QV */
+#define IXGBE_QV_STATE_NAPI    1     /* NAPI owns this QV */
+#define IXGBE_QV_STATE_POLL    2     /* poll owns this QV */
+#define IXGBE_QV_STATE_DISABLED    4     /* QV is disabled */
+#define IXGBE_QV_OWNED (IXGBE_QV_STATE_NAPI | IXGBE_QV_STATE_POLL)
+#define IXGBE_QV_LOCKED (IXGBE_QV_OWNED | IXGBE_QV_STATE_DISABLED)
+#define IXGBE_QV_STATE_NAPI_YIELD  8     /* NAPI yielded this QV */
+#define IXGBE_QV_STATE_POLL_YIELD  16    /* poll yielded this QV */
 #define IXGBE_QV_YIELD (IXGBE_QV_STATE_NAPI_YIELD | IXGBE_QV_STATE_POLL_YIELD)
 #define IXGBE_QV_USER_PEND (IXGBE_QV_STATE_POLL | IXGBE_QV_STATE_POLL_YIELD)
  spinlock_t lock;
@@ -398,7 +400,7 @@ static inline void ixgbe_qv_init_lock(struct ixgbe_q_vector *q_vector)
 static inline bool ixgbe_qv_lock_napi(struct ixgbe_q_vector *q_vector)
 {
  int rc = true;
- spin_lock(&q_vector->lock);
+ spin_lock_bh(&q_vector->lock);
  if (q_vector->state & IXGBE_QV_LOCKED) {
   WARN_ON(q_vector->state & IXGBE_QV_STATE_NAPI);
   q_vector->state |= IXGBE_QV_STATE_NAPI_YIELD;
@@ -409,7 +411,7 @@ static inline bool ixgbe_qv_lock_napi(struct ixgbe_q_vector *q_vector)
  } else
   /* we don't care if someone yielded */
   q_vector->state = IXGBE_QV_STATE_NAPI;
- spin_unlock(&q_vector->lock);
+ spin_unlock_bh(&q_vector->lock);
  return rc;
 }
 
@@ -417,14 +419,15 @@ static inline bool ixgbe_qv_lock_napi(struct ixgbe_q_vector *q_vector)
 static inline bool ixgbe_qv_unlock_napi(struct ixgbe_q_vector *q_vector)
 {
  int rc = false;
- spin_lock(&q_vector->lock);
+ spin_lock_bh(&q_vector->lock);
  WARN_ON(q_vector->state & (IXGBE_QV_STATE_POLL |
           IXGBE_QV_STATE_NAPI_YIELD));
 
  if (q_vector->state & IXGBE_QV_STATE_POLL_YIELD)
   rc = true;
- q_vector->state = IXGBE_QV_STATE_IDLE;
- spin_unlock(&q_vector->lock);
+ /* will reset state to idle, unless QV is disabled */
+ q_vector->state &= IXGBE_QV_STATE_DISABLED;
+ spin_unlock_bh(&q_vector->lock);
  return rc;
 }
 
@@ -455,7 +458,8 @@ static inline bool ixgbe_qv_unlock_poll(struct ixgbe_q_vector *q_vector)
 
  if (q_vector->state & IXGBE_QV_STATE_POLL_YIELD)
   rc = true;
- q_vector->state = IXGBE_QV_STATE_IDLE;
+ /* will reset state to idle, unless QV is disabled */
+ q_vector->state &= IXGBE_QV_STATE_DISABLED;
  spin_unlock_bh(&q_vector->lock);
  return rc;
 }
@@ -463,9 +467,23 @@ static inline bool ixgbe_qv_unlock_poll(struct ixgbe_q_vector *q_vector)
 /* true if a socket is polling, even if it did not get the lock */
 static inline bool ixgbe_qv_ll_polling(struct ixgbe_q_vector *q_vector)
 {
- WARN_ON(!(q_vector->state & IXGBE_QV_LOCKED));
+ WARN_ON(!(q_vector->state & IXGBE_QV_OWNED));
  return q_vector->state & IXGBE_QV_USER_PEND;
 }
+
+/* false if QV is currently owned */
+static inline bool ixgbe_qv_disable(struct ixgbe_q_vector *q_vector)
+{
+ int rc = true;
+ spin_lock_bh(&q_vector->lock);
+ if (q_vector->state & IXGBE_QV_OWNED)
+  rc = false;
+ q_vector->state |= IXGBE_QV_STATE_DISABLED;
+ spin_unlock_bh(&q_vector->lock);
+
+ return rc;
+}
+
 #else /* CONFIG_NET_RX_BUSY_POLL */
 static inline void ixgbe_qv_init_lock(struct ixgbe_q_vector *q_vector)
 {
@@ -495,6 +513,12 @@ static inline bool ixgbe_qv_ll_polling(struct ixgbe_q_vector *q_vector)
 {
  return false;
 }
+
+static inline bool ixgbe_qv_disable(struct ixgbe_q_vector *q_vector)
+{
+ return true;
+}
+
 #endif /* CONFIG_NET_RX_BUSY_POLL */
 
 #ifdef CONFIG_IXGBE_HWMON
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index 26ea537..b7d4346 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -3886,15 +3886,13 @@ static void ixgbe_napi_disable_all(struct ixgbe_adapter *adapter)
 {
  int q_idx;
 
- local_bh_disable(); /* for ixgbe_qv_lock_napi() */
  for (q_idx = 0; q_idx < adapter->num_q_vectors; q_idx++) {
   napi_disable(&adapter->q_vector[q_idx]->napi);
-  while (!ixgbe_qv_lock_napi(adapter->q_vector[q_idx])) {
+  while (!ixgbe_qv_disable(adapter->q_vector[q_idx])) {
    pr_info("QV %d locked\n", q_idx);
-   mdelay(1);
+   usleep_range(1000, 20000);
   }
  }
- local_bh_enable();
 }
 
 #ifdef CONFIG_IXGBE_DCB
-- 
1.7.1