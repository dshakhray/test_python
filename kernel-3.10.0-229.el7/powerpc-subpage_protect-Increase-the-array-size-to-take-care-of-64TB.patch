From 3f977fb1d3c9b1cbd6071673ea35a863b7260c88 Mon Sep 17 00:00:00 2001
From: Don Zickus <dzickus@redhat.com>
Date: Fri, 12 Sep 2014 18:13:49 -0400
Subject: [powerpc] subpage_protect: Increase the array size to take care of 64TB

Message-id: <1410545655-205645-601-git-send-email-dzickus@redhat.com>
Patchwork-id: 94475
O-Subject: [RHEL7 PATCH 600/626] powerpc: subpage_protect: Increase the array size to take care of 64TB
Bugzilla: 1127366
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Steve Best <sbest@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>
RH-Acked-by: David Gibson <dgibson@redhat.com>

https://bugzilla.redhat.com/show_bug.cgi?id=1127366

commit dad6f37c2602e4af6c3aecfdb41f2d8bd4668163
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 15 20:22:30 2014 +0530

    powerpc: subpage_protect: Increase the array size to take care of 64TB

    We now support TASK_SIZE of 16TB, hence the array should be 8.

    Fixes the below crash:

    Unable to handle kernel paging request for data at address 0x000100bd
    Faulting instruction address: 0xc00000000004f914
    cpu 0x13: Vector: 300 (Data Access) at [c000000fea75fa90]
        pc: c00000000004f914: .sys_subpage_prot+0x2d4/0x5c0
        lr: c00000000004fb5c: .sys_subpage_prot+0x51c/0x5c0
        sp: c000000fea75fd10
       msr: 9000000000009032
       dar: 100bd
     dsisr: 40000000
      current = 0xc000000fea6ae490
      paca    = 0xc00000000fb8ab00   softe: 0        irq_happened: 0x00
        pid   = 8237, comm = a.out
    enter ? for help
    [c000000fea75fe30] c00000000000a164 syscall_exit+0x0/0x98

    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

kabi reason: The protptrs array was original 16 bytes and embedded in
struct mm.  It was grown to 64 bytes with this patch which changes
all the offsets in struct mm.  To fix that I created a new pointer
inside the old array and dynamically created memory for the enlarged
array.

As a result, kabi is worked around through an extra hop.  No drivers
are expected to use this array, so renaming the element in the struct
is safe.
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/include/asm/mmu-hash64.h b/arch/powerpc/include/asm/mmu-hash64.h
index 807014d..31411e6 100644
--- a/arch/powerpc/include/asm/mmu-hash64.h
+++ b/arch/powerpc/include/asm/mmu-hash64.h
@@ -22,6 +22,7 @@
  */
 #include <asm/pgtable-ppc64.h>
 #include <asm/bug.h>
+#include <asm/processor.h>
 
 /*
  * Segment table
@@ -484,6 +485,18 @@ extern void slb_set_size(u16 size);
 
 #ifdef CONFIG_PPC_SUBPAGE_PROT
 /*
+ * RH KABI restrictions limits protptrs to 2 32-bit pointers, but
+ * powerpc64 needs 8 to support 64TB of memory.  Workaround this
+ * by implementing another level of indirection to jump through
+ * to get to the real protptr.  Unfortunately, this will require
+ * allocating memory dynamically instead of statically like the
+ * original structs were.
+ */
+struct protptrs_kabi {
+ unsigned int **protptrs[(TASK_SIZE_USER64 >> 43)];
+};
+
+/*
  * For the sub-page protection option, we extend the PGD with one of
  * these.  Basically we have a 3-level tree, with the top level being
  * the protptrs array.  To optimize speed and memory consumption when
@@ -496,7 +509,12 @@ extern void slb_set_size(u16 size);
  */
 struct subpage_prot_table {
  unsigned long maxaddr; /* only addresses < this are protected */
+#ifdef __GENKSYMS__
  unsigned int **protptrs[2];
+#else
+ struct protptrs_kabi *rh_kabi;
+ void *unused;
+#endif
  unsigned int *low_prot[4];
 };
 
diff --git a/arch/powerpc/mm/hash_utils_64.c b/arch/powerpc/mm/hash_utils_64.c
index b435e22..96df393 100644
--- a/arch/powerpc/mm/hash_utils_64.c
+++ b/arch/powerpc/mm/hash_utils_64.c
@@ -933,13 +933,16 @@ static int subpage_protection(struct mm_struct *mm, unsigned long ea)
  u32 spp = 0;
  u32 **sbpm, *sbpp;
 
+ if (!spt->rh_kabi)
+  return 0;
+
  if (ea >= spt->maxaddr)
   return 0;
  if (ea < 0x100000000UL) {
   /* addresses below 4GB use spt->low_prot */
   sbpm = spt->low_prot;
  } else {
-  sbpm = spt->protptrs[ea >> SBP_L3_SHIFT];
+  sbpm = spt->rh_kabi->protptrs[ea >> SBP_L3_SHIFT];
   if (!sbpm)
    return 0;
  }
diff --git a/arch/powerpc/mm/subpage-prot.c b/arch/powerpc/mm/subpage-prot.c
index 6c0b1f5..fc12bf5 100644
--- a/arch/powerpc/mm/subpage-prot.c
+++ b/arch/powerpc/mm/subpage-prot.c
@@ -19,6 +19,18 @@
 #include <asm/tlbflush.h>
 
 /*
+ *  * Allocate memory for an auxillary struct to workaround kabi
+ *   */
+struct protptrs_kabi *subpage_prot_alloc_kabi(void)
+{
+ struct protptrs_kabi *p;
+
+ p = kzalloc(sizeof(*p), GFP_KERNEL);
+
+ return p;
+}
+
+/*
  * Free all pages allocated for subpage protection maps and pointers.
  * Also makes sure that the subpage_prot_table structure is
  * reinitialized for the next user.
@@ -35,18 +47,27 @@ void subpage_prot_free(struct mm_struct *mm)
    spt->low_prot[i] = NULL;
   }
  }
+
+ if (!spt->rh_kabi) {
+  /* no need to allocate, just skip free'ing pages */
+  goto next;
+ }
+
  addr = 0;
  for (i = 0; i < 2; ++i) {
-  p = spt->protptrs[i];
+  p = spt->rh_kabi->protptrs[i];
   if (!p)
    continue;
-  spt->protptrs[i] = NULL;
+  spt->rh_kabi->protptrs[i] = NULL;
   for (j = 0; j < SBP_L2_COUNT && addr < spt->maxaddr;
        ++j, addr += PAGE_SIZE)
    if (p[j])
     free_page((unsigned long)p[j]);
   free_page((unsigned long)p);
  }
+ kfree(spt->rh_kabi);
+
+next:
  spt->maxaddr = 0;
 }
 
@@ -99,6 +120,11 @@ static void subpage_prot_clear(unsigned long addr, unsigned long len)
  size_t nw;
  unsigned long next, limit;
 
+ if (!spt->rh_kabi) {
+  spt->rh_kabi = subpage_prot_alloc_kabi();
+  /* can't return failure here, deal with it below */
+ }
+
  down_write(&mm->mmap_sem);
  limit = addr + len;
  if (limit > spt->maxaddr)
@@ -108,7 +134,9 @@ static void subpage_prot_clear(unsigned long addr, unsigned long len)
   if (addr < 0x100000000UL) {
    spm = spt->low_prot;
   } else {
-   spm = spt->protptrs[addr >> SBP_L3_SHIFT];
+   if (!spt->rh_kabi)
+    continue;
+   spm = spt->rh_kabi->protptrs[addr >> SBP_L3_SHIFT];
    if (!spm)
     continue;
   }
@@ -205,6 +233,12 @@ long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)
  if (is_hugepage_only_range(mm, addr, len))
   return -EINVAL;
 
+ if (!spt->rh_kabi) {
+  spt->rh_kabi = subpage_prot_alloc_kabi();
+  if (!spt->rh_kabi)
+   return -ENOMEM;
+ }
+
  if (!map) {
   /* Clear out the protection map for the address range */
   subpage_prot_clear(addr, len);
@@ -222,12 +256,12 @@ long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)
   if (addr < 0x100000000UL) {
    spm = spt->low_prot;
   } else {
-   spm = spt->protptrs[addr >> SBP_L3_SHIFT];
+   spm = spt->rh_kabi->protptrs[addr >> SBP_L3_SHIFT];
    if (!spm) {
     spm = (u32 **)get_zeroed_page(GFP_KERNEL);
     if (!spm)
      goto out;
-    spt->protptrs[addr >> SBP_L3_SHIFT] = spm;
+    spt->rh_kabi->protptrs[addr >> SBP_L3_SHIFT] = spm;
    }
   }
   spm += (addr >> SBP_L2_SHIFT) & (SBP_L2_COUNT - 1);
-- 
1.7.1