From 6c4db7ebd8c2b42079b26bb5e38a2544a8d5eb34 Mon Sep 17 00:00:00 2001
From: Paolo Bonzini <pbonzini@redhat.com>
Date: Tue, 22 Jul 2014 14:38:00 -0400
Subject: [virt] kvm/nvmx: Amend nested_run_pending logic

Message-id: <1406040016-3289-86-git-send-email-pbonzini@redhat.com>
Patchwork-id: 86164
O-Subject: [RHEL7 PATCH v2 085/221] KVM: nVMX: Amend nested_run_pending logic
Bugzilla: 1116936
RH-Acked-by: Radim Krcmar <rkrcmar@redhat.com>
RH-Acked-by: Bandan Das <bsd@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

From: Gleb Natapov <gleb@redhat.com>

EXIT_REASON_VMLAUNCH/EXIT_REASON_VMRESUME exit does not mean that nested
VM will actually run during next entry. Move setting nested_run_pending
closer to vmentry emulation code and move its clearing close to vmexit to
minimize amount of code that will erroneously run with nested_run_pending
set.

Signed-off-by: Gleb Natapov <gleb@redhat.com>
Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit e0b890d35cdff506b72129b55a166d22fc9bfa64)
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 46bc83c..11fb251 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -6748,20 +6748,6 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu)
  if (vmx->emulation_required)
   return handle_invalid_guest_state(vcpu);
 
- /*
-  * the KVM_REQ_EVENT optimization bit is only on for one entry, and if
-  * we did not inject a still-pending event to L1 now because of
-  * nested_run_pending, we need to re-enable this bit.
-  */
- if (vmx->nested.nested_run_pending)
-  kvm_make_request(KVM_REQ_EVENT, vcpu);
-
- if (!is_guest_mode(vcpu) && (exit_reason == EXIT_REASON_VMLAUNCH ||
-     exit_reason == EXIT_REASON_VMRESUME))
-  vmx->nested.nested_run_pending = 1;
- else
-  vmx->nested.nested_run_pending = 0;
-
  if (is_guest_mode(vcpu) && nested_vmx_exit_handled(vcpu)) {
   nested_vmx_vmexit(vcpu);
   return 1;
@@ -7296,6 +7282,16 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
  vmx->exit_reason = vmcs_read32(VM_EXIT_REASON);
  trace_kvm_exit(vmx->exit_reason, vcpu, KVM_ISA_VMX);
 
+ /*
+  * the KVM_REQ_EVENT optimization bit is only on for one entry, and if
+  * we did not inject a still-pending event to L1 now because of
+  * nested_run_pending, we need to re-enable this bit.
+  */
+ if (vmx->nested.nested_run_pending)
+  kvm_make_request(KVM_REQ_EVENT, vcpu);
+
+ vmx->nested.nested_run_pending = 0;
+
  vmx_complete_atomic_exit(vmx);
  vmx_recover_nmi_blocking(vmx);
  vmx_complete_interrupts(vmx);
@@ -7949,6 +7945,8 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 
  enter_guest_mode(vcpu);
 
+ vmx->nested.nested_run_pending = 1;
+
  vmx->nested.vmcs01_tsc_offset = vmcs_read64(TSC_OFFSET);
 
  cpu = get_cpu();
-- 
1.7.1