From a68bb7aabb595bdf95433066cdba00d845326a9f Mon Sep 17 00:00:00 2001
From: Mike Snitzer <snitzer@redhat.com>
Date: Thu, 23 Oct 2014 20:01:13 -0400
Subject: [md] dm-bufio: when done scanning return from __scan immediately

Message-id: <1414094494-22068-4-git-send-email-snitzer@redhat.com>
Patchwork-id: 98337
O-Subject: [RHEL7.2 PATCH 03/24] dm bufio: when done scanning return from __scan immediately
Bugzilla: 1156161
RH-Acked-by: Heinz Mauelshagen <heinzm@redhat.com>
RH-Acked-by: Joe Thornber <thornber@redhat.com>

BZ: 1156161

RHEL7 doesn't have upstream's shrinker API so needed to adjust this
patch accordingly.  Inverted __cleanup_old_buffer's return so that it
matches upstream to ease the backport of "dm bufio: evict buffers that
are past the max age but retain some buffers".

Upstream commit 0e825862f3c04cee40e25f55680333728a4ffa9b
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Wed Oct 1 13:29:48 2014 -0400

    dm bufio: when done scanning return from __scan immediately

    When __scan frees the required number of buffer entries that the
    shrinker requested (nr_to_scan becomes zero) it must return.  Before
    this fix the __scan code exited only the inner loop and continued in the
    outer loop -- which could result in reduced performance due to extra
    buffers being freed (e.g. unnecessarily evicted thinp metadata needing
    to be synchronously re-read into bufio's cache).

    Also, move dm_bufio_cond_resched to __scan's inner loop, so that
    iterating the bufio client's lru lists doesn't result in scheduling
    latency.

    Reported-by: Joe Thornber <thornber@redhat.com>
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Cc: stable@vger.kernel.org # 3.2+
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/md/dm-bufio.c b/drivers/md/dm-bufio.c
index 34a4845..148d46f 100644
--- a/drivers/md/dm-bufio.c
+++ b/drivers/md/dm-bufio.c
@@ -1456,23 +1456,23 @@ static int __cleanup_old_buffer(struct dm_buffer *b, gfp_t gfp,
     unsigned long max_jiffies)
 {
  if (jiffies - b->last_accessed < max_jiffies)
-  return 1;
+  return 0;
 
  if (!(gfp & __GFP_IO)) {
   if (test_bit(B_READING, &b->state) ||
       test_bit(B_WRITING, &b->state) ||
       test_bit(B_DIRTY, &b->state))
-   return 1;
+   return 0;
  }
 
  if (b->hold_count)
-  return 1;
+  return 0;
 
  __make_buffer_clean(b);
  __unlink_buffer(b);
  __free_buffer_wake(b);
 
- return 0;
+ return 1;
 }
 
 static void __scan(struct dm_bufio_client *c, unsigned long nr_to_scan,
@@ -1480,13 +1480,15 @@ static void __scan(struct dm_bufio_client *c, unsigned long nr_to_scan,
 {
  int l;
  struct dm_buffer *b, *tmp;
+ long freed = 0;
 
  for (l = 0; l < LIST_SIZE; l++) {
-  list_for_each_entry_safe_reverse(b, tmp, &c->lru[l], lru_list)
-   if (!__cleanup_old_buffer(b, sc->gfp_mask, 0) &&
-       !--nr_to_scan)
+  list_for_each_entry_safe_reverse(b, tmp, &c->lru[l], lru_list) {
+   freed += __cleanup_old_buffer(b, sc->gfp_mask, 0);
+   if (!--nr_to_scan)
     return;
-  dm_bufio_cond_resched();
+   dm_bufio_cond_resched();
+  }
  }
 }
 
@@ -1702,7 +1704,7 @@ static void cleanup_old_buffers(void)
    struct dm_buffer *b;
    b = list_entry(c->lru[LIST_CLEAN].prev,
            struct dm_buffer, lru_list);
-   if (__cleanup_old_buffer(b, 0, max_age * HZ))
+   if (!__cleanup_old_buffer(b, 0, max_age * HZ))
     break;
    dm_bufio_cond_resched();
   }
-- 
1.7.1