From 5abeb1997611500b2cec813d09af118e73927d82 Mon Sep 17 00:00:00 2001
From: Mike Snitzer <snitzer@redhat.com>
Date: Wed, 16 Jul 2014 20:42:22 -0400
Subject: [md] dm-thin: switch to an atomic_t for tracking pending new block preparations

Message-id: <1405543364-20901-19-git-send-email-snitzer@redhat.com>
Patchwork-id: 85902
O-Subject: [RHEL7.1 PATCH 18/40] dm thin: switch to an atomic_t for tracking pending new block preparations
Bugzilla: 1065474
RH-Acked-by: Mikulas Patocka <mpatocka@redhat.com>
RH-Acked-by: Heinz Mauelshagen <heinzm@redhat.com>
RH-Acked-by: Joe Thornber <thornber@redhat.com>

BZ: 1065474

Upstream linux-dm.git commit d599cb6958a770b2ea362accc1a22a3071da28f5
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Jun 13 13:57:09 2014 +0100

    dm thin: switch to an atomic_t for tracking pending new block preparations

    Previously we used separate boolean values to track quiescing and
    copying actions.  By switching to an atomic_t we can support blocks that
    need a partial copy and partial zero.

    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/md/dm-thin.c b/drivers/md/dm-thin.c
index 6cc0cf0..49a9954 100644
--- a/drivers/md/dm-thin.c
+++ b/drivers/md/dm-thin.c
@@ -553,11 +553,16 @@ static void remap_and_issue(struct thin_c *tc, struct bio *bio,
 struct dm_thin_new_mapping {
  struct list_head list;
 
- bool quiesced:1;
- bool prepared:1;
  bool pass_discard:1;
  bool definitely_not_shared:1;
 
+ /*
+  * Track quiescing, copying and zeroing preparation actions.  When this
+  * counter hits zero the block is prepared and can be inserted into the
+  * btree.
+  */
+ atomic_t prepare_actions;
+
  int err;
  struct thin_c *tc;
  dm_block_t virt_block;
@@ -574,11 +579,11 @@ struct dm_thin_new_mapping {
  bio_end_io_t *saved_bi_end_io;
 };
 
-static void __maybe_add_mapping(struct dm_thin_new_mapping *m)
+static void __complete_mapping_preparation(struct dm_thin_new_mapping *m)
 {
  struct pool *pool = m->tc->pool;
 
- if (m->quiesced && m->prepared) {
+ if (atomic_dec_and_test(&m->prepare_actions)) {
   list_add_tail(&m->list, &pool->prepared_mappings);
   wake_worker(pool);
  }
@@ -593,8 +598,7 @@ static void copy_complete(int read_err, unsigned long write_err, void *context)
  m->err = read_err || write_err ? -EIO : 0;
 
  spin_lock_irqsave(&pool->lock, flags);
- m->prepared = true;
- __maybe_add_mapping(m);
+ __complete_mapping_preparation(m);
  spin_unlock_irqrestore(&pool->lock, flags);
 }
 
@@ -608,8 +612,7 @@ static void overwrite_endio(struct bio *bio, int err)
  m->err = err;
 
  spin_lock_irqsave(&pool->lock, flags);
- m->prepared = true;
- __maybe_add_mapping(m);
+ __complete_mapping_preparation(m);
  spin_unlock_irqrestore(&pool->lock, flags);
 }
 
@@ -830,7 +833,9 @@ static void schedule_copy(struct thin_c *tc, dm_block_t virt_block,
  m->cell = cell;
 
  if (!dm_deferred_set_add_work(pool->shared_read_ds, &m->list))
-  m->quiesced = true;
+  atomic_set(&m->prepare_actions, 1); /* copy only */
+ else
+  atomic_set(&m->prepare_actions, 2); /* quiesce + copy */
 
  /*
   * IO to pool_dev remaps to the pool target's data_dev.
@@ -890,8 +895,7 @@ static void schedule_zero(struct thin_c *tc, dm_block_t virt_block,
  struct pool *pool = tc->pool;
  struct dm_thin_new_mapping *m = get_next_mapping(pool);
 
- m->quiesced = true;
- m->prepared = false;
+ atomic_set(&m->prepare_actions, 1); /* no need to quiesce */
  m->tc = tc;
  m->virt_block = virt_block;
  m->data_block = data_block;
@@ -3355,8 +3359,7 @@ static int thin_endio(struct dm_target *ti, struct bio *bio, int err)
   spin_lock_irqsave(&pool->lock, flags);
   list_for_each_entry_safe(m, tmp, &work, list) {
    list_del(&m->list);
-   m->quiesced = true;
-   __maybe_add_mapping(m);
+   __complete_mapping_preparation(m);
   }
   spin_unlock_irqrestore(&pool->lock, flags);
  }
-- 
1.7.1