From db1dc7d9276b89b3ac75591954438dd50045bec3 Mon Sep 17 00:00:00 2001
From: Jiri Olsa <jolsa@redhat.com>
Date: Mon, 8 Sep 2014 13:04:27 -0400
Subject: [kernel] perf: Rework free paths

Message-id: <1410181752-25631-23-git-send-email-jolsa@redhat.com>
Patchwork-id: 91705
O-Subject: [PATCH RHEL7.1 BZ1134356 022/307] perf: Rework free paths
Bugzilla: 1134356
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Don Zickus <dzickus@redhat.com>

From: Jiri Olsa <jolsa@kernel.org>

Bugzilla: 1134356
https://bugzilla.redhat.com/show_bug.cgi?id=1134356

upstream
========
commit 683ede43dd412c6cad0d23578a018409ac9c683e
Author: Peter Zijlstra <peterz@infradead.org>
Date: Mon May 5 12:11:24 2014 +0200

description
===========
Primarily make perf_event_release_kernel() into put_event(), this will
allow kernel space to create per-task inherited events, and is safer
in general.

Also, document the free_event() assumptions.
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/events/core.c b/kernel/events/core.c
index d8b4639..f7ce810 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -3263,7 +3263,8 @@ static void __free_event(struct perf_event *event)
 
  call_rcu(&event->rcu_head, free_event_rcu);
 }
-static void free_event(struct perf_event *event)
+
+static void _free_event(struct perf_event *event)
 {
  irq_work_sync(&event->pending);
 
@@ -3291,42 +3292,31 @@ static void free_event(struct perf_event *event)
  if (is_cgroup_event(event))
   perf_detach_cgroup(event);
 
-
  __free_event(event);
 }
 
-int perf_event_release_kernel(struct perf_event *event)
+/*
+ * Used to free events which have a known refcount of 1, such as in error paths
+ * where the event isn't exposed yet and inherited events.
+ */
+static void free_event(struct perf_event *event)
 {
- struct perf_event_context *ctx = event->ctx;
-
- WARN_ON_ONCE(ctx->parent_ctx);
- /*
-  * There are two ways this annotation is useful:
-  *
-  *  1) there is a lock recursion from perf_event_exit_task
-  *     see the comment there.
-  *
-  *  2) there is a lock-inversion with mmap_sem through
-  *     perf_event_read_group(), which takes faults while
-  *     holding ctx->mutex, however this is called after
-  *     the last filedesc died, so there is no possibility
-  *     to trigger the AB-BA case.
-  */
- mutex_lock_nested(&ctx->mutex, SINGLE_DEPTH_NESTING);
- perf_remove_from_context(event, true);
- mutex_unlock(&ctx->mutex);
-
- free_event(event);
+ if (WARN(atomic_long_cmpxchg(&event->refcount, 1, 0) != 1,
+    "unexpected event refcount: %ld; ptr=%p\n",
+    atomic_long_read(&event->refcount), event)) {
+  /* leak to avoid use-after-free */
+  return;
+ }
 
- return 0;
+ _free_event(event);
 }
-EXPORT_SYMBOL_GPL(perf_event_release_kernel);
 
 /*
  * Called when the last reference to the file is gone.
  */
 static void put_event(struct perf_event *event)
 {
+ struct perf_event_context *ctx = event->ctx;
  struct task_struct *owner;
 
  if (!atomic_long_dec_and_test(&event->refcount))
@@ -3365,9 +3355,33 @@ static void put_event(struct perf_event *event)
   put_task_struct(owner);
  }
 
- perf_event_release_kernel(event);
+ WARN_ON_ONCE(ctx->parent_ctx);
+ /*
+  * There are two ways this annotation is useful:
+  *
+  *  1) there is a lock recursion from perf_event_exit_task
+  *     see the comment there.
+  *
+  *  2) there is a lock-inversion with mmap_sem through
+  *     perf_event_read_group(), which takes faults while
+  *     holding ctx->mutex, however this is called after
+  *     the last filedesc died, so there is no possibility
+  *     to trigger the AB-BA case.
+  */
+ mutex_lock_nested(&ctx->mutex, SINGLE_DEPTH_NESTING);
+ perf_remove_from_context(event, true);
+ mutex_unlock(&ctx->mutex);
+
+ _free_event(event);
 }
 
+int perf_event_release_kernel(struct perf_event *event)
+{
+ put_event(event);
+ return 0;
+}
+EXPORT_SYMBOL_GPL(perf_event_release_kernel);
+
 static int perf_release(struct inode *inode, struct file *file)
 {
  put_event(file->private_data);
-- 
1.7.1