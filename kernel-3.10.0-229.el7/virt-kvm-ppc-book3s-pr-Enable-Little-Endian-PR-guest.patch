From d22270483811b4ea739c10f8ab977f02afd7dd44 Mon Sep 17 00:00:00 2001
From: David Gibson <dgibson@redhat.com>
Date: Mon, 15 Sep 2014 07:13:06 -0400
Subject: [virt] kvm/ppc: book3s/pr - Enable Little Endian PR guest

Message-id: <1410765214-16377-51-git-send-email-dgibson@redhat.com>
Patchwork-id: 94733
O-Subject: [PATCH 50/78] KVM: PPC: BOOK3S: PR: Enable Little Endian PR guest
Bugzilla: 1123145 1123133 1123367
RH-Acked-by: Radim Krcmar <rkrcmar@redhat.com>
RH-Acked-by: Don Zickus <dzickus@redhat.com>
RH-Acked-by: Steve Best <sbest@redhat.com>

Signed-off-by: David Gibson <dgibson@redhat.com>

commit e5ee5422f8867d8b8108f8e1f0f47dc59b043f5b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon May 5 08:39:44 2014 +0530

    KVM: PPC: BOOK3S: PR: Enable Little Endian PR guest

    This patch make sure we inherit the LE bit correctly in different case
    so that we can run Little Endian distro in PR mode

    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 84c24db..c3b4871 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -578,6 +578,7 @@ struct kvm_vcpu_arch {
 #ifdef CONFIG_PPC_BOOK3S
  ulong fault_dar;
  u32 fault_dsisr;
+ unsigned long intr_msr;
 #endif
 
 #ifdef CONFIG_BOOKE
@@ -670,7 +671,6 @@ struct kvm_vcpu_arch {
  spinlock_t tbacct_lock;
  u64 busy_stolen;
  u64 busy_preempt;
- unsigned long intr_msr;
 #endif
 };
 
diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 92a8d12..71ef537 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -490,7 +490,6 @@ int main(void)
  DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
  DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
  DEFINE(VCPU_VPA_DIRTY, offsetof(struct kvm_vcpu, arch.vpa.dirty));
- DEFINE(VCPU_INTR_MSR, offsetof(struct kvm_vcpu, arch.intr_msr));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
  DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
@@ -525,6 +524,7 @@ int main(void)
  DEFINE(VCPU_SLB_NR, offsetof(struct kvm_vcpu, arch.slb_nr));
  DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
  DEFINE(VCPU_FAULT_DAR, offsetof(struct kvm_vcpu, arch.fault_dar));
+ DEFINE(VCPU_INTR_MSR, offsetof(struct kvm_vcpu, arch.intr_msr));
  DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
  DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
  DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
diff --git a/arch/powerpc/kvm/book3s_64_mmu.c b/arch/powerpc/kvm/book3s_64_mmu.c
index 10f3cc4..b26d664 100644
--- a/arch/powerpc/kvm/book3s_64_mmu.c
+++ b/arch/powerpc/kvm/book3s_64_mmu.c
@@ -38,7 +38,7 @@
 
 static void kvmppc_mmu_book3s_64_reset_msr(struct kvm_vcpu *vcpu)
 {
- kvmppc_set_msr(vcpu, MSR_SF);
+ kvmppc_set_msr(vcpu, vcpu->arch.intr_msr);
 }
 
 static struct kvmppc_slb *kvmppc_mmu_book3s_64_find_slbe(
diff --git a/arch/powerpc/kvm/book3s_pr.c b/arch/powerpc/kvm/book3s_pr.c
index 2896901..6712c44 100644
--- a/arch/powerpc/kvm/book3s_pr.c
+++ b/arch/powerpc/kvm/book3s_pr.c
@@ -250,7 +250,7 @@ static void kvmppc_recalc_shadow_msr(struct kvm_vcpu *vcpu)
  ulong smsr = guest_msr;
 
  /* Guest MSR values */
- smsr &= MSR_FE0 | MSR_FE1 | MSR_SF | MSR_SE | MSR_BE;
+ smsr &= MSR_FE0 | MSR_FE1 | MSR_SF | MSR_SE | MSR_BE | MSR_LE;
  /* Process MSR values */
  smsr |= MSR_ME | MSR_RI | MSR_IR | MSR_DR | MSR_PR | MSR_EE;
  /* External providers the guest reserved */
@@ -1123,6 +1123,15 @@ static int kvmppc_get_one_reg_pr(struct kvm_vcpu *vcpu, u64 id,
  case KVM_REG_PPC_HIOR:
   *val = get_reg_val(id, to_book3s(vcpu)->hior);
   break;
+ case KVM_REG_PPC_LPCR:
+  /*
+   * We are only interested in the LPCR_ILE bit
+   */
+  if (vcpu->arch.intr_msr & MSR_LE)
+   *val = get_reg_val(id, LPCR_ILE);
+  else
+   *val = get_reg_val(id, 0);
+  break;
  default:
   r = -EINVAL;
   break;
@@ -1131,6 +1140,14 @@ static int kvmppc_get_one_reg_pr(struct kvm_vcpu *vcpu, u64 id,
  return r;
 }
 
+static void kvmppc_set_lpcr_pr(struct kvm_vcpu *vcpu, u64 new_lpcr)
+{
+ if (new_lpcr & LPCR_ILE)
+  vcpu->arch.intr_msr |= MSR_LE;
+ else
+  vcpu->arch.intr_msr &= ~MSR_LE;
+}
+
 static int kvmppc_set_one_reg_pr(struct kvm_vcpu *vcpu, u64 id,
      union kvmppc_one_reg *val)
 {
@@ -1141,6 +1158,9 @@ static int kvmppc_set_one_reg_pr(struct kvm_vcpu *vcpu, u64 id,
   to_book3s(vcpu)->hior = set_reg_val(id, *val);
   to_book3s(vcpu)->hior_explicit = true;
   break;
+ case KVM_REG_PPC_LPCR:
+  kvmppc_set_lpcr_pr(vcpu, set_reg_val(id, *val));
+  break;
  default:
   r = -EINVAL;
   break;
@@ -1199,6 +1219,7 @@ static struct kvm_vcpu *kvmppc_core_vcpu_create_pr(struct kvm *kvm,
  vcpu->arch.pvr = 0x3C0301;
  if (mmu_has_feature(MMU_FTR_1T_SEGMENT))
   vcpu->arch.pvr = mfspr(SPRN_PVR);
+ vcpu->arch.intr_msr = MSR_SF;
 #else
  /* default to book3s_32 (750) */
  vcpu->arch.pvr = 0x84202;
-- 
1.7.1