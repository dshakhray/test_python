From 600476166fcf681b4e85e7daf66e07b3bc9f5bfe Mon Sep 17 00:00:00 2001
From: Larry Woodman <lwoodman@redhat.com>
Date: Fri, 30 May 2014 11:22:32 -0400
Subject: [misc] arch: Introduce smp_load_acquire(), smp_store_release()

Message-id: <1401448958-5278-5-git-send-email-lwoodman@redhat.com>
Patchwork-id: 81236
O-Subject: [RHEL7.1 PATCH 04/10] arch: Introduce smp_load_acquire(), smp_store_release()
Bugzilla: 1087655 1087919 1087922
RH-Acked-by: Rafael Aquini <aquini@redhat.com>
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>

commit 47933ad41a86a4a9b50bed7c9b9bd2ba242aac63
 Author: Peter Zijlstra <peterz@infradead.org>
 Date:   Wed Nov 6 14:57:36 2013 +0100

    arch: Introduce smp_load_acquire(), smp_store_release()

    A number of situations currently require the heavyweight smp_mb(),
    even though there is no need to order prior stores against later
    loads.  Many architectures have much cheaper ways to handle these
    situations, but the Linux kernel currently has no portable way
    to make use of them.

    This commit therefore supplies smp_load_acquire() and
    smp_store_release() to remedy this situation.  The new
    smp_load_acquire() primitive orders the specified load against
    any subsequent reads or writes, while the new smp_store_release()
    primitive orders the specifed store against any prior reads or
    writes.  These primitives allow array-based circular FIFOs to be
    implemented without an smp_mb(), and also allow a theoretical
    hole in rcu_assign_pointer() to be closed at no additional
    expense on most architectures.

    In addition, the RCU experience transitioning from explicit
    smp_read_barrier_depends() and smp_wmb() to rcu_dereference()
    and rcu_assign_pointer(), respectively resulted in substantial
    improvements in readability.  It therefore seems likely that
    replacing other explicit barriers with smp_load_acquire() and
    smp_store_release() will provide similar benefits.  It appears
    that roughly half of the explicit barriers in core kernel code
    might be so replaced.

    [Changelog by PaulMck]

    Reviewed-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Victor Kaplansky <VICTORK@il.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20131213150640.908486364@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/include/asm/barrier.h b/arch/powerpc/include/asm/barrier.h
index ae78225..f89da80 100644
--- a/arch/powerpc/include/asm/barrier.h
+++ b/arch/powerpc/include/asm/barrier.h
@@ -45,11 +45,15 @@
 #    define SMPWMB      eieio
 #endif
 
+#define __lwsync() __asm__ __volatile__ (stringify_in_c(LWSYNC) : : :"memory")
+
 #define smp_mb() mb()
-#define smp_rmb() __asm__ __volatile__ (stringify_in_c(LWSYNC) : : :"memory")
+#define smp_rmb() __lwsync()
 #define smp_wmb() __asm__ __volatile__ (stringify_in_c(SMPWMB) : : :"memory")
 #define smp_read_barrier_depends() read_barrier_depends()
 #else
+#define __lwsync() barrier()
+
 #define smp_mb() barrier()
 #define smp_rmb() barrier()
 #define smp_wmb() barrier()
@@ -65,4 +69,19 @@
 #define data_barrier(x) \
  asm volatile("twi 0,%0,0; isync" : : "r" (x) : "memory");
 
+#define smp_store_release(p, v)      \
+do {         \
+ compiletime_assert_atomic_type(*p);    \
+ __lwsync();       \
+ ACCESS_ONCE(*p) = (v);      \
+} while (0)
+
+#define smp_load_acquire(p)      \
+({         \
+ typeof(*p) ___p1 = ACCESS_ONCE(*p);    \
+ compiletime_assert_atomic_type(*p);    \
+ __lwsync();       \
+ ___p1;        \
+})
+
 #endif /* _ASM_POWERPC_BARRIER_H */
diff --git a/arch/s390/include/asm/barrier.h b/arch/s390/include/asm/barrier.h
index 16760ee..578680f 100644
--- a/arch/s390/include/asm/barrier.h
+++ b/arch/s390/include/asm/barrier.h
@@ -32,4 +32,19 @@
 
 #define set_mb(var, value)  do { var = value; mb(); } while (0)
 
+#define smp_store_release(p, v)      \
+do {         \
+ compiletime_assert_atomic_type(*p);    \
+ barrier();       \
+ ACCESS_ONCE(*p) = (v);      \
+} while (0)
+
+#define smp_load_acquire(p)      \
+({         \
+ typeof(*p) ___p1 = ACCESS_ONCE(*p);    \
+ compiletime_assert_atomic_type(*p);    \
+ barrier();       \
+ ___p1;        \
+})
+
 #endif /* __ASM_BARRIER_H */
diff --git a/arch/x86/include/asm/barrier.h b/arch/x86/include/asm/barrier.h
index c6cd358..04a4890 100644
--- a/arch/x86/include/asm/barrier.h
+++ b/arch/x86/include/asm/barrier.h
@@ -92,12 +92,53 @@
 #endif
 #define smp_read_barrier_depends() read_barrier_depends()
 #define set_mb(var, value) do { (void)xchg(&var, value); } while (0)
-#else
+#else /* !SMP */
 #define smp_mb() barrier()
 #define smp_rmb() barrier()
 #define smp_wmb() barrier()
 #define smp_read_barrier_depends() do { } while (0)
 #define set_mb(var, value) do { var = value; barrier(); } while (0)
+#endif /* SMP */
+
+#if defined(CONFIG_X86_OOSTORE) || defined(CONFIG_X86_PPRO_FENCE)
+
+/*
+ * For either of these options x86 doesn't have a strong TSO memory
+ * model and we should fall back to full barriers.
+ */
+
+#define smp_store_release(p, v)      \
+do {         \
+ compiletime_assert_atomic_type(*p);    \
+ smp_mb();       \
+ ACCESS_ONCE(*p) = (v);      \
+} while (0)
+
+#define smp_load_acquire(p)      \
+({         \
+ typeof(*p) ___p1 = ACCESS_ONCE(*p);    \
+ compiletime_assert_atomic_type(*p);    \
+ smp_mb();       \
+ ___p1;        \
+})
+
+#else /* regular x86 TSO memory ordering */
+
+#define smp_store_release(p, v)      \
+do {         \
+ compiletime_assert_atomic_type(*p);    \
+ barrier();       \
+ ACCESS_ONCE(*p) = (v);      \
+} while (0)
+
+#define smp_load_acquire(p)      \
+({         \
+ typeof(*p) ___p1 = ACCESS_ONCE(*p);    \
+ compiletime_assert_atomic_type(*p);    \
+ barrier();       \
+ ___p1;        \
+})
+
 #endif
 
 /*
diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 639d7a4..01613b3 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -46,5 +46,20 @@
 #define read_barrier_depends()  do {} while (0)
 #define smp_read_barrier_depends() do {} while (0)
 
+#define smp_store_release(p, v)      \
+do {         \
+ compiletime_assert_atomic_type(*p);    \
+ smp_mb();       \
+ ACCESS_ONCE(*p) = (v);      \
+} while (0)
+
+#define smp_load_acquire(p)      \
+({         \
+ typeof(*p) ___p1 = ACCESS_ONCE(*p);    \
+ compiletime_assert_atomic_type(*p);    \
+ smp_mb();       \
+ ___p1;        \
+})
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_GENERIC_BARRIER_H */
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 92669cd..fe7a686 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -298,6 +298,11 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))
 #endif
 
+/* Is this type a native word size -- useful for atomic operations */
+#ifndef __native_word
+# define __native_word(t) (sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))
+#endif
+
 /* Compile time object size, -1 for unknown */
 #ifndef __compiletime_object_size
 # define __compiletime_object_size(obj) -1
@@ -337,6 +342,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #define compiletime_assert(condition, msg) \
  _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
 
+#define compiletime_assert_atomic_type(t)    \
+ compiletime_assert(__native_word(t),    \
+  "Need native word sized stores/loads for atomicity.")
+
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),
-- 
1.7.1