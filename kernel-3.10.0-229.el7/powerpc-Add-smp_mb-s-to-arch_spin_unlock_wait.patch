From c2205da54c52a073785f25b5541bd3a2fc7f663e Mon Sep 17 00:00:00 2001
From: Gustavo Duarte <gduarte@redhat.com>
Date: Mon, 22 Sep 2014 19:33:41 -0400
Subject: [powerpc] Add smp_mb()s to arch_spin_unlock_wait()

Message-id: <1411414421-28349-3-git-send-email-gduarte@redhat.com>
Patchwork-id: 96444
O-Subject: [RHEL7.1 PATCH BZ 1136528 2/2 v2] powerpc: Add smp_mb()s to arch_spin_unlock_wait()
Bugzilla: 1136528
RH-Acked-by: Jes Sorensen <Jes.Sorensen@redhat.com>
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1136528
Upstream Status: 78e05b1421fa41ae8457701140933baa5e7d9479

commit 78e05b1421fa41ae8457701140933baa5e7d9479
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Aug 7 15:36:18 2014 +1000

    powerpc: Add smp_mb()s to arch_spin_unlock_wait()

    Similar to the previous commit which described why we need to add a
    barrier to arch_spin_is_locked(), we have a similar problem with
    spin_unlock_wait().

    We need a barrier on entry to ensure any spinlock we have previously
    taken is visibly locked prior to the load of lock->slock.

    It's also not clear if spin_unlock_wait() is intended to have ACQUIRE
    semantics. For now be conservative and add a barrier on exit to give it
    ACQUIRE semantics.

    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/lib/locks.c b/arch/powerpc/lib/locks.c
index 0c9c8d7..170a034 100644
--- a/arch/powerpc/lib/locks.c
+++ b/arch/powerpc/lib/locks.c
@@ -70,12 +70,16 @@ void __rw_yield(arch_rwlock_t *rw)
 
 void arch_spin_unlock_wait(arch_spinlock_t *lock)
 {
+ smp_mb();
+
  while (lock->slock) {
   HMT_low();
   if (SHARED_PROCESSOR)
    __spin_yield(lock);
  }
  HMT_medium();
+
+ smp_mb();
 }
 
 EXPORT_SYMBOL(arch_spin_unlock_wait);
-- 
1.7.1