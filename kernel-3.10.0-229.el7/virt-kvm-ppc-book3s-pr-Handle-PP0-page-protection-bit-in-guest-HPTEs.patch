From 841c783f2a4239258d3f9c16880fcf7ee1ab6d7c Mon Sep 17 00:00:00 2001
From: Don Zickus <dzickus@redhat.com>
Date: Fri, 12 Sep 2014 18:06:02 -0400
Subject: [virt] kvm/ppc: book3s pr - Handle PP0 page-protection bit in guest HPTEs

Message-id: <1410545655-205645-134-git-send-email-dzickus@redhat.com>
Patchwork-id: 94657
O-Subject: [RHEL7 PATCH 133/626] KVM: PPC: Book3S PR: Handle PP0 page-protection bit in guest HPTEs
Bugzilla: 1127366
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Steve Best <sbest@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>
RH-Acked-by: David Gibson <dgibson@redhat.com>

https://bugzilla.redhat.com/show_bug.cgi?id=1127366

commit 03a9c90334d611c3006ac9569579f25f64812bc1
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:46 2013 +1000

    KVM: PPC: Book3S PR: Handle PP0 page-protection bit in guest HPTEs

    64-bit POWER processors have a three-bit field for page protection in
    the hashed page table entry (HPTE).  Currently we only interpret the two
    bits that were present in older versions of the architecture.  The only
    defined combination that has the new bit set is 110, meaning read-only
    for supervisor and no access for user mode.

    This adds code to kvmppc_mmu_book3s_64_xlate() to interpret the extra
    bit appropriately.

    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/kvm/book3s_64_mmu.c b/arch/powerpc/kvm/book3s_64_mmu.c
index ffcde01..9e6e112 100644
--- a/arch/powerpc/kvm/book3s_64_mmu.c
+++ b/arch/powerpc/kvm/book3s_64_mmu.c
@@ -298,6 +298,8 @@ do_second:
  v = pteg[i];
  r = pteg[i+1];
  pp = (r & HPTE_R_PP) | key;
+ if (r & HPTE_R_PP0)
+  pp |= 8;
 
  gpte->eaddr = eaddr;
  gpte->vpage = kvmppc_mmu_book3s_64_ea_to_vp(vcpu, eaddr, data);
@@ -319,6 +321,7 @@ do_second:
  case 3:
  case 5:
  case 7:
+ case 10:
   gpte->may_read = true;
   break;
  }
-- 
1.7.1