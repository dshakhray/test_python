From 73157a7e2926fa1e7eeec8ea65b8f8c63a677bf8 Mon Sep 17 00:00:00 2001
From: Jiri Olsa <jolsa@redhat.com>
Date: Tue, 19 Aug 2014 15:22:54 -0400
Subject: [x86] perf/intel: Clean-up/reduce PEBS code

Message-id: <1408462094-14194-6-git-send-email-jolsa@redhat.com>
Patchwork-id: 87992
O-Subject: [PATCH RHEL7.1 BZ1131394 005/325] perf/x86/intel: Clean-up/reduce PEBS code
Bugzilla: 1131394
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

From: Jiri Olsa <jolsa@kernel.org>

Bugzilla: 1131394
https://bugzilla.redhat.com/show_bug.cgi?id=1131394

upstream
========
commit d2beea4a3419e63804094e9ac4b6d1518bc17a9b
Author: Peter Zijlstra <peterz@infradead.org>
Date: Thu Sep 12 13:00:47 2013 +0200

description
===========
Get rid of some pointless duplication introduced by the Haswell code.
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/x86/kernel/cpu/perf_event_intel_ds.c b/arch/x86/kernel/cpu/perf_event_intel_ds.c
index b0fafdc..741068b 100644
--- a/arch/x86/kernel/cpu/perf_event_intel_ds.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_ds.c
@@ -188,8 +188,7 @@ struct pebs_record_hsw {
  u64 r8,  r9,  r10, r11;
  u64 r12, r13, r14, r15;
  u64 status, dla, dse, lat;
- u64 real_ip; /* the actual eventing ip */
- u64 tsx_tuning; /* TSX abort cycles and flags */
+ u64 real_ip, tsx_tuning;
 };
 
 union hsw_tsx_tuning {
@@ -805,10 +804,8 @@ static void __intel_pmu_pebs_event(struct perf_event *event,
        struct pt_regs *iregs, void *__pebs)
 {
  /*
-  * We cast to pebs_record_nhm to get the load latency data
-  * if extra_reg MSR_PEBS_LD_LAT_THRESHOLD used
-  * We cast to the biggest PEBS record are careful not
-  * to access out-of-bounds members.
+  * We cast to the biggest pebs_record but are careful not to
+  * unconditionally access the 'extra' entries.
   */
  struct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);
  struct pebs_record_hsw *pebs = __pebs;
@@ -878,12 +875,11 @@ static void __intel_pmu_pebs_event(struct perf_event *event,
   regs.flags &= ~PERF_EFLAGS_EXACT;
 
  if ((event->attr.sample_type & PERF_SAMPLE_ADDR) &&
-  x86_pmu.intel_cap.pebs_format >= 1)
+     x86_pmu.intel_cap.pebs_format >= 1)
   data.addr = pebs->dla;
 
  /* Only set the TSX weight when no memory weight was requested. */
- if ((event->attr.sample_type & PERF_SAMPLE_WEIGHT) &&
-     !fll &&
+ if ((event->attr.sample_type & PERF_SAMPLE_WEIGHT) && !fll &&
      (x86_pmu.intel_cap.pebs_format >= 2))
   data.weight = intel_hsw_weight(pebs);
 
@@ -935,17 +931,34 @@ static void intel_pmu_drain_pebs_core(struct pt_regs *iregs)
  __intel_pmu_pebs_event(event, iregs, at);
 }
 
-static void __intel_pmu_drain_pebs_nhm(struct pt_regs *iregs, void *at,
-     void *top)
+static void intel_pmu_drain_pebs_nhm(struct pt_regs *iregs)
 {
  struct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);
  struct debug_store *ds = cpuc->ds;
  struct perf_event *event = NULL;
+ void *at, *top;
  u64 status = 0;
- int bit;
+ int bit, n;
+
+ if (!x86_pmu.pebs_active)
+  return;
+
+ at  = (struct pebs_record_nhm *)(unsigned long)ds->pebs_buffer_base;
+ top = (struct pebs_record_nhm *)(unsigned long)ds->pebs_index;
 
  ds->pebs_index = ds->pebs_buffer_base;
 
+ n = (top - at) / x86_pmu.pebs_record_size;
+ if (n <= 0)
+  return;
+
+ /*
+  * Should not happen, we program the threshold at 1 and do not
+  * set a reset value.
+  */
+ WARN_ONCE(n > x86_pmu.max_pebs_events,
+    "Unexpected number of pebs records %d\n", n);
+
  for (; at < top; at += x86_pmu.pebs_record_size) {
   struct pebs_record_nhm *p = at;
 
@@ -973,61 +986,6 @@ static void __intel_pmu_drain_pebs_nhm(struct pt_regs *iregs, void *at,
  }
 }
 
-static void intel_pmu_drain_pebs_nhm(struct pt_regs *iregs)
-{
- struct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);
- struct debug_store *ds = cpuc->ds;
- struct pebs_record_nhm *at, *top;
- int n;
-
- if (!x86_pmu.pebs_active)
-  return;
-
- at  = (struct pebs_record_nhm *)(unsigned long)ds->pebs_buffer_base;
- top = (struct pebs_record_nhm *)(unsigned long)ds->pebs_index;
-
- ds->pebs_index = ds->pebs_buffer_base;
-
- n = top - at;
- if (n <= 0)
-  return;
-
- /*
-  * Should not happen, we program the threshold at 1 and do not
-  * set a reset value.
-  */
- WARN_ONCE(n > x86_pmu.max_pebs_events,
-    "Unexpected number of pebs records %d\n", n);
-
- return __intel_pmu_drain_pebs_nhm(iregs, at, top);
-}
-
-static void intel_pmu_drain_pebs_hsw(struct pt_regs *iregs)
-{
- struct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);
- struct debug_store *ds = cpuc->ds;
- struct pebs_record_hsw *at, *top;
- int n;
-
- if (!x86_pmu.pebs_active)
-  return;
-
- at  = (struct pebs_record_hsw *)(unsigned long)ds->pebs_buffer_base;
- top = (struct pebs_record_hsw *)(unsigned long)ds->pebs_index;
-
- n = top - at;
- if (n <= 0)
-  return;
- /*
-  * Should not happen, we program the threshold at 1 and do not
-  * set a reset value.
-  */
- WARN_ONCE(n > x86_pmu.max_pebs_events,
-    "Unexpected number of pebs records %d\n", n);
-
- return __intel_pmu_drain_pebs_nhm(iregs, at, top);
-}
-
 /*
  * BTS, PEBS probe and setup
  */
@@ -1062,7 +1020,7 @@ void intel_ds_init(void)
   case 2:
    pr_cont("PEBS fmt2%c, ", pebs_type);
    x86_pmu.pebs_record_size = sizeof(struct pebs_record_hsw);
-   x86_pmu.drain_pebs = intel_pmu_drain_pebs_hsw;
+   x86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;
    break;
 
   default:
-- 
1.7.1