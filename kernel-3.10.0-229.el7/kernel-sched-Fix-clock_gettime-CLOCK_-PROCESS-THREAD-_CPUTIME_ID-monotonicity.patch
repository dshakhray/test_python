From af101f9b2e86622796bb38ce8d0e59cfd9cc4487 Mon Sep 17 00:00:00 2001
From: Seth Jennings <sjenning@redhat.com>
Date: Wed, 3 Sep 2014 18:06:11 -0400
Subject: [kernel] sched: Fix clock_gettime(CLOCK_[PROCESS/THREAD]_CPUTIME_ID) monotonicity

Message-id: <1409767571-23670-1-git-send-email-sjenning@redhat.com>
Patchwork-id: 91183
O-Subject: [RHEL7.1 PATCH] sched: Fix clock_gettime(CLOCK_[PROCESS/THREAD]_CPUTIME_ID) monotonicity
Bugzilla: 1113932
RH-Acked-by: Jarod Wilson <jarod@redhat.com>
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>

From: Mike Galbraith <umgwanakikbuti@gmail.com>

BZ: https://bugzilla.redhat.com/show_bug.cgi?id=1113932
Brew: http://brewweb.devel.redhat.com/brew/taskinfo?taskID=7913153
Upstream: 4036ac1567834222fc763ab18e3e17df93b4eaaf

  If a task has been dequeued, it has been accounted.  Do not project
  cycles that may or may not ever be accounted to a dequeued task, as
  that may make clock_gettime() both inaccurate and non-monotonic.

  Protect update_rq_clock() from slight TSC skew while at it.

  Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
  Cc: kosaki.motohiro@jp.fujitsu.com
  Cc: pjt@google.com
  Cc: Linus Torvalds <torvalds@linux-foundation.org>
  Signed-off-by: Peter Zijlstra <peterz@infradead.org>
  Link: http://lkml.kernel.org/r/1403588980.29711.11.camel@marge.simpson.net
  Signed-off-by: Ingo Molnar <mingo@kernel.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 6e588b0..8a2b02e 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -138,6 +138,8 @@ void update_rq_clock(struct rq *rq)
   return;
 
  delta = sched_clock_cpu(cpu_of(rq)) - rq->clock;
+ if (delta < 0)
+  return;
  rq->clock += delta;
  update_rq_clock_task(rq, delta);
 }
@@ -2846,7 +2848,12 @@ static u64 do_task_delta_exec(struct task_struct *p, struct rq *rq)
 {
  u64 ns = 0;
 
- if (task_current(rq, p)) {
+ /*
+  * Must be ->curr _and_ ->on_rq.  If dequeued, we would
+  * project cycles that may never be accounted to this
+  * thread, breaking clock_gettime().
+  */
+ if (task_current(rq, p) && p->on_rq) {
   update_rq_clock(rq);
   ns = rq_clock_task(rq) - p->se.exec_start;
   if ((s64)ns < 0)
@@ -2889,8 +2896,10 @@ unsigned long long task_sched_runtime(struct task_struct *p)
   * If we race with it leaving cpu, we'll take a lock. So we're correct.
   * If we race with it entering cpu, unaccounted time is 0. This is
   * indistinguishable from the read occurring a few cycles earlier.
+  * If we see ->on_cpu without ->on_rq, the task is leaving, and has
+  * been accounted, so we're correct here as well.
   */
- if (!p->on_cpu)
+ if (!p->on_cpu || !p->on_rq)
   return p->se.sum_exec_runtime;
 #endif
 
-- 
1.7.1