From ceaa95ba645c2780125ef4d7553bfcdaf85ad7b5 Mon Sep 17 00:00:00 2001
From: Eric Sandeen <sandeen@redhat.com>
Date: Mon, 4 Aug 2014 19:00:09 -0400
Subject: [fs] btrfs: change the members' order of btrfs_space_info structure to reduce the cache miss

Message-id: <1407179109-13714-123-git-send-email-sandeen@redhat.com>
Patchwork-id: 87069
O-Subject: [RHEL7.1 PATCH 122/422] Btrfs: change the members' order of btrfs_space_info structure to reduce the cache miss
Bugzilla: 1071179
RH-Acked-by: Zach Brown <zab@redhat.com>

From: Miao Xie <miaox@cn.fujitsu.com>

commit 26b47ff65bcdff8473b87680d8f876c66208087b
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Jan 15 20:00:54 2014 +0800

    Btrfs: change the members' order of btrfs_space_info structure to reduce the cache miss

    It is better that the position of the lock is close to the data which is
    protected by it, because they may be in the same cache line, we will load
    less cache lines when we access them. So we rearrange the members' position
    of btrfs_space_info structure to make the lock be closer to the its data.

    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Reviewed-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>
Signed-off-by: Eric Sandeen <sandeen@redhat.com>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/fs/btrfs/ctree.h b/fs/btrfs/ctree.h
index 52c96db..84d4c05 100644
--- a/fs/btrfs/ctree.h
+++ b/fs/btrfs/ctree.h
@@ -1105,7 +1105,7 @@ struct btrfs_qgroup_limit_item {
 } __attribute__ ((__packed__));
 
 struct btrfs_space_info {
- u64 flags;
+ spinlock_t lock;
 
  u64 total_bytes; /* total bytes in the space,
        this doesn't take mirrors into account */
@@ -1115,14 +1115,25 @@ struct btrfs_space_info {
        transaction finishes */
  u64 bytes_reserved; /* total bytes the allocator has reserved for
        current allocations */
- u64 bytes_readonly; /* total bytes that are read only */
-
  u64 bytes_may_use; /* number of bytes that may be used for
        delalloc/allocations */
+ u64 bytes_readonly; /* total bytes that are read only */
+
+ unsigned int full:1; /* indicates that we cannot allocate any more
+       chunks for this space */
+ unsigned int chunk_alloc:1; /* set if we are allocating a chunk */
+
+ unsigned int flush:1;  /* set if we are trying to make space */
+
+ unsigned int force_alloc; /* set if we need to force a chunk
+        alloc for this space */
+
  u64 disk_used;  /* total bytes used on disk */
  u64 disk_total;  /* total bytes on disk, takes mirrors into
        account */
 
+ u64 flags;
+
  /*
   * bytes_pinned is kept in line with what is actually pinned, as in
   * we've called update_block_group and dropped the bytes_used counter
@@ -1135,21 +1146,11 @@ struct btrfs_space_info {
   */
  struct percpu_counter total_bytes_pinned;
 
- unsigned int full:1; /* indicates that we cannot allocate any more
-       chunks for this space */
- unsigned int chunk_alloc:1; /* set if we are allocating a chunk */
-
- unsigned int flush:1;  /* set if we are trying to make space */
-
- unsigned int force_alloc; /* set if we need to force a chunk
-        alloc for this space */
-
  struct list_head list;
 
+ struct rw_semaphore groups_sem;
  /* for block groups in our same type */
  struct list_head block_groups[BTRFS_NR_RAID_TYPES];
- spinlock_t lock;
- struct rw_semaphore groups_sem;
  wait_queue_head_t wait;
 
  struct kobject kobj;
-- 
1.7.1