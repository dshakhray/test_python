From 8eea83734eac0e90cdda9ec5f14851bc77f69473 Mon Sep 17 00:00:00 2001
From: Mike Snitzer <snitzer@redhat.com>
Date: Wed, 16 Jul 2014 20:42:13 -0400
Subject: [md] dm: introduce dm_accept_partial_bio

Message-id: <1405543364-20901-10-git-send-email-snitzer@redhat.com>
Patchwork-id: 85892
O-Subject: [RHEL7.1 PATCH 09/40] dm: introduce dm_accept_partial_bio
Bugzilla: 1117872
RH-Acked-by: Mikulas Patocka <mpatocka@redhat.com>
RH-Acked-by: Heinz Mauelshagen <heinzm@redhat.com>

BZ: 1117872

Upstream commit 1dd40c3ecd9b8a4ab91dbf2e6ce10b82a3b5ae63
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Fri Mar 14 18:41:24 2014 -0400

    dm: introduce dm_accept_partial_bio

    The function dm_accept_partial_bio allows the target to specify how many
    sectors of the current bio it will process.  If the target only wants to
    accept part of the bio, it calls dm_accept_partial_bio and the DM core
    sends the rest of the data in next bio.

    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/md/dm.c b/drivers/md/dm.c
index b5efd14..886b001 100644
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@ -1121,6 +1121,46 @@ int dm_set_target_max_io_len(struct dm_target *ti, sector_t len)
 }
 EXPORT_SYMBOL_GPL(dm_set_target_max_io_len);
 
+/*
+ * A target may call dm_accept_partial_bio only from the map routine.  It is
+ * allowed for all bio types except REQ_FLUSH.
+ *
+ * dm_accept_partial_bio informs the dm that the target only wants to process
+ * additional n_sectors sectors of the bio and the rest of the data should be
+ * sent in a next bio.
+ *
+ * A diagram that explains the arithmetics:
+ * +--------------------+---------------+-------+
+ * |         1          |       2       |   3   |
+ * +--------------------+---------------+-------+
+ *
+ * <-------------- *tio->len_ptr --------------->
+ *                      <------- bi_size ------->
+ *                      <-- n_sectors -->
+ *
+ * Region 1 was already iterated over with bio_advance or similar function.
+ * (it may be empty if the target doesn't use bio_advance)
+ * Region 2 is the remaining bio size that the target wants to process.
+ * (it may be empty if region 1 is non-empty, although there is no reason
+ *  to make it empty)
+ * The target requires that region 3 is to be sent in the next bio.
+ *
+ * If the target wants to receive multiple copies of the bio (via num_*bios, etc),
+ * the partially processed part (the sum of regions 1+2) must be the same for all
+ * copies of the bio.
+ */
+void dm_accept_partial_bio(struct bio *bio, unsigned n_sectors)
+{
+ struct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);
+ unsigned bi_size = bio->bi_size >> SECTOR_SHIFT;
+ BUG_ON(bio->bi_rw & REQ_FLUSH);
+ BUG_ON(bi_size > *tio->len_ptr);
+ BUG_ON(n_sectors > bi_size);
+ *tio->len_ptr -= bi_size - n_sectors;
+ bio->bi_size = n_sectors << SECTOR_SHIFT;
+}
+EXPORT_SYMBOL_GPL(dm_accept_partial_bio);
+
 static void __map_bio(struct dm_target_io *tio)
 {
  int r;
@@ -1255,11 +1295,13 @@ static struct dm_target_io *alloc_tio(struct clone_info *ci,
 
 static void __clone_and_map_simple_bio(struct clone_info *ci,
            struct dm_target *ti,
-           unsigned target_bio_nr, unsigned len)
+           unsigned target_bio_nr, unsigned *len)
 {
  struct dm_target_io *tio = alloc_tio(ci, ti, ci->bio->bi_max_vecs, target_bio_nr);
  struct bio *clone = &tio->clone;
 
+ tio->len_ptr = len;
+
  /*
   * Discard requests require the bio's inline iovecs be initialized.
   * ci->bio->bi_max_vecs is BIO_INLINE_VECS anyway, for both flush
@@ -1267,13 +1309,13 @@ static void __clone_and_map_simple_bio(struct clone_info *ci,
   */
   __bio_clone(clone, ci->bio);
  if (len)
-  bio_setup_sector(clone, ci->sector, len);
+  bio_setup_sector(clone, ci->sector, *len);
 
  __map_bio(tio);
 }
 
 static void __send_duplicate_bios(struct clone_info *ci, struct dm_target *ti,
-      unsigned num_bios, unsigned len)
+      unsigned num_bios, unsigned *len)
 {
  unsigned target_bio_nr;
 
@@ -1288,7 +1330,7 @@ static int __send_empty_flush(struct clone_info *ci)
 
  BUG_ON(bio_has_data(ci->bio));
  while ((ti = dm_table_get_target(ci->map, target_nr++)))
-  __send_duplicate_bios(ci, ti, ti->num_flush_bios, 0);
+  __send_duplicate_bios(ci, ti, ti->num_flush_bios, NULL);
 
  return 0;
 }
@@ -1296,7 +1338,7 @@ static int __send_empty_flush(struct clone_info *ci)
 static void __clone_and_map_data_bio(struct clone_info *ci, struct dm_target *ti,
          sector_t sector, int nr_iovecs,
          unsigned short idx, unsigned short bv_count,
-         unsigned offset, unsigned len,
+         unsigned offset, unsigned *len,
          unsigned split_bvec)
 {
  struct bio *bio = ci->bio;
@@ -1312,10 +1354,11 @@ static void __clone_and_map_data_bio(struct clone_info *ci, struct dm_target *ti
 
  for (target_bio_nr = 0; target_bio_nr < num_target_bios; target_bio_nr++) {
   tio = alloc_tio(ci, ti, nr_iovecs, target_bio_nr);
+  tio->len_ptr = len;
   if (split_bvec)
-   clone_split_bio(tio, bio, sector, idx, offset, len);
+   clone_split_bio(tio, bio, sector, idx, offset, *len);
   else
-   clone_bio(tio, bio, sector, idx, bv_count, len);
+   clone_bio(tio, bio, sector, idx, bv_count, *len);
   __map_bio(tio);
  }
 }
@@ -1367,7 +1410,7 @@ static int __send_changing_extent_only(struct clone_info *ci,
   else
    len = min((sector_t)ci->sector_count, max_io_len(ci->sector, ti));
 
-  __send_duplicate_bios(ci, ti, num_bios, len);
+  __send_duplicate_bios(ci, ti, num_bios, &len);
 
   ci->sector += len;
  } while (ci->sector_count -= len);
@@ -1428,7 +1471,7 @@ static int __split_bvec_across_targets(struct clone_info *ci,
   len = min(remaining, max);
 
   __clone_and_map_data_bio(ci, ti, ci->sector, 1, ci->idx, 0,
-      bv->bv_offset + offset, len, 1);
+      bv->bv_offset + offset, &len, 1);
 
   ci->sector += len;
   ci->sector_count -= len;
@@ -1469,7 +1512,7 @@ static int __split_and_process_non_flush(struct clone_info *ci)
  if (ci->sector_count <= max) {
   __clone_and_map_data_bio(ci, ti, ci->sector, bio->bi_max_vecs,
       ci->idx, bio->bi_vcnt - ci->idx, 0,
-      ci->sector_count, 0);
+      &ci->sector_count, 0);
   ci->sector_count = 0;
   return 0;
  }
@@ -1482,7 +1525,7 @@ static int __split_and_process_non_flush(struct clone_info *ci)
   len = __len_within_target(ci, max, &idx);
 
   __clone_and_map_data_bio(ci, ti, ci->sector, bio->bi_max_vecs,
-      ci->idx, idx - ci->idx, 0, len, 0);
+      ci->idx, idx - ci->idx, 0, &len, 0);
 
   ci->sector += len;
   ci->sector_count -= len;
diff --git a/include/linux/device-mapper.h b/include/linux/device-mapper.h
index ecda6e8..730751e 100644
--- a/include/linux/device-mapper.h
+++ b/include/linux/device-mapper.h
@@ -285,6 +285,7 @@ struct dm_target_io {
  struct dm_io *io;
  struct dm_target *ti;
  unsigned target_bio_nr;
+ unsigned *len_ptr;
  struct bio clone;
 };
 
@@ -395,6 +396,7 @@ int dm_copy_name_and_uuid(struct mapped_device *md, char *name, char *uuid);
 struct gendisk *dm_disk(struct mapped_device *md);
 int dm_suspended(struct dm_target *ti);
 int dm_noflush_suspending(struct dm_target *ti);
+void dm_accept_partial_bio(struct bio *bio, unsigned n_sectors);
 union map_info *dm_get_rq_mapinfo(struct request *rq);
 
 struct queue_limits *dm_get_queue_limits(struct mapped_device *md);
-- 
1.7.1