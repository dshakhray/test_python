From fdae42e99a160f84fa11d897dbcf0030ae0444e6 Mon Sep 17 00:00:00 2001
From: Larry Woodman <lwoodman@redhat.com>
Date: Tue, 23 Sep 2014 19:40:12 -0400
Subject: [kernel] sched/fair: Implement fast idling of CPUs when the system is partially loaded

Message-id: <1411501213-15939-3-git-send-email-lwoodman@redhat.com>
Patchwork-id: 96504
O-Subject: [RHEL7.1 PATCH 2/3] sched/fair: Implement fast idling of CPUs when the system is partially loaded
Bugzilla: 1120318
RH-Acked-by: Aristeu Rozanski <aris@redhat.com>

commit 4486edd12b5ac8a9af7a5e16e4b9eeb3b8339c10
 Author: Tim Chen <tim.c.chen@linux.intel.com>
 Date:   Mon Jun 23 12:16:49 2014 -0700

 sched/fair: Implement fast idling of CPUs when the system is partially loaded

 When a system is lightly loaded (i.e. no more than 1 job per cpu),
 attempt to pull job to a cpu before putting it to idle is unnecessary
 and can be skipped.  This patch adds an indicator so the scheduler can know
 when there's no more than 1 active job is on any CPU in the system to
 skip needless job pulls.

 On a 4 socket machine with a request/response kind of workload from
 clients, we saw about 0.13 msec delay when we go through a full load
 balance to try pull job from all the other cpus.  While 0.1 msec was
 spent on processing the request and generating a response, the 0.13 msec
 load balance overhead was actually more than the actual work being done.
 This overhead can be skipped much of the time for lightly loaded
 systems.

 With this patch, we tested with a netperf request/response workload that
 has the server busy with half the cpus in a 4 socket system.  We found
 the patch eliminated 75% of the load balance attempts before idling a
 cpu.

 The overhead of setting/clearing the indicator is low as we already
 gather the necessary info while we call add_nr_running() and
 update_sd_lb_stats.() We switch to full load balance load immediately
 if any cpu got more than one job on its run queue in add_nr_running.
 We'll clear the indicator to avoid load balance when we detect no cpu's
 have more than one job when we scan the work queues in update_sg_lb_stats().
 We are aggressive in turning on the load balance and opportunistic in skipping
 the load balance.

 ***  Fixed a kABI breaker in the struct root_domain ***

Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
Acked-by: Rik van Riel <riel@redhat.com>
Acked-by: Jason Low <jason.low2@hp.com>
Cc: "Paul E.McKenney" <paulmck@linux.vnet.ibm.com>
Cc: Andrew Morton <akpm@linux-foundation.org>
Cc: Davidlohr Bueso <davidlohr@hp.com>
Cc: Alex Shi <alex.shi@linaro.org>
Cc: Michel Lespinasse <walken@google.com>
Cc: Peter Hurley <peter@hurleysoftware.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1403551009.2970.613.camel@schen9-DESK
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 19bfaa0..5ffa7a2 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5585,7 +5585,8 @@ static inline int sg_imbalanced(struct sched_group *group)
  */
 static inline void update_sg_lb_stats(struct lb_env *env,
    struct sched_group *group, int load_idx,
-   int local_group, struct sg_lb_stats *sgs)
+   int local_group, struct sg_lb_stats *sgs,
+   bool *overload)
 {
  unsigned long nr_running;
  unsigned long load;
@@ -5604,6 +5605,10 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 
   sgs->group_load += load;
   sgs->sum_nr_running += nr_running;
+
+  if (rq->nr_running > 1)
+   *overload = true;
+
 #ifdef CONFIG_NUMA_BALANCING
   sgs->nr_numa_running += rq->nr_numa_running;
   sgs->nr_preferred_running += rq->nr_preferred_running;
@@ -5724,6 +5729,7 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
  struct sched_group *sg = env->sd->groups;
  struct sg_lb_stats tmp_sgs;
  int load_idx, prefer_sibling = 0;
+ bool overload = false;
 
  if (child && child->flags & SD_PREFER_SIBLING)
   prefer_sibling = 1;
@@ -5741,7 +5747,8 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
   }
 
   memset(sgs, 0, sizeof(*sgs));
-  update_sg_lb_stats(env, sg, load_idx, local_group, sgs);
+  update_sg_lb_stats(env, sg, load_idx, local_group, sgs,
+      &overload);
 
   /*
    * In case the child domain prefers tasks go to siblings
@@ -5771,6 +5778,13 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 
  if (env->sd->flags & SD_NUMA)
   env->fbq_type = fbq_classify_group(&sds->busiest_stat);
+
+ if (!env->sd->parent) {
+  /* update overload indicator if we are at root domain */
+  if (env->dst_rq->rd->overload != overload)
+   env->dst_rq->rd->overload = overload;
+ }
+
 }
 
 /**
@@ -6449,7 +6463,8 @@ void idle_balance(int this_cpu, struct rq *this_rq)
 
  this_rq->idle_stamp = rq_clock(this_rq);
 
- if (this_rq->avg_idle < sysctl_sched_migration_cost)
+ if (this_rq->avg_idle < sysctl_sched_migration_cost ||
+     !this_rq->rd->overload)
   return;
 
  /*
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7156b16..910a7bc 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -382,6 +382,10 @@ struct root_domain {
   */
  cpumask_var_t rto_mask;
  struct cpupri cpupri;
+#ifndef __GENKSYMS__
+ /* Indicate more than one runnable task for any CPU */
+ bool overload;
+#endif
 };
 
 extern struct root_domain def_root_domain;
@@ -1116,15 +1120,20 @@ static inline void inc_nr_running(struct rq *rq)
 {
  rq->nr_running++;
 
-#ifdef CONFIG_NO_HZ_FULL
  if (rq->nr_running == 2) {
+#ifdef CONFIG_SMP
+  if (!rq->rd->overload)
+   rq->rd->overload = true;
+#endif
+
+#ifdef CONFIG_NO_HZ_FULL
   if (tick_nohz_full_cpu(rq->cpu)) {
    /* Order rq->nr_running write against the IPI */
    smp_wmb();
    smp_send_reschedule(rq->cpu);
   }
-       }
 #endif
+ }
 }
 
 static inline void dec_nr_running(struct rq *rq)
-- 
1.7.1