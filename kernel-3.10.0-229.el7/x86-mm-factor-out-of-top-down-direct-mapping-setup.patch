From 78b6b82781a7d9a9ce80d6fcb70db36e7f894c9e Mon Sep 17 00:00:00 2001
From: Motohiro Kosaki <mkosaki@redhat.com>
Date: Wed, 10 Sep 2014 16:16:00 -0400
Subject: [x86] mm: factor out of top-down direct mapping setup

Message-id: <1410365775-5132-6-git-send-email-mkosaki@redhat.com>
Patchwork-id: 93497
O-Subject: [RHEL7 PATCH 05/20] x86/mm: factor out of top-down direct mapping setup
Bugzilla: 810042
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Larry Woodman <lwoodman@redhat.com>
RH-Acked-by: Rik van Riel <riel@redhat.com>

From: KOSAKI Motohiro <mkosaki@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=810042
Brew: https://brewweb.devel.redhat.com/taskinfo?taskID=7914549
target RHEL version: 7.1
upstream status: merged
changes from upstream: none

commit 0167d7d8b0beb4cf12076b47e4dc73897ae5acb0
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Nov 12 15:08:02 2013 -0800

    x86/mm: factor out of top-down direct mapping setup

    Create a new function memory_map_top_down to factor out of the top-down
    direct memory mapping pagetable setup.  This is also a preparation for the
    following patch, which will introduce the bottom-up memory mapping.  That
    said, we will put the two ways of pagetable setup into separate functions,
    and choose to use which way in init_mem_mapping, which makes the code more
    clear.

    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Thomas Renninger <trenn@suse.de>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Signed-off-by: KOSAKI Motohiro <mkosaki@redhat.com>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c
index 4eb71ff..fbb9773 100644
--- a/arch/x86/mm/init.c
+++ b/arch/x86/mm/init.c
@@ -418,27 +418,27 @@ static unsigned long __init get_new_step_size(unsigned long step_size)
  return step_size << 5;
 }
 
-void __init init_mem_mapping(void)
+/**
+ * memory_map_top_down - Map [map_start, map_end) top down
+ * @map_start: start address of the target memory range
+ * @map_end: end address of the target memory range
+ *
+ * This function will setup direct mapping for memory range
+ * [map_start, map_end) in top-down. That said, the page tables
+ * will be allocated at the end of the memory, and we map the
+ * memory in top-down.
+ */
+static void __init memory_map_top_down(unsigned long map_start,
+           unsigned long map_end)
 {
- unsigned long end, real_end, start, last_start;
+ unsigned long real_end, start, last_start;
  unsigned long step_size;
  unsigned long addr;
  unsigned long mapped_ram_size = 0;
  unsigned long new_mapped_ram_size;
 
- probe_page_size_mask();
-
-#ifdef CONFIG_X86_64
- end = max_pfn << PAGE_SHIFT;
-#else
- end = max_low_pfn << PAGE_SHIFT;
-#endif
-
- /* the ISA range is always mapped regardless of memory holes */
- init_memory_mapping(0, ISA_END_ADDRESS);
-
  /* xen has big range in reserved near end of ram, skip it at first.*/
- addr = memblock_find_in_range(ISA_END_ADDRESS, end, PMD_SIZE, PMD_SIZE);
+ addr = memblock_find_in_range(map_start, map_end, PMD_SIZE, PMD_SIZE);
  real_end = addr + PMD_SIZE;
 
  /* step_size need to be small so pgt_buf from BRK could cover it */
@@ -453,13 +453,13 @@ void __init init_mem_mapping(void)
   * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages
   * for page table.
   */
- while (last_start > ISA_END_ADDRESS) {
+ while (last_start > map_start) {
   if (last_start > step_size) {
    start = round_down(last_start - 1, step_size);
-   if (start < ISA_END_ADDRESS)
-    start = ISA_END_ADDRESS;
+   if (start < map_start)
+    start = map_start;
   } else
-   start = ISA_END_ADDRESS;
+   start = map_start;
   new_mapped_ram_size = init_range_memory_mapping(start,
        last_start);
   last_start = start;
@@ -470,8 +470,27 @@ void __init init_mem_mapping(void)
   mapped_ram_size += new_mapped_ram_size;
  }
 
- if (real_end < end)
-  init_range_memory_mapping(real_end, end);
+ if (real_end < map_end)
+  init_range_memory_mapping(real_end, map_end);
+}
+
+void __init init_mem_mapping(void)
+{
+ unsigned long end;
+
+ probe_page_size_mask();
+
+#ifdef CONFIG_X86_64
+ end = max_pfn << PAGE_SHIFT;
+#else
+ end = max_low_pfn << PAGE_SHIFT;
+#endif
+
+ /* the ISA range is always mapped regardless of memory holes */
+ init_memory_mapping(0, ISA_END_ADDRESS);
+
+ /* setup direct mapping for range [ISA_END_ADDRESS, end) in top-down*/
+ memory_map_top_down(ISA_END_ADDRESS, end);
 
 #ifdef CONFIG_X86_64
  if (max_pfn > max_low_pfn) {
-- 
1.7.1