From 187ca71af8d3974f84c7405dbb91873aff50794f Mon Sep 17 00:00:00 2001
From: Rob Clark <rclark@redhat.com>
Date: Wed, 17 Dec 2014 21:39:37 -0500
Subject: [drm] ttm: Fix possible stack overflow by recursive shrinker calls

Message-id: <1418852380-13061-87-git-send-email-rclark@redhat.com>
Patchwork-id: 102213
O-Subject: [RHEL7 drm 86/89] drm/ttm: Fix possible stack overflow by recursive shrinker calls.
Bugzilla: 1173317
RH-Acked-by: Jerome Glisse <jglisse@redhat.com>
RH-Acked-by: Dave Airlie <airlied@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1173317
Upstream: since topic/core-stuff-2014-08-15

commit 71336e011d1d2312bcbcaa8fcec7365024f3a95d

Author:     Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
AuthorDate: Sun Aug 3 20:02:03 2014 +0900
Commit:     Rob Clark <rclark@redhat.com>
CommitDate: Mon Dec 15 14:53:59 2014 -0500

    drm/ttm: Fix possible stack overflow by recursive shrinker calls.

    While ttm_dma_pool_shrink_scan() tries to take mutex before doing GFP_KERNEL
    allocation, ttm_pool_shrink_scan() does not do it. This can result in stack
    overflow if kmalloc() in ttm_page_pool_free() triggered recursion due to
    memory pressure.

      shrink_slab()
      => ttm_pool_shrink_scan()
         => ttm_page_pool_free()
            => kmalloc(GFP_KERNEL)
               => shrink_slab()
                  => ttm_pool_shrink_scan()
                     => ttm_page_pool_free()
                        => kmalloc(GFP_KERNEL)

    Change ttm_pool_shrink_scan() to do like ttm_dma_pool_shrink_scan() does.

    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: stable <stable@kernel.org> [2.6.35+]
    Signed-off-by: Dave Airlie <airlied@redhat.com>

Signed-off-by: Rob Clark <rclark@redhat.com>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/gpu/drm/ttm/ttm_page_alloc.c b/drivers/gpu/drm/ttm/ttm_page_alloc.c
index bd2a3b4..606b598 100644
--- a/drivers/gpu/drm/ttm/ttm_page_alloc.c
+++ b/drivers/gpu/drm/ttm/ttm_page_alloc.c
@@ -394,13 +394,16 @@ static int ttm_pool_get_num_unused_pages(void)
 static int ttm_pool_mm_shrink(struct shrinker *shrink,
          struct shrink_control *sc)
 {
- static atomic_t start_pool = ATOMIC_INIT(0);
+ static DEFINE_MUTEX(lock);
+ static unsigned start_pool;
  unsigned i;
- unsigned pool_offset = atomic_add_return(1, &start_pool);
+ unsigned pool_offset;
  struct ttm_page_pool *pool;
  int shrink_pages = sc->nr_to_scan;
 
- pool_offset = pool_offset % NUM_POOLS;
+ if (!mutex_trylock(&lock))
+  return 0;
+ pool_offset = ++start_pool % NUM_POOLS;
  /* select start pool in round robin fashion */
  for (i = 0; i < NUM_POOLS; ++i) {
   unsigned nr_free = shrink_pages;
@@ -409,6 +412,7 @@ static int ttm_pool_mm_shrink(struct shrinker *shrink,
   pool = &_manager->pools[(i + pool_offset)%NUM_POOLS];
   shrink_pages = ttm_page_pool_free(pool, nr_free);
  }
+ mutex_unlock(&lock);
  /* return estimated number of unused pages in pool */
  return ttm_pool_get_num_unused_pages();
 }
-- 
1.7.1