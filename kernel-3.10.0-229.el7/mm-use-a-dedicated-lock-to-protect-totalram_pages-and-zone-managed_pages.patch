From e1067e35f9fa9fecc0a928282355488512121134 Mon Sep 17 00:00:00 2001
From: Motohiro Kosaki <mkosaki@redhat.com>
Date: Tue, 2 Dec 2014 17:49:00 -0500
Subject: [mm] use a dedicated lock to protect totalram_pages and zone->managed_pages

Message-id: <1417542543-1642-3-git-send-email-mkosaki@redhat.com>
Patchwork-id: 101076
O-Subject: [RHEL7 PATCH 2/5] mm: use a dedicated lock to protect totalram_pages and zone->managed_pages
Bugzilla: 1156396
RH-Acked-by: Rafael Aquini <aquini@redhat.com>
RH-Acked-by: Larry Woodman <lwoodman@redhat.com>

From: KOSAKI Motohiro <mkosaki@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1156396
Upstream-tree: linus
Brew: https://brewweb.devel.redhat.com/taskinfo?taskID=8243943
Changes-from-upstream: none

commit c3d5f5f0c2bc4eabeaf49f1a21e1aeb965246cd2
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:03:14 2013 -0700

    mm: use a dedicated lock to protect totalram_pages and zone->managed_pages

    Currently lock_memory_hotplug()/unlock_memory_hotplug() are used to
    protect totalram_pages and zone->managed_pages.  Other than the memory
    hotplug driver, totalram_pages and zone->managed_pages may also be
    modified at runtime by other drivers, such as Xen balloon,
    virtio_balloon etc.  For those cases, memory hotplug lock is a little
    too heavy, so introduce a dedicated lock to protect totalram_pages and
    zone->managed_pages.

    Now we have a simplified locking rules totalram_pages and
    zone->managed_pages as:

    1) no locking for read accesses because they are unsigned long.
    2) no locking for write accesses at boot time in single-threaded context.
    3) serialize write accesses at runtime by acquiring the dedicated
       managed_page_count_lock.

    Also adjust zone->managed_pages when freeing reserved pages into the
    buddy system, to keep totalram_pages and zone->managed_pages in
    consistence.

    [akpm@linux-foundation.org: don't export adjust_managed_page_count to modules (for now)]
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Signed-off-by: KOSAKI Motohiro <mkosaki@redhat.com>
Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 61117b3..9dee9e8 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1538,6 +1538,7 @@ extern void free_initmem(void);
  */
 extern unsigned long free_reserved_area(unsigned long start, unsigned long end,
      int poison, char *s);
+
 #ifdef CONFIG_HIGHMEM
 /*
  * Free a highmem page into the buddy system, adjusting totalhigh_pages
@@ -1546,10 +1547,7 @@ extern unsigned long free_reserved_area(unsigned long start, unsigned long end,
 extern void free_highmem_page(struct page *page);
 #endif
 
-static inline void adjust_managed_page_count(struct page *page, long count)
-{
- totalram_pages += count;
-}
+extern void adjust_managed_page_count(struct page *page, long count);
 extern void mem_init_print_info(const char *str);
 
 /* Free the reserved page into the buddy system, so it gets managed. */
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index cd832a1..2d47b2d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -483,10 +483,16 @@ struct zone {
   * frequently read in proximity to zone->lock.  It's good to
   * give them a chance of being in the same cacheline.
   *
-  * Write access to present_pages and managed_pages at runtime should
-  * be protected by lock_memory_hotplug()/unlock_memory_hotplug().
-  * Any reader who can't tolerant drift of present_pages and
-  * managed_pages should hold memory hotplug lock to get a stable value.
+  * Write access to present_pages at runtime should be protected by
+  * lock_memory_hotplug()/unlock_memory_hotplug().  Any reader who can't
+  * tolerant drift of present_pages should hold memory hotplug lock to
+  * get a stable value.
+  *
+  * Read access to managed_pages should be safe because it's unsigned
+  * long. Write access to zone->managed_pages and totalram_pages are
+  * protected by managed_page_count_lock at runtime. Idealy only
+  * adjust_managed_page_count() should be used instead of directly
+  * touching zone->managed_pages and totalram_pages.
   */
  unsigned long  spanned_pages;
  unsigned long  present_pages;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 6adf43d..7d8d264 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -101,6 +101,9 @@ nodemask_t node_states[NR_NODE_STATES] __read_mostly = {
 };
 EXPORT_SYMBOL(node_states);
 
+/* Protect totalram_pages and zone->managed_pages */
+static DEFINE_SPINLOCK(managed_page_count_lock);
+
 unsigned long totalram_pages __read_mostly;
 unsigned long totalreserve_pages __read_mostly;
 /*
@@ -5273,6 +5276,14 @@ early_param("movablecore", cmdline_parse_movablecore);
 
 #endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
 
+void adjust_managed_page_count(struct page *page, long count)
+{
+ spin_lock(&managed_page_count_lock);
+ page_zone(page)->managed_pages += count;
+ totalram_pages += count;
+ spin_unlock(&managed_page_count_lock);
+}
+
 unsigned long free_reserved_area(unsigned long start, unsigned long end,
      int poison, char *s)
 {
-- 
1.7.1