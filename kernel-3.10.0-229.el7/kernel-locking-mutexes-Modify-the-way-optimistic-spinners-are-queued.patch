From 46ff1a3ccf33b4132391d27193dbf2df96741050 Mon Sep 17 00:00:00 2001
From: Larry Woodman <lwoodman@redhat.com>
Date: Fri, 30 May 2014 11:22:35 -0400
Subject: [kernel] locking/mutexes: Modify the way optimistic spinners are queued

Message-id: <1401448958-5278-8-git-send-email-lwoodman@redhat.com>
Patchwork-id: 81234
O-Subject: [RHEL7.1 PATCH 07/10] locking/mutexes: Modify the way optimistic spinners are queued
Bugzilla: 1087655 1087919 1087922
RH-Acked-by: Rafael Aquini <aquini@redhat.com>
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>

commit 47667fa1502e4d759df87e9cc7fbc0f202483361
 Author: Jason Low <jason.low2@hp.com>
 Date:   Tue Jan 28 11:13:13 2014 -0800

    locking/mutexes: Modify the way optimistic spinners are queued

    The mutex->spin_mlock was introduced in order to ensure that only 1 thread
    spins for lock acquisition at a time to reduce cache line contention. When
    lock->owner is NULL and the lock->count is still not 1, the spinner(s) will
    continually release and obtain the lock->spin_mlock. This can generate
    quite a bit of overhead/contention, and also might just delay the spinner
    from getting the lock.

    This patch modifies the way optimistic spinners are queued by queuing before
    entering the optimistic spinning loop as oppose to acquiring before every
    call to mutex_spin_on_owner(). So in situations where the spinner requires
    a few extra spins before obtaining the lock, then there will only be 1 spinner
    trying to get the lock and it will avoid the overhead from unnecessarily
    unlocking and locking the spin_mlock.

    Signed-off-by: Jason Low <jason.low2@hp.com>
    Cc: tglx@linutronix.de
    Cc: riel@redhat.com
    Cc: akpm@linux-foundation.org
    Cc: davidlohr@hp.com
    Cc: hpa@zytor.com
    Cc: andi@firstfloor.org
    Cc: aswin@hp.com
    Cc: scott.norton@hp.com
    Cc: chegu_vinod@hp.com
    Cc: Waiman.Long@hp.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: torvalds@linux-foundation.org
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1390936396-3962-3-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/mutex.c b/kernel/mutex.c
index e79515f..aacdea8 100644
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@ -377,6 +377,9 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
  struct mutex_waiter waiter;
  unsigned long flags;
  int ret;
+#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
+ struct mcs_spinlock node;
+#endif
 
  preempt_disable();
  mutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);
@@ -407,9 +410,9 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
  if (!mutex_can_spin_on_owner(lock))
   goto slowpath;
 
+ mcs_spin_lock(&lock->mcs_lock, &node);
  for (;;) {
   struct task_struct *owner;
-  struct mcs_spinlock  node;
 
   if (!__builtin_constant_p(ww_ctx == NULL) && ww_ctx->acquired > 0) {
    struct ww_mutex *ww;
@@ -424,19 +427,16 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
     * performed the optimistic spinning cannot be done.
     */
    if (ACCESS_ONCE(ww->ctx))
-    goto slowpath;
+    break;
   }
 
   /*
    * If there's an owner, wait for it to either
    * release the lock or go to sleep.
    */
-  mcs_spin_lock(&lock->mcs_lock, &node);
   owner = ACCESS_ONCE(lock->owner);
-  if (owner && !mutex_spin_on_owner(lock, owner)) {
-   mcs_spin_unlock(&lock->mcs_lock, &node);
-   goto slowpath;
-  }
+  if (owner && !mutex_spin_on_owner(lock, owner))
+   break;
 
   if ((atomic_read(&lock->count) == 1) &&
       (atomic_cmpxchg(&lock->count, 1, 0) == 1)) {
@@ -453,7 +453,6 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
    preempt_enable();
    return 0;
   }
-  mcs_spin_unlock(&lock->mcs_lock, &node);
 
   /*
    * When there's no owner, we might have preempted between the
@@ -462,7 +461,7 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
    * the owner complete.
    */
   if (!owner && (need_resched() || rt_task(task)))
-   goto slowpath;
+   break;
 
   /*
    * The cpu_relax() call is a compiler barrier which forces
@@ -472,6 +471,7 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
    */
   arch_mutex_cpu_relax();
  }
+ mcs_spin_unlock(&lock->mcs_lock, &node);
 slowpath:
 #endif
  spin_lock_mutex(&lock->wait_lock, flags);
-- 
1.7.1