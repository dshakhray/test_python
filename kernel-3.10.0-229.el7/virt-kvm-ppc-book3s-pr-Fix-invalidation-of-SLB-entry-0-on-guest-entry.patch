From d8b8894fca048f080896c0e2109e2f5058b9bfa2 Mon Sep 17 00:00:00 2001
From: Don Zickus <dzickus@redhat.com>
Date: Fri, 12 Sep 2014 18:05:34 -0400
Subject: [virt] kvm/ppc: book3s pr - Fix invalidation of SLB entry 0 on guest entry

Message-id: <1410545655-205645-106-git-send-email-dzickus@redhat.com>
Patchwork-id: 94233
O-Subject: [RHEL7 PATCH 105/626] KVM: PPC: Book3S PR: Fix invalidation of SLB entry 0 on guest entry
Bugzilla: 1127366
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Steve Best <sbest@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>
RH-Acked-by: David Gibson <dgibson@redhat.com>

https://bugzilla.redhat.com/show_bug.cgi?id=1127366

commit bc1bc4e3929d7a2e4078a9e48fe1e0bb853de1e6
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Jun 22 17:14:11 2013 +1000

    KVM: PPC: Book3S PR: Fix invalidation of SLB entry 0 on guest entry

    On entering a PR KVM guest, we invalidate the whole SLB before loading
    up the guest entries.  We do this using an slbia instruction, which
    invalidates all entries except entry 0, followed by an slbie to
    invalidate entry 0.  However, the slbie turns out to be ineffective
    in some circumstances (specifically when the host linear mapping uses
    64k pages) because of errors in computing the parameter to the slbie.
    The result is that the guest kernel hangs very early in boot because
    it takes a DSI the first time it tries to access kernel data using
    a linear mapping address in real mode.

    Currently we construct bits 36 - 43 (big-endian numbering) of the slbie
    parameter by taking bits 56 - 63 of the SLB VSID doubleword.  These bits
    for the tlbie are C (class, 1 bit), B (segment size, 2 bits) and 5
    reserved bits.  For the SLB VSID doubleword these are C (class, 1 bit),
    reserved (1 bit), LP (large page size, 2 bits), and 4 reserved bits.
    Thus we are not setting the B field correctly, and when LP = 01 as
    it is for 64k pages, we are setting a reserved bit.

    Rather than add more instructions to calculate the slbie parameter
    correctly, this takes a simpler approach, which is to set entry 0 to
    zeroes explicitly.  Normally slbmte should not be used to invalidate
    an entry, since it doesn't invalidate the ERATs, but it is OK to use
    it to invalidate an entry if it is immediately followed by slbia,
    which does invalidate the ERATs.  (This has been confirmed with the
    Power architects.)  This approach takes fewer instructions and will
    work whatever the contents of entry 0.

    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/kvm/book3s_64_slb.S b/arch/powerpc/kvm/book3s_64_slb.S
index e3007c3..4f12e8f 100644
--- a/arch/powerpc/kvm/book3s_64_slb.S
+++ b/arch/powerpc/kvm/book3s_64_slb.S
@@ -70,10 +70,6 @@ slb_exit_skip_ ## num:
 
  ld r12, PACA_SLBSHADOWPTR(r13)
 
- /* Save off the first entry so we can slbie it later */
- ld r10, SHADOW_SLB_ESID(0)(r12)
- ld r11, SHADOW_SLB_VSID(0)(r12)
-
  /* Remove bolted entries */
  UNBOLT_SLB_ENTRY(0)
  UNBOLT_SLB_ENTRY(1)
@@ -85,15 +81,10 @@ slb_exit_skip_ ## num:
 
  /* Flush SLB */
 
+ li r10, 0
+ slbmte r10, r10
  slbia
 
- /* r0 = esid & ESID_MASK */
- rldicr  r10, r10, 0, 35
- /* r0 |= CLASS_BIT(VSID) */
- rldic   r12, r11, 56 - 36, 36
- or      r10, r10, r12
- slbie r10
-
  /* Fill SLB with our shadow */
 
  lbz r12, SVCPU_SLB_MAX(r3)
-- 
1.7.1