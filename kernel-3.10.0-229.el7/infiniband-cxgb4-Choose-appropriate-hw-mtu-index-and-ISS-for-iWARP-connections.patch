From 1c9544016fe767d99dcebea058b90a93f386f77b Mon Sep 17 00:00:00 2001
From: Sai Vemuri <svemuri@redhat.com>
Date: Thu, 11 Sep 2014 22:43:29 -0400
Subject: [infiniband] cxgb4: Choose appropriate hw mtu index and ISS for iWARP connections

Message-id: <1410475447-94392-71-git-send-email-svemuri@redhat.com>
Patchwork-id: 93801
O-Subject: [RHEL7.1 PATCH BZ 1124947 070/108] iw_cxgb4: Choose appropriate hw mtu index and ISS for iWARP connections
Bugzilla: 1124947
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

commit 92e7ae71726ca9e16a8a88ebeee14eb5177575a1
Author: Hariprasad Shenai <hariprasad@chelsio.com>
Date:   Fri Jun 6 21:40:43 2014 +0530

    iw_cxgb4: Choose appropriate hw mtu index and ISS for iWARP connections

Select the appropriate hw mtu index and initial sequence number to optimize
hw memory performance.

Add new cxgb4_best_aligned_mtu() which allows callers to provide enough
information to be used to [possibly] select an MTU which will result in the
TCP Data Segment Size (AKA Maximum Segment Size) to be an aligned value.

If an RTR message exhange is required, then align the ISS to 8B - 1 + 4, so
that after the SYN the send seqno will align on a 4B boundary. The RTR
message exchange will leave the send seqno aligned on an 8B boundary.
If an RTR is not required, then align the ISS to 8B - 1.  The goal is
to have the send seqno be 8B aligned when we send the first FPDU.

Based on original work by Casey Leedom <leeedom@chelsio.com> and
Steve Wise <swise@opengridcomputing.com>

Signed-off-by: Casey Leedom <leedom@chelsio.com>
Signed-off-by: Steve Wise <swise@opengridcomputing.com>
Signed-off-by: Hariprasad Shenai <hariprasad@chelsio.com>
Signed-off-by: David S. Miller <davem@davemloft.net>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/infiniband/hw/cxgb4/cm.c b/drivers/infiniband/hw/cxgb4/cm.c
index 96d7131..2660e88 100644
--- a/drivers/infiniband/hw/cxgb4/cm.c
+++ b/drivers/infiniband/hw/cxgb4/cm.c
@@ -234,12 +234,16 @@ static void release_tid(struct c4iw_rdev *rdev, u32 hwtid, struct sk_buff *skb)
 
 static void set_emss(struct c4iw_ep *ep, u16 opt)
 {
- ep->emss = ep->com.dev->rdev.lldi.mtus[GET_TCPOPT_MSS(opt)] - 40;
+ ep->emss = ep->com.dev->rdev.lldi.mtus[GET_TCPOPT_MSS(opt)] -
+     sizeof(struct iphdr) - sizeof(struct tcphdr);
  ep->mss = ep->emss;
  if (GET_TCPOPT_TSTAMP(opt))
   ep->emss -= 12;
  if (ep->emss < 128)
   ep->emss = 128;
+ if (ep->emss & 7)
+  PDBG("Warning: misaligned mtu idx %u mss %u emss=%u\n",
+       GET_TCPOPT_MSS(opt), ep->mss, ep->emss);
  PDBG("%s mss_idx %u mss %u emss=%u\n", __func__, GET_TCPOPT_MSS(opt),
       ep->mss, ep->emss);
 }
@@ -565,6 +569,17 @@ static void c4iw_record_pm_msg(struct c4iw_ep *ep,
   sizeof(ep->com.mapped_remote_addr));
 }
 
+static void best_mtu(const unsigned short *mtus, unsigned short mtu,
+       unsigned int *idx, int use_ts)
+{
+ unsigned short hdr_size = sizeof(struct iphdr) +
+      sizeof(struct tcphdr) +
+      (use_ts ? 12 : 0);
+ unsigned short data_size = mtu - hdr_size;
+
+ cxgb4_best_aligned_mtu(mtus, hdr_size, data_size, 8, idx);
+}
+
 static int send_connect(struct c4iw_ep *ep)
 {
  struct cpl_act_open_req *req;
@@ -606,7 +621,8 @@ static int send_connect(struct c4iw_ep *ep)
  }
  set_wr_txq(skb, CPL_PRIORITY_SETUP, ep->ctrlq_idx);
 
- cxgb4_best_mtu(ep->com.dev->rdev.lldi.mtus, ep->mtu, &mtu_idx);
+ best_mtu(ep->com.dev->rdev.lldi.mtus, ep->mtu, &mtu_idx,
+   enable_tcp_timestamps);
  wscale = compute_wscale(rcv_win);
  opt0 = (nocong ? NO_CONG(1) : 0) |
         KEEP_ALIVE(1) |
@@ -674,6 +690,13 @@ static int send_connect(struct c4iw_ep *ep)
    req6->opt2 = cpu_to_be32(opt2);
   }
  } else {
+  u32 isn = (prandom_u32() & ~7UL) - 1;
+
+  opt2 |= T5_OPT_2_VALID;
+  opt2 |= CONG_CNTRL_VALID; /* OPT_2_ISS for T5 */
+  if (peer2peer)
+   isn += 4;
+
   if (ep->com.remote_addr.ss_family == AF_INET) {
    t5_req = (struct cpl_t5_act_open_req *)
      skb_put(skb, wrlen);
@@ -690,6 +713,9 @@ static int send_connect(struct c4iw_ep *ep)
            cxgb4_select_ntuple(
           ep->com.dev->rdev.lldi.ports[0],
           ep->l2t)));
+   t5_req->rsvd = cpu_to_be32(isn);
+   PDBG("%s snd_isn %u\n", __func__,
+        be32_to_cpu(t5_req->rsvd));
    t5_req->opt2 = cpu_to_be32(opt2);
   } else {
    t5_req6 = (struct cpl_t5_act_open_req6 *)
@@ -713,6 +739,9 @@ static int send_connect(struct c4iw_ep *ep)
        cxgb4_select_ntuple(
       ep->com.dev->rdev.lldi.ports[0],
       ep->l2t));
+   t5_req6->rsvd = cpu_to_be32(isn);
+   PDBG("%s snd_isn %u\n", __func__,
+        be32_to_cpu(t5_req6->rsvd));
    t5_req6->opt2 = cpu_to_be32(opt2);
   }
  }
@@ -1681,7 +1710,8 @@ static void send_fw_act_open_req(struct c4iw_ep *ep, unsigned int atid)
    htons(F_FW_OFLD_CONNECTION_WR_CPLRXDATAACK);
  req->tcb.tx_max = (__force __be32) jiffies;
  req->tcb.rcv_adv = htons(1);
- cxgb4_best_mtu(ep->com.dev->rdev.lldi.mtus, ep->mtu, &mtu_idx);
+ best_mtu(ep->com.dev->rdev.lldi.mtus, ep->mtu, &mtu_idx,
+   enable_tcp_timestamps);
  wscale = compute_wscale(rcv_win);
  req->tcb.opt0 = (__force __be64) (TCAM_BYPASS(1) |
   (nocong ? NO_CONG(1) : 0) |
@@ -2027,12 +2057,26 @@ static void accept_cr(struct c4iw_ep *ep, struct sk_buff *skb,
  u64 opt0;
  u32 opt2;
  int wscale;
+ struct cpl_t5_pass_accept_rpl *rpl5 = NULL;
 
  PDBG("%s ep %p tid %u\n", __func__, ep, ep->hwtid);
  BUG_ON(skb_cloned(skb));
- skb_trim(skb, sizeof(*rpl));
+
  skb_get(skb);
- cxgb4_best_mtu(ep->com.dev->rdev.lldi.mtus, ep->mtu, &mtu_idx);
+ rpl = cplhdr(skb);
+ if (is_t5(ep->com.dev->rdev.lldi.adapter_type)) {
+  skb_trim(skb, roundup(sizeof(*rpl5), 16));
+  rpl5 = (void *)rpl;
+  INIT_TP_WR(rpl5, ep->hwtid);
+ } else {
+  skb_trim(skb, sizeof(*rpl));
+  INIT_TP_WR(rpl, ep->hwtid);
+ }
+ OPCODE_TID(rpl) = cpu_to_be32(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL,
+          ep->hwtid));
+
+ best_mtu(ep->com.dev->rdev.lldi.mtus, ep->mtu, &mtu_idx,
+   enable_tcp_timestamps && req->tcpopt.tstamp);
  wscale = compute_wscale(rcv_win);
  opt0 = (nocong ? NO_CONG(1) : 0) |
         KEEP_ALIVE(1) |
@@ -2064,14 +2108,18 @@ static void accept_cr(struct c4iw_ep *ep, struct sk_buff *skb,
    opt2 |= CCTRL_ECN(1);
  }
  if (is_t5(ep->com.dev->rdev.lldi.adapter_type)) {
+  u32 isn = (prandom_u32() & ~7UL) - 1;
   opt2 |= T5_OPT_2_VALID;
   opt2 |= V_CONG_CNTRL(CONG_ALG_TAHOE);
+  opt2 |= CONG_CNTRL_VALID; /* OPT_2_ISS for T5 */
+  rpl5 = (void *)rpl;
+  memset(&rpl5->iss, 0, roundup(sizeof(*rpl5)-sizeof(*rpl), 16));
+  if (peer2peer)
+   isn += 4;
+  rpl5->iss = cpu_to_be32(isn);
+  PDBG("%s iss %u\n", __func__, be32_to_cpu(rpl5->iss));
  }
 
- rpl = cplhdr(skb);
- INIT_TP_WR(rpl, ep->hwtid);
- OPCODE_TID(rpl) = cpu_to_be32(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL,
-          ep->hwtid));
  rpl->opt0 = cpu_to_be64(opt0);
  rpl->opt2 = cpu_to_be32(opt2);
  set_wr_txq(skb, CPL_PRIORITY_SETUP, ep->ctrlq_idx);
@@ -2136,6 +2184,7 @@ static int pass_accept_req(struct c4iw_dev *dev, struct sk_buff *skb)
  int err;
  u16 peer_mss = ntohs(req->tcpopt.mss);
  int iptype;
+ unsigned short hdrs;
 
  parent_ep = lookup_stid(t, stid);
  if (!parent_ep) {
@@ -2193,8 +2242,10 @@ static int pass_accept_req(struct c4iw_dev *dev, struct sk_buff *skb)
   goto reject;
  }
 
- if (peer_mss && child_ep->mtu > (peer_mss + 40))
-  child_ep->mtu = peer_mss + 40;
+ hdrs = sizeof(struct iphdr) + sizeof(struct tcphdr) +
+        ((enable_tcp_timestamps && req->tcpopt.tstamp) ? 12 : 0);
+ if (peer_mss && child_ep->mtu > (peer_mss + hdrs))
+  child_ep->mtu = peer_mss + hdrs;
 
  state_set(&child_ep->com, CONNECTING);
  child_ep->com.dev = dev;
diff --git a/drivers/infiniband/hw/cxgb4/t4fw_ri_api.h b/drivers/infiniband/hw/cxgb4/t4fw_ri_api.h
index efad0cc..df0bad8 100644
--- a/drivers/infiniband/hw/cxgb4/t4fw_ri_api.h
+++ b/drivers/infiniband/hw/cxgb4/t4fw_ri_api.h
@@ -848,6 +848,7 @@ enum {                     /* TCP congestion control algorithms */
 #define V_CONG_CNTRL(x) ((x) << S_CONG_CNTRL)
 #define G_CONG_CNTRL(x) (((x) >> S_CONG_CNTRL) & M_CONG_CNTRL)
 
+#define CONG_CNTRL_VALID   (1 << 18)
 #define T5_OPT_2_VALID      ((1U) << 31)
 
 #endif /* _T4FW_RI_API_H_ */
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
index f3175d1..f341e20 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
@@ -3428,6 +3428,77 @@ unsigned int cxgb4_best_mtu(const unsigned short *mtus, unsigned short mtu,
 EXPORT_SYMBOL(cxgb4_best_mtu);
 
 /**
+ *     cxgb4_best_aligned_mtu - find best MTU, [hopefully] data size aligned
+ *     @mtus: the HW MTU table
+ *     @header_size: Header Size
+ *     @data_size_max: maximum Data Segment Size
+ *     @data_size_align: desired Data Segment Size Alignment (2^N)
+ *     @mtu_idxp: HW MTU Table Index return value pointer (possibly NULL)
+ *
+ *     Similar to cxgb4_best_mtu() but instead of searching the Hardware
+ *     MTU Table based solely on a Maximum MTU parameter, we break that
+ *     parameter up into a Header Size and Maximum Data Segment Size, and
+ *     provide a desired Data Segment Size Alignment.  If we find an MTU in
+ *     the Hardware MTU Table which will result in a Data Segment Size with
+ *     the requested alignment _and_ that MTU isn't "too far" from the
+ *     closest MTU, then we'll return that rather than the closest MTU.
+ */
+unsigned int cxgb4_best_aligned_mtu(const unsigned short *mtus,
+        unsigned short header_size,
+        unsigned short data_size_max,
+        unsigned short data_size_align,
+        unsigned int *mtu_idxp)
+{
+ unsigned short max_mtu = header_size + data_size_max;
+ unsigned short data_size_align_mask = data_size_align - 1;
+ int mtu_idx, aligned_mtu_idx;
+
+ /* Scan the MTU Table till we find an MTU which is larger than our
+  * Maximum MTU or we reach the end of the table.  Along the way,
+  * record the last MTU found, if any, which will result in a Data
+  * Segment Length matching the requested alignment.
+  */
+ for (mtu_idx = 0, aligned_mtu_idx = -1; mtu_idx < NMTUS; mtu_idx++) {
+  unsigned short data_size = mtus[mtu_idx] - header_size;
+
+  /* If this MTU minus the Header Size would result in a
+   * Data Segment Size of the desired alignment, remember it.
+   */
+  if ((data_size & data_size_align_mask) == 0)
+   aligned_mtu_idx = mtu_idx;
+
+  /* If we're not at the end of the Hardware MTU Table and the
+   * next element is larger than our Maximum MTU, drop out of
+   * the loop.
+   */
+  if (mtu_idx+1 < NMTUS && mtus[mtu_idx+1] > max_mtu)
+   break;
+ }
+
+ /* If we fell out of the loop because we ran to the end of the table,
+  * then we just have to use the last [largest] entry.
+  */
+ if (mtu_idx == NMTUS)
+  mtu_idx--;
+
+ /* If we found an MTU which resulted in the requested Data Segment
+  * Length alignment and that's "not far" from the largest MTU which is
+  * less than or equal to the maximum MTU, then use that.
+  */
+ if (aligned_mtu_idx >= 0 &&
+     mtu_idx - aligned_mtu_idx <= 1)
+  mtu_idx = aligned_mtu_idx;
+
+ /* If the caller has passed in an MTU Index pointer, pass the
+  * MTU Index back.  Return the MTU value.
+  */
+ if (mtu_idxp)
+  *mtu_idxp = mtu_idx;
+ return mtus[mtu_idx];
+}
+EXPORT_SYMBOL(cxgb4_best_aligned_mtu);
+
+/**
  * cxgb4_port_chan - get the HW channel of a port
  * @dev: the net device for the port
  *
@@ -5572,13 +5643,41 @@ static int adap_init0(struct adapter *adap)
 #undef FW_PARAM_PFVF
 #undef FW_PARAM_DEV
 
- /*
-  * These are finalized by FW initialization, load their values now.
+ /* The MTU/MSS Table is initialized by now, so load their values.  If
+  * we're initializing the adapter, then we'll make any modifications
+  * we want to the MTU/MSS Table and also initialize the congestion
+  * parameters.
   */
  t4_read_mtu_tbl(adap, adap->params.mtus, NULL);
- t4_load_mtus(adap, adap->params.mtus, adap->params.a_wnd,
-       adap->params.b_wnd);
+ if (state != DEV_STATE_INIT) {
+  int i;
+
+  /* The default MTU Table contains values 1492 and 1500.
+   * However, for TCP, it's better to have two values which are
+   * a multiple of 8 +/- 4 bytes apart near this popular MTU.
+   * This allows us to have a TCP Data Payload which is a
+   * multiple of 8 regardless of what combination of TCP Options
+   * are in use (always a multiple of 4 bytes) which is
+   * important for performance reasons.  For instance, if no
+   * options are in use, then we have a 20-byte IP header and a
+   * 20-byte TCP header.  In this case, a 1500-byte MSS would
+   * result in a TCP Data Payload of 1500 - 40 == 1460 bytes
+   * which is not a multiple of 8.  So using an MSS of 1488 in
+   * this case results in a TCP Data Payload of 1448 bytes which
+   * is a multiple of 8.  On the other hand, if 12-byte TCP Time
+   * Stamps have been negotiated, then an MTU of 1500 bytes
+   * results in a TCP Data Payload of 1448 bytes which, as
+   * above, is a multiple of 8 bytes ...
+   */
+  for (i = 0; i < NMTUS; i++)
+   if (adap->params.mtus[i] == 1492) {
+    adap->params.mtus[i] = 1488;
+    break;
+   }
 
+  t4_load_mtus(adap, adap->params.mtus, adap->params.a_wnd,
+        adap->params.b_wnd);
+ }
  t4_init_tp_params(adap);
  adap->flags |= FW_OK;
  return 0;
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
index 87af314..55e9daf 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
@@ -276,6 +276,11 @@ unsigned int cxgb4_port_viid(const struct net_device *dev);
 unsigned int cxgb4_port_idx(const struct net_device *dev);
 unsigned int cxgb4_best_mtu(const unsigned short *mtus, unsigned short mtu,
        unsigned int *idx);
+unsigned int cxgb4_best_aligned_mtu(const unsigned short *mtus,
+        unsigned short header_size,
+        unsigned short data_size_max,
+        unsigned short data_size_align,
+        unsigned int *mtu_idxp);
 void cxgb4_get_tcp_stats(struct pci_dev *pdev, struct tp_tcp_stats *v4,
     struct tp_tcp_stats *v6);
 void cxgb4_iscsi_init(struct net_device *dev, unsigned int tag_mask,
diff --git a/drivers/net/ethernet/chelsio/cxgb4/t4_msg.h b/drivers/net/ethernet/chelsio/cxgb4/t4_msg.h
index f2738c7..a4d7bdb 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/t4_msg.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/t4_msg.h
@@ -278,6 +278,15 @@ struct cpl_pass_accept_rpl {
  __be64 opt0;
 };
 
+struct cpl_t5_pass_accept_rpl {
+ WR_HDR;
+ union opcode_tid ot;
+ __be32 opt2;
+ __be64 opt0;
+ __be32 iss;
+ __be32 rsvd;
+};
+
 struct cpl_act_open_req {
  WR_HDR;
  union opcode_tid ot;
-- 
1.7.1