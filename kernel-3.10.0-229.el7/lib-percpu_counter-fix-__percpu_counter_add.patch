From b4fc8ee0a3d8027472c6995af98aa5803a4d7a6e Mon Sep 17 00:00:00 2001
From: Eric Sandeen <sandeen@redhat.com>
Date: Tue, 29 Jul 2014 18:34:48 -0400
Subject: [lib] percpu_counter: fix __percpu_counter_add()

Message-id: <1406658889-9265-2-git-send-email-sandeen@redhat.com>
Patchwork-id: 86825
O-Subject: [RHEL7.1 PATCH 1/2] lib/percpu_counter.c: fix __percpu_counter_add()
Bugzilla: 1123968
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Rik van Riel <riel@redhat.com>
RH-Acked-by: Zach Brown <zab@redhat.com>

From: Ming Lei <tom.leiming@gmail.com>

commit 74e72f894d56eb9d2e1218530c658e7d297e002b
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Tue Jan 14 17:56:42 2014 -0800

    lib/percpu_counter.c: fix __percpu_counter_add()

    __percpu_counter_add() may be called in softirq/hardirq handler (such
    as, blk_mq_queue_exit() is typically called in hardirq/softirq handler),
    so we need to call this_cpu_add()(irq safe helper) to update percpu
    counter, otherwise counts may be lost.

    This fixes the problem that 'rmmod null_blk' hangs in blk_cleanup_queue()
    because of miscounting of request_queue->mq_usage_counter.

    This patch is the v1 of previous one of "lib/percpu_counter.c:
    disable local irq when updating percpu couter", and takes Andrew's
    approach which may be more efficient for ARCHs(x86, s390) that
    have optimized this_cpu_add().

    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Shaohua Li <shli@fusionio.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Fan Du <fan.du@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Eric Sandeen <sandeen@redhat.com>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 7473ee3..1da85bb 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -82,10 +82,10 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
   unsigned long flags;
   raw_spin_lock_irqsave(&fbc->lock, flags);
   fbc->count += count;
+   __this_cpu_sub(*fbc->counters, count);
   raw_spin_unlock_irqrestore(&fbc->lock, flags);
-  __this_cpu_write(*fbc->counters, 0);
  } else {
-  __this_cpu_write(*fbc->counters, count);
+  this_cpu_add(*fbc->counters, amount);
  }
  preempt_enable();
 }
-- 
1.7.1