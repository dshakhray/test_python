From a6c44a7b68fb71ff2eaa0efc2df4a51e376b5147 Mon Sep 17 00:00:00 2001
From: Prarit Bhargava <prarit@redhat.com>
Date: Mon, 14 Jul 2014 13:41:40 -0400
Subject: [x86] mce: Fix CMCI preemption bugs

Message-id: <1405345303-24468-11-git-send-email-prarit@redhat.com>
Patchwork-id: 85782
O-Subject: [RHEL7.1 PATCH BZ 1119306 10/13] x86/mce: Fix CMCI preemption bugs
Bugzilla: 1119306
RH-Acked-by: Dean Nelson <dnelson@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1119306

commit ea431643d6c38728195e2c456801c3ef66bb9991
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 17 10:25:53 2014 +0200

    x86/mce: Fix CMCI preemption bugs

    The following commit:

      27f6c573e0f7 ("x86, CMCI: Add proper detection of end of CMCI storms")

    Added two preemption bugs:

     - machine_check_poll() does a get_cpu_var() without a matching
       put_cpu_var(), which causes preemption imbalance and crashes upon
       bootup.

     - it does percpu ops without disabling preemption. Preemption is not
       disabled due to the mistaken use of a raw spinlock.

    To fix these bugs fix the imbalance and change
    cmci_discover_lock to a regular spinlock.

    Reported-by: Owen Kibel <qmewlo@gmail.com>
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Chen, Gong <gong.chen@linux.intel.com>
    Cc: Josh Boyer <jwboyer@fedoraproject.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexander Todorov <atodorov@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/n/tip-jtjptvgigpfkpvtQxpEk1at2@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    --

Cc: Tony Camuso <tcamuso@redhat.com>
Cc: Dean Nelson <dnelson@redhat.com>
Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/x86/kernel/cpu/mcheck/mce.c b/arch/x86/kernel/cpu/mcheck/mce.c
index eeee23f..68317c8 100644
--- a/arch/x86/kernel/cpu/mcheck/mce.c
+++ b/arch/x86/kernel/cpu/mcheck/mce.c
@@ -598,7 +598,6 @@ void machine_check_poll(enum mcp_flags flags, mce_banks_t *b)
 {
  struct mce m;
  int i;
- unsigned long *v;
 
  this_cpu_inc(mce_poll_count);
 
@@ -618,8 +617,7 @@ void machine_check_poll(enum mcp_flags flags, mce_banks_t *b)
   if (!(m.status & MCI_STATUS_VAL))
    continue;
 
-  v = &get_cpu_var(mce_polled_error);
-  set_bit(0, v);
+  this_cpu_write(mce_polled_error, 1);
   /*
    * Uncorrected or signalled events are handled by the exception
    * handler when it is enabled, so don't process those here.
diff --git a/arch/x86/kernel/cpu/mcheck/mce_intel.c b/arch/x86/kernel/cpu/mcheck/mce_intel.c
index 177cd48..cb674d1 100644
--- a/arch/x86/kernel/cpu/mcheck/mce_intel.c
+++ b/arch/x86/kernel/cpu/mcheck/mce_intel.c
@@ -43,7 +43,7 @@ static DEFINE_PER_CPU(mce_banks_t, mce_banks_owned);
  * cmci_discover_lock protects against parallel discovery attempts
  * which could race against each other.
  */
-static DEFINE_RAW_SPINLOCK(cmci_discover_lock);
+static DEFINE_SPINLOCK(cmci_discover_lock);
 
 #define CMCI_THRESHOLD  1
 #define CMCI_POLL_INTERVAL (30 * HZ)
@@ -145,14 +145,14 @@ static void cmci_storm_disable_banks(void)
  int bank;
  u64 val;
 
- raw_spin_lock_irqsave(&cmci_discover_lock, flags);
+ spin_lock_irqsave(&cmci_discover_lock, flags);
  owned = __get_cpu_var(mce_banks_owned);
  for_each_set_bit(bank, owned, MAX_NR_BANKS) {
   rdmsrl(MSR_IA32_MCx_CTL2(bank), val);
   val &= ~MCI_CTL2_CMCI_EN;
   wrmsrl(MSR_IA32_MCx_CTL2(bank), val);
  }
- raw_spin_unlock_irqrestore(&cmci_discover_lock, flags);
+ spin_unlock_irqrestore(&cmci_discover_lock, flags);
 }
 
 static bool cmci_storm_detect(void)
@@ -212,7 +212,7 @@ static void cmci_discover(int banks)
  int i;
  int bios_wrong_thresh = 0;
 
- raw_spin_lock_irqsave(&cmci_discover_lock, flags);
+ spin_lock_irqsave(&cmci_discover_lock, flags);
  for (i = 0; i < banks; i++) {
   u64 val;
   int bios_zero_thresh = 0;
@@ -267,7 +267,7 @@ static void cmci_discover(int banks)
    WARN_ON(!test_bit(i, __get_cpu_var(mce_poll_banks)));
   }
  }
- raw_spin_unlock_irqrestore(&cmci_discover_lock, flags);
+ spin_unlock_irqrestore(&cmci_discover_lock, flags);
  if (mca_cfg.bios_cmci_threshold && bios_wrong_thresh) {
   pr_info_once(
    "bios_cmci_threshold: Some banks do not have valid thresholds set\n");
@@ -317,10 +317,10 @@ void cmci_clear(void)
 
  if (!cmci_supported(&banks))
   return;
- raw_spin_lock_irqsave(&cmci_discover_lock, flags);
+ spin_lock_irqsave(&cmci_discover_lock, flags);
  for (i = 0; i < banks; i++)
   __cmci_disable_bank(i);
- raw_spin_unlock_irqrestore(&cmci_discover_lock, flags);
+ spin_unlock_irqrestore(&cmci_discover_lock, flags);
 }
 
 static void cmci_rediscover_work_func(void *arg)
@@ -361,9 +361,9 @@ void cmci_disable_bank(int bank)
  if (!cmci_supported(&banks))
   return;
 
- raw_spin_lock_irqsave(&cmci_discover_lock, flags);
+ spin_lock_irqsave(&cmci_discover_lock, flags);
  __cmci_disable_bank(bank);
- raw_spin_unlock_irqrestore(&cmci_discover_lock, flags);
+ spin_unlock_irqrestore(&cmci_discover_lock, flags);
 }
 
 static void intel_init_cmci(void)
-- 
1.7.1