From 2a55eef536e4b743771b14c10ba83daab84617bb Mon Sep 17 00:00:00 2001
From: David Gibson <dgibson@redhat.com>
Date: Mon, 15 Sep 2014 07:12:22 -0400
Subject: [virt] kvm/ppc: book3s/hv - Prefer CMA region for hash page table allocation

Message-id: <1410765214-16377-7-git-send-email-dgibson@redhat.com>
Patchwork-id: 94704
O-Subject: [PATCH 06/78] KVM: PPC: BOOK3S: HV: Prefer CMA region for hash page table allocation
Bugzilla: 1123145 1123133 1123367
RH-Acked-by: Radim Krcmar <rkrcmar@redhat.com>
RH-Acked-by: Don Zickus <dzickus@redhat.com>
RH-Acked-by: Steve Best <sbest@redhat.com>

https://bugzilla.redhat.com/show_bug.cgi?id=1123145

Signed-off-by: David Gibson <dgibson@redhat.com>

commit 792fc49787cb7df13f1c38d3e25c863e1c3a6bb2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue May 6 21:24:18 2014 +0530

    KVM: PPC: BOOK3S: HV: Prefer CMA region for hash page table allocation

    Today when KVM tries to reserve memory for the hash page table it
    allocates from the normal page allocator first. If that fails it
    falls back to CMA's reserved region. One of the side effects of
    this is that we could end up exhausting the page allocator and
    get linux into OOM conditions while we still have plenty of space
    available in CMA.

    This patch addresses this issue by first trying hash page table
    allocation from CMA's reserved region before falling back to the normal
    page allocator. So if we run out of memory, we really are out of memory.

    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c
index 508f57c..7e63139 100644
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@ -52,7 +52,7 @@ static void kvmppc_rmap_reset(struct kvm *kvm);
 
 long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)
 {
- unsigned long hpt;
+ unsigned long hpt = 0;
  struct revmap_entry *rev;
  struct page *page = NULL;
  long order = KVM_DEFAULT_HPT_ORDER;
@@ -64,22 +64,11 @@ long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)
  }
 
  kvm->arch.hpt_cma_alloc = 0;
- /*
-  * try first to allocate it from the kernel page allocator.
-  * We keep the CMA reserved for failed allocation.
-  */
- hpt = __get_free_pages(GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT |
-          __GFP_NOWARN, order - PAGE_SHIFT);
-
- /* Next try to allocate from the preallocated pool */
- if (!hpt) {
-  VM_BUG_ON(order < KVM_CMA_CHUNK_ORDER);
-  page = kvm_alloc_hpt(1 << (order - PAGE_SHIFT));
-  if (page) {
-   hpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));
-   kvm->arch.hpt_cma_alloc = 1;
-  } else
-   --order;
+ VM_BUG_ON(order < KVM_CMA_CHUNK_ORDER);
+ page = kvm_alloc_hpt(1 << (order - PAGE_SHIFT));
+ if (page) {
+  hpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));
+  kvm->arch.hpt_cma_alloc = 1;
  }
 
  /* Lastly try successively smaller sizes from the page allocator */
-- 
1.7.1