From 6069b06b55831407945f0bea036568c45e7722ef Mon Sep 17 00:00:00 2001
From: Jeff Moyer <jmoyer@redhat.com>
Date: Fri, 10 Oct 2014 21:03:20 -0400
Subject: [block] blk-mq: scale depth and rq map appropriate if low on memory

Message-id: <1412975015-5370-20-git-send-email-jmoyer@redhat.com>
Patchwork-id: 97454
O-Subject: [RHEL7 PATCH 19/34] blk-mq: scale depth and rq map appropriate if low on memory
Bugzilla: 1146660
RH-Acked-by: David Milburn <dmilburn@redhat.com>
RH-Acked-by: Mike Snitzer <snitzer@redhat.com>

This is a backport of the following commit.  This resolves bug
1146660.

  commit a516440542afcb9647f88d12c35640baf02d07ea
  Author: Jens Axboe <axboe@fb.com>
  Date:   Wed Sep 10 09:02:03 2014 -0600

    blk-mq: scale depth and rq map appropriate if low on memory

    If we are running in a kdump environment, resources are scarce.
    For some SCSI setups with a huge set of shared tags, we run out
    of memory allocating what the drivers is asking for. So implement
    a scale back logic to reduce the tag depth for those cases, allowing
    the driver to successfully load.

    We should extend this to detect low memory situations, and implement
    a sane fallback for those (1 queue, 64 tags, or something like that).

    Tested-by: Robert Elliott <elliott@hp.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

Signed-off-by: Jeff Moyer <jmoyer@redhat.com>
Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/block/blk-mq.c b/block/blk-mq.c
index 098c936..ff865b6 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -1324,6 +1324,7 @@ static void blk_mq_free_rq_map(struct blk_mq_tag_set *set,
     continue;
    set->ops->exit_request(set->driver_data, tags->rqs[i],
       hctx_idx, i);
+   tags->rqs[i] = NULL;
   }
  }
 
@@ -1357,8 +1358,9 @@ static struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,
 
  INIT_LIST_HEAD(&tags->page_list);
 
- tags->rqs = kmalloc_node(set->queue_depth * sizeof(struct request *),
-     GFP_KERNEL, set->numa_node);
+ tags->rqs = kzalloc_node(set->queue_depth * sizeof(struct request *),
+     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY,
+     set->numa_node);
  if (!tags->rqs) {
   blk_mq_free_tags(tags);
   return NULL;
@@ -1382,8 +1384,9 @@ static struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,
    this_order--;
 
   do {
-   page = alloc_pages_node(set->numa_node, GFP_KERNEL,
-      this_order);
+   page = alloc_pages_node(set->numa_node,
+    GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY,
+    this_order);
    if (page)
     break;
    if (!this_order--)
@@ -1407,8 +1410,10 @@ static struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,
    if (set->ops->init_request) {
     if (set->ops->init_request(set->driver_data,
       tags->rqs[i], hctx_idx, i,
-      set->numa_node))
+      set->numa_node)) {
+     tags->rqs[i] = NULL;
      goto fail;
+    }
    }
 
    p += rq_size;
@@ -1419,7 +1424,6 @@ static struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,
  return tags;
 
 fail:
- pr_warn("%s: failed to allocate requests\n", __func__);
  blk_mq_free_rq_map(set, tags, hctx_idx);
  return NULL;
 }
@@ -1939,6 +1943,61 @@ static int blk_mq_queue_reinit_notify(struct notifier_block *nb,
  return NOTIFY_OK;
 }
 
+static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
+{
+ int i;
+
+ for (i = 0; i < set->nr_hw_queues; i++) {
+  set->tags[i] = blk_mq_init_rq_map(set, i);
+  if (!set->tags[i])
+   goto out_unwind;
+ }
+
+ return 0;
+
+out_unwind:
+ while (--i >= 0)
+  blk_mq_free_rq_map(set, set->tags[i], i);
+
+ set->tags = NULL;
+ return -ENOMEM;
+}
+
+/*
+ * Allocate the request maps associated with this tag_set. Note that this
+ * may reduce the depth asked for, if memory is tight. set->queue_depth
+ * will be updated to reflect the allocated depth.
+ */
+static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
+{
+ unsigned int depth;
+ int err;
+
+ depth = set->queue_depth;
+ do {
+  err = __blk_mq_alloc_rq_maps(set);
+  if (!err)
+   break;
+
+  set->queue_depth >>= 1;
+  if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {
+   err = -ENOMEM;
+   break;
+  }
+ } while (set->queue_depth);
+
+ if (!set->queue_depth || err) {
+  pr_err("blk-mq: failed to allocate request map\n");
+  return -ENOMEM;
+ }
+
+ if (depth != set->queue_depth)
+  pr_info("blk-mq: reduced tag depth (%u -> %u)\n",
+      depth, set->queue_depth);
+
+ return 0;
+}
+
 /*
  * Alloc a tag set to be associated with one or more request queues.
  * May fail with EINVAL for various error conditions. May adjust the
@@ -1947,8 +2006,6 @@ static int blk_mq_queue_reinit_notify(struct notifier_block *nb,
  */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
- int i;
-
  if (!set->nr_hw_queues)
   return -EINVAL;
  if (!set->queue_depth)
@@ -1969,25 +2026,18 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
      sizeof(struct blk_mq_tags *),
      GFP_KERNEL, set->numa_node);
  if (!set->tags)
-  goto out;
+  return -ENOMEM;
 
- for (i = 0; i < set->nr_hw_queues; i++) {
-  set->tags[i] = blk_mq_init_rq_map(set, i);
-  if (!set->tags[i])
-   goto out_unwind;
- }
+ if (blk_mq_alloc_rq_maps(set))
+  goto enomem;
 
  mutex_init(&set->tag_list_lock);
  INIT_LIST_HEAD(&set->tag_list);
 
  return 0;
-
-out_unwind:
- while (--i >= 0)
-  blk_mq_free_rq_map(set, set->tags[i], i);
+enomem:
  kfree(set->tags);
  set->tags = NULL;
-out:
  return -ENOMEM;
 }
 EXPORT_SYMBOL(blk_mq_alloc_tag_set);
-- 
1.7.1