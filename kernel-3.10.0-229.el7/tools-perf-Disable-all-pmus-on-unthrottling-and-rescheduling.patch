From 5d0b5f0439d0153069b30b97fcf98c6025c924f8 Mon Sep 17 00:00:00 2001
From: Jiri Olsa <jolsa@redhat.com>
Date: Tue, 19 Aug 2014 15:23:17 -0400
Subject: [tools] perf: Disable all pmus on unthrottling and rescheduling

Message-id: <1408462094-14194-29-git-send-email-jolsa@redhat.com>
Patchwork-id: 88037
O-Subject: [PATCH RHEL7.1 BZ1131394 028/325] perf: Disable all pmus on unthrottling and rescheduling
Bugzilla: 1131394
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

From: Jiri Olsa <jolsa@kernel.org>

Bugzilla: 1131394
https://bugzilla.redhat.com/show_bug.cgi?id=1131394

upstream
========
commit 443772776c69ac9293d66b4d69fd9af16299cc2a
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date: Mon Dec 16 14:17:36 2013 +0200

description
===========
Currently, only one PMU in a context gets disabled during unthrottling
and event_sched_{out,in}(), however, events in one context may belong to
different pmus, which results in PMUs being reprogrammed while they are
still enabled.

This means that mixed PMU use [which is rare in itself] resulted in
potentially completely unreliable results: corrupted events, bogus
results, etc.

This patch temporarily disables PMUs that correspond to
each event in the context while these events are being modified.
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/kernel/events/core.c b/kernel/events/core.c
index 0da9f61..1026f3a 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -1393,6 +1393,8 @@ event_sched_out(struct perf_event *event,
  if (event->state != PERF_EVENT_STATE_ACTIVE)
   return;
 
+ perf_pmu_disable(event->pmu);
+
  event->state = PERF_EVENT_STATE_INACTIVE;
  if (event->pending_disable) {
   event->pending_disable = 0;
@@ -1409,6 +1411,8 @@ event_sched_out(struct perf_event *event,
   ctx->nr_freq--;
  if (event->attr.exclusive || !cpuctx->active_oncpu)
   cpuctx->exclusive = 0;
+
+ perf_pmu_enable(event->pmu);
 }
 
 static void
@@ -1649,6 +1653,7 @@ event_sched_in(struct perf_event *event,
    struct perf_event_context *ctx)
 {
  u64 tstamp = perf_event_time(event);
+ int ret = 0;
 
  if (event->state <= PERF_EVENT_STATE_OFF)
   return 0;
@@ -1671,10 +1676,13 @@ event_sched_in(struct perf_event *event,
   */
  smp_wmb();
 
+ perf_pmu_disable(event->pmu);
+
  if (event->pmu->add(event, PERF_EF_START)) {
   event->state = PERF_EVENT_STATE_INACTIVE;
   event->oncpu = -1;
-  return -EAGAIN;
+  ret = -EAGAIN;
+  goto out;
  }
 
  event->tstamp_running += tstamp - event->tstamp_stopped;
@@ -1690,7 +1698,10 @@ event_sched_in(struct perf_event *event,
  if (event->attr.exclusive)
   cpuctx->exclusive = 1;
 
- return 0;
+out:
+ perf_pmu_enable(event->pmu);
+
+ return ret;
 }
 
 static int
@@ -2740,6 +2751,8 @@ static void perf_adjust_freq_unthr_context(struct perf_event_context *ctx,
   if (!event_filter_match(event))
    continue;
 
+  perf_pmu_disable(event->pmu);
+
   hwc = &event->hw;
 
   if (hwc->interrupts == MAX_INTERRUPTS) {
@@ -2749,7 +2762,7 @@ static void perf_adjust_freq_unthr_context(struct perf_event_context *ctx,
   }
 
   if (!event->attr.freq || !event->attr.sample_freq)
-   continue;
+   goto next;
 
   /*
    * stop the event and update event->count
@@ -2771,6 +2784,8 @@ static void perf_adjust_freq_unthr_context(struct perf_event_context *ctx,
    perf_adjust_period(event, period, delta, false);
 
   event->pmu->start(event, delta > 0 ? PERF_EF_RELOAD : 0);
+ next:
+  perf_pmu_enable(event->pmu);
  }
 
  perf_pmu_enable(ctx->pmu);
-- 
1.7.1