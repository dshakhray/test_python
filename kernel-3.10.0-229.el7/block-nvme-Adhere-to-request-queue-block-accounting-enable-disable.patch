From 069a920084fc00ea445bb92e854a9f06f0bd6132 Mon Sep 17 00:00:00 2001
From: David Milburn <dmilburn@redhat.com>
Date: Mon, 18 Aug 2014 19:58:49 -0400
Subject: [block] nvme: Adhere to request queue block accounting enable/disable

Message-id: <1408391935-24886-17-git-send-email-dmilburn@redhat.com>
Patchwork-id: 87888
O-Subject: [RHEL7.1 PATCH BZ 1111259 16/22] NVMe: Adhere to request queue block accounting enable/disable
Bugzilla: 1111259
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

From: Sam Bradshaw <sbradshaw@micron.com>

Recently, a new sysfs control "iostats" was added to selectively
enable or disable io statistics collection for request queues.  This
patch hooks that control.

IO statistics collection is rather expensive on large, multi-node
machines with drives pushing millions of iops.  Having the ability to
disable collection if not needed can improve throughput significantly.

As a data point, on a quad E5-4640, I see more than 50% throughput
improvement when io statistics accounting is disabled during heavily
multi-threaded small block random read benchmarks where device
performance is in the million iops+ range.

Signed-off-by: Sam Bradshaw <sbradshaw@micron.com>
Signed-off-by: Matthew Wilcox <matthew.r.wilcox@intel.com>
(cherry picked from commit b4e75cbf1364c4bbce3599c3279892a55b6ede07)
Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/block/nvme-core.c b/drivers/block/nvme-core.c
index 3c94418..50baac9 100644
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@ -406,25 +406,30 @@ void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod)
 static void nvme_start_io_acct(struct bio *bio)
 {
  struct gendisk *disk = bio->bi_bdev->bd_disk;
- const int rw = bio_data_dir(bio);
- int cpu = part_stat_lock();
- part_round_stats(cpu, &disk->part0);
- part_stat_inc(cpu, &disk->part0, ios[rw]);
- part_stat_add(cpu, &disk->part0, sectors[rw], bio_sectors(bio));
- part_inc_in_flight(&disk->part0, rw);
- part_stat_unlock();
+ if (blk_queue_io_stat(disk->queue)) {
+  const int rw = bio_data_dir(bio);
+  int cpu = part_stat_lock();
+  part_round_stats(cpu, &disk->part0);
+  part_stat_inc(cpu, &disk->part0, ios[rw]);
+  part_stat_add(cpu, &disk->part0, sectors[rw],
+       bio_sectors(bio));
+  part_inc_in_flight(&disk->part0, rw);
+  part_stat_unlock();
+ }
 }
 
 static void nvme_end_io_acct(struct bio *bio, unsigned long start_time)
 {
  struct gendisk *disk = bio->bi_bdev->bd_disk;
- const int rw = bio_data_dir(bio);
- unsigned long duration = jiffies - start_time;
- int cpu = part_stat_lock();
- part_stat_add(cpu, &disk->part0, ticks[rw], duration);
- part_round_stats(cpu, &disk->part0);
- part_dec_in_flight(&disk->part0, rw);
- part_stat_unlock();
+ if (blk_queue_io_stat(disk->queue)) {
+  const int rw = bio_data_dir(bio);
+  unsigned long duration = jiffies - start_time;
+  int cpu = part_stat_lock();
+  part_stat_add(cpu, &disk->part0, ticks[rw], duration);
+  part_round_stats(cpu, &disk->part0);
+  part_dec_in_flight(&disk->part0, rw);
+  part_stat_unlock();
+ }
 }
 
 static void bio_completion(struct nvme_queue *nvmeq, void *ctx,
-- 
1.7.1