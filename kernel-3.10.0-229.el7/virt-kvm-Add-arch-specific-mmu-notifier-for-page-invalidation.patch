From 86d9491dbe801449dece0018d465bf26c364dd2c Mon Sep 17 00:00:00 2001
From: Paolo Bonzini <pbonzini@redhat.com>
Date: Mon, 10 Nov 2014 14:17:30 -0500
Subject: [virt] kvm: Add arch specific mmu notifier for page invalidation

Message-id: <1415629055-7922-5-git-send-email-pbonzini@redhat.com>
Patchwork-id: 99803
O-Subject: [RHEL7 PATCH v2 4/9] kvm: Add arch specific mmu notifier for page invalidation
Bugzilla: 1140974
RH-Acked-by: Marcelo Tosatti <mtosatti@redhat.com>
RH-Acked-by: Radim Krcmar <rkrcmar@redhat.com>
RH-Acked-by: Andrew Jones <drjones@redhat.com>

From: Tang Chen <tangchen@cn.fujitsu.com>

This will be used to let the guest run while the APIC access page is
not pinned.  Because subsequent patches will fill in the function
for x86, place the (still empty) x86 implementation in the x86.c file
instead of adding an inline function in kvm_host.h.

Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from upstream commit fe71557afbec641fee73711e40602bed37f6f33b)

Signed-off-by: Jarod Wilson <jarod@redhat.com>

Conflicts:
 arch/arm64/include/asm/kvm_host.h [not in RHEL7]
 arch/powerpc/include/asm/kvm_host.h [trivial]
 arch/x86/kvm/x86.c [trivial]

diff --git a/arch/arm/include/asm/kvm_host.h b/arch/arm/include/asm/kvm_host.h
index 57cb786..9ead7f5 100644
--- a/arch/arm/include/asm/kvm_host.h
+++ b/arch/arm/include/asm/kvm_host.h
@@ -178,6 +178,11 @@ static inline int kvm_test_age_hva(struct kvm *kvm, unsigned long hva)
  return 0;
 }
 
+static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
+        unsigned long address)
+{
+}
+
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu __percpu **kvm_get_running_vcpus(void);
 
diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
index 86dc975..c71536d 100644
--- a/arch/powerpc/include/asm/kvm_host.h
+++ b/arch/powerpc/include/asm/kvm_host.h
@@ -64,6 +64,11 @@ extern void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 
 #endif
 
+static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
+        unsigned long address)
+{
+}
+
 #define HPTEG_CACHE_NUM   (1 << 15)
 #define HPTEG_HASH_BITS_PTE  13
 #define HPTEG_HASH_BITS_PTE_LONG 12
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c2537e6..b197a6c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1043,6 +1043,8 @@ int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu);
+void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
+        unsigned long address);
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
 void kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 918be1b..2a46c38 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5937,6 +5937,11 @@ static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
  kvm_apic_update_tmr(vcpu, tmr);
 }
 
+void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
+        unsigned long address)
+{
+}
+
 /*
  * Returns 1 to let __vcpu_run() continue the guest execution loop without
  * exiting to the userspace.  Otherwise, the value will be returned to the
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 4ea5263..d11a9e5 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -295,6 +295,9 @@ static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,
   kvm_flush_remote_tlbs(kvm);
 
  spin_unlock(&kvm->mmu_lock);
+
+ kvm_arch_mmu_notifier_invalidate_page(kvm, address);
+
  srcu_read_unlock(&kvm->srcu, idx);
 }
 
-- 
1.7.1