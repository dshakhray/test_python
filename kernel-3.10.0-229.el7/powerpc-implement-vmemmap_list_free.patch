From 5d5649c96d3a2b9f65534d7897929f1633a92e6c Mon Sep 17 00:00:00 2001
From: Gustavo Duarte <gduarte@redhat.com>
Date: Tue, 2 Sep 2014 21:09:16 -0400
Subject: [powerpc] implement vmemmap_list_free()

Message-id: <1409692159-32351-2-git-send-email-gduarte@redhat.com>
Patchwork-id: 90689
O-Subject: [RHEL7.1 PATCH BZ 1090174 1/4] powerpc: implement vmemmap_list_free()
Bugzilla: 1090174
RH-Acked-by: Stefan Assmann <sassmann@redhat.com>
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>

Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1090174
Upstream Status: bd8cb03dbe77a529945aa270a18c1ba074f729c6

commit bd8cb03dbe77a529945aa270a18c1ba074f729c6
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Wed Jun 11 16:23:36 2014 +0800

    powerpc: implement vmemmap_list_free()

    This patch implements vmemmap_list_free() for vmemmap_free().

    The freed entries will be removed from vmemmap_list, and form a freed list,
    with next as the header. The next position in the last allocated page is kept
    at the list tail.

    When allocation, if there are freed entries left, get it from the freed list;
    if no freed entries left, get it like before from the last allocated pages.

    With this change, realmode_pfn_to_page() also needs to be changed to walk
    all the entries in the vmemmap_list, as the virt_addr of the entries might not
    be stored in order anymore.

    It helps to reuse the memory when continuous doing memory hot-plug/remove
    operations, but didn't reclaim the pages already allocated, so the memory usage
    will only increase, but won't exceed the value for the largest memory
    configuration.

    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Acked-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/powerpc/mm/init_64.c b/arch/powerpc/mm/init_64.c
index 8ed035d..35731fe 100644
--- a/arch/powerpc/mm/init_64.c
+++ b/arch/powerpc/mm/init_64.c
@@ -226,14 +226,24 @@ static void __meminit vmemmap_create_mapping(unsigned long start,
 #endif /* CONFIG_PPC_BOOK3E */
 
 struct vmemmap_backing *vmemmap_list;
+static struct vmemmap_backing *next;
+static int num_left;
+static int num_freed;
 
 static __meminit struct vmemmap_backing * vmemmap_list_alloc(int node)
 {
- static struct vmemmap_backing *next;
- static int num_left;
+ struct vmemmap_backing *vmem_back;
+ /* get from freed entries first */
+ if (num_freed) {
+  num_freed--;
+  vmem_back = next;
+  next = next->list;
+
+  return vmem_back;
+ }
 
  /* allocate a page when required and hand out chunks */
- if (!next || !num_left) {
+ if (!num_left) {
   next = vmemmap_alloc_block(PAGE_SIZE, node);
   if (unlikely(!next)) {
    WARN_ON(1);
@@ -266,6 +276,38 @@ static __meminit void vmemmap_list_populate(unsigned long phys,
  vmemmap_list = vmem_back;
 }
 
+static unsigned long vmemmap_list_free(unsigned long start)
+{
+ struct vmemmap_backing *vmem_back, *vmem_back_prev;
+
+ vmem_back_prev = vmem_back = vmemmap_list;
+
+ /* look for it with prev pointer recorded */
+ for (; vmem_back; vmem_back = vmem_back->list) {
+  if (vmem_back->virt_addr == start)
+   break;
+  vmem_back_prev = vmem_back;
+ }
+
+ if (unlikely(!vmem_back)) {
+  WARN_ON(1);
+  return 0;
+ }
+
+ /* remove it from vmemmap_list */
+ if (vmem_back == vmemmap_list) /* remove head */
+  vmemmap_list = vmem_back->list;
+ else
+  vmem_back_prev->list = vmem_back->list;
+
+ /* next point to this freed entry */
+ vmem_back->list = next;
+ next = vmem_back;
+ num_freed++;
+
+ return vmem_back->phys;
+}
+
 int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)
 {
  unsigned long page_size = 1 << mmu_psize_defs[mmu_vmemmap_psize].shift;
-- 
1.7.1