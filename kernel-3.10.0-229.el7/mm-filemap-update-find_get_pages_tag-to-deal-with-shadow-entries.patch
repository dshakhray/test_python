From 585508cdda11bb614243ad864a42195850bfb679 Mon Sep 17 00:00:00 2001
From: Johannes Weiner <jweiner@redhat.com>
Date: Thu, 1 May 2014 14:49:38 -0400
Subject: [mm] filemap: update find_get_pages_tag() to deal with shadow entries

Message-id: <1398955778-27146-1-git-send-email-jweiner@redhat.com>
Patchwork-id: 79278
O-Subject: [PATCH RHEL7 BZ1091795] mm: filemap: update find_get_pages_tag() to deal with shadow entries
Bugzilla: 1091795
RH-Acked-by: Larry Woodman <lwoodman@redhat.com>
RH-Acked-by: Rik van Riel <riel@redhat.com>

commit: in -mm
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Apr 24 21:02:53 2014 +0000

    mm: filemap: update find_get_pages_tag() to deal with shadow entries

    Dave Jones reports the following crash when find_get_pages_tag() runs into
    an exceptional entry:

    kernel BUG at mm/filemap.c:1347!
    RIP: 0010:[<ffffffffb815aeab>]  [<ffffffffb815aeab>] find_get_pages_tag+0x1cb/0x220
    Call Trace:
     [<ffffffffb815ad16>] ? find_get_pages_tag+0x36/0x220
     [<ffffffffb8168511>] pagevec_lookup_tag+0x21/0x30
     [<ffffffffb81595de>] filemap_fdatawait_range+0xbe/0x1e0
     [<ffffffffb8159727>] filemap_fdatawait+0x27/0x30
     [<ffffffffb81f2fa4>] sync_inodes_sb+0x204/0x2a0
     [<ffffffffb874d98f>] ? wait_for_completion+0xff/0x130
     [<ffffffffb81fa5b0>] ? vfs_fsync+0x40/0x40
     [<ffffffffb81fa5c9>] sync_inodes_one_sb+0x19/0x20
     [<ffffffffb81caab2>] iterate_supers+0xb2/0x110
     [<ffffffffb81fa864>] sys_sync+0x44/0xb0
     [<ffffffffb875c4a9>] ia32_do_call+0x13/0x13

    1343                         /*
    1344                          * This function is never used on a shmem/tmpfs
    1345                          * mapping, so a swap entry won't be found here.
    1346                          */
    1347                         BUG();

    After 0cd6144aadd2 ("mm + fs: prepare for non-page entries in page cache
    radix trees") this comment and BUG() are out of date because exceptional
    entries can now appear in all mappings - as shadows of recently evicted
    pages.

    However, as Hugh Dickins notes,

      "it is truly surprising for a PAGECACHE_TAG_WRITEBACK (and probably
       any other PAGECACHE_TAG_*) to appear on an exceptional entry.

       I expect it comes down to an occasional race in RCU lookup of the
       radix_tree: lacking absolute synchronization, we might sometimes
       catch an exceptional entry, with the tag which really belongs with
       the unexceptional entry which was there an instant before."

    And indeed, not only is the tree walk lockless, the tags are also read in
    chunks, one radix tree node at a time.  There is plenty of time for page
    reclaim to swoop in and replace a page that was already looked up as
    tagged with a shadow entry.

    Remove the BUG() and update the comment.  While reviewing all other lookup
    sites for whether they properly deal with shadow entries of evicted pages,
    update all the comments and fix memcg file charge moving to not miss
    shmem/tmpfs swapcache pages.

    Fixes: 0cd6144aadd2 ("mm + fs: prepare for non-page entries in page cache radix trees")
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: Dave Jones <davej@redhat.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

Signed-off-by: Johannes Weiner <jweiner@redhat.com>
Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/mm/filemap.c b/mm/filemap.c
index 33fdcc2..7bf72f0 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -902,8 +902,8 @@ EXPORT_SYMBOL(page_cache_prev_hole);
  * Looks up the page cache slot at @mapping & @offset.  If there is a
  * page cache page, it is returned with an increased refcount.
  *
- * If the slot holds a shadow entry of a previously evicted page, it
- * is returned.
+ * If the slot holds a shadow entry of a previously evicted page, or a
+ * swap entry from shmem/tmpfs, it is returned.
  *
  * Otherwise, %NULL is returned.
  */
@@ -924,9 +924,9 @@ repeat:
    if (radix_tree_deref_retry(page))
     goto repeat;
    /*
-    * Otherwise, shmem/tmpfs must be storing a swap entry
-    * here as an exceptional entry: so return it without
-    * attempting to raise page count.
+    * A shadow entry of a recently evicted page,
+    * or a swap entry from shmem/tmpfs.  Return
+    * it without attempting to raise page count.
     */
    goto out;
   }
@@ -979,8 +979,8 @@ EXPORT_SYMBOL(find_get_page);
  * page cache page, it is returned locked and with an increased
  * refcount.
  *
- * If the slot holds a shadow entry of a previously evicted page, it
- * is returned.
+ * If the slot holds a shadow entry of a previously evicted page, or a
+ * swap entry from shmem/tmpfs, it is returned.
  *
  * Otherwise, %NULL is returned.
  *
@@ -1092,8 +1092,8 @@ EXPORT_SYMBOL(find_or_create_page);
  * The search returns a group of mapping-contiguous pages with ascending
  * indexes.  There may be holes in the indices due to not-present pages.
  *
- * Any shadow entries of evicted pages are included in the returned
- * array.
+ * Any shadow entries of evicted pages, or swap entries from
+ * shmem/tmpfs, are included in the returned array.
  *
  * __find_get_pages() returns the number of pages and shadow entries
  * which were found.
@@ -1121,9 +1121,9 @@ repeat:
    if (radix_tree_deref_retry(page))
     goto restart;
    /*
-    * Otherwise, we must be storing a swap entry
-    * here as an exceptional entry: so return it
-    * without attempting to raise page count.
+    * A shadow entry of a recently evicted page,
+    * or a swap entry from shmem/tmpfs.  Return
+    * it without attempting to raise page count.
     */
    goto export;
   }
@@ -1191,9 +1191,9 @@ repeat:
     goto restart;
    }
    /*
-    * Otherwise, shmem/tmpfs must be storing a swap entry
-    * here as an exceptional entry: so skip over it -
-    * we only reach this from invalidate_mapping_pages().
+    * A shadow entry of a recently evicted page,
+    * or a swap entry from shmem/tmpfs.  Skip
+    * over it.
     */
    continue;
   }
@@ -1258,9 +1258,9 @@ repeat:
     goto restart;
    }
    /*
-    * Otherwise, shmem/tmpfs must be storing a swap entry
-    * here as an exceptional entry: so stop looking for
-    * contiguous pages.
+    * A shadow entry of a recently evicted page,
+    * or a swap entry from shmem/tmpfs.  Stop
+    * looking for contiguous pages.
     */
    break;
   }
@@ -1334,10 +1334,17 @@ repeat:
     goto restart;
    }
    /*
-    * This function is never used on a shmem/tmpfs
-    * mapping, so a swap entry won't be found here.
+    * A shadow entry of a recently evicted page.
+    *
+    * Those entries should never be tagged, but
+    * this tree walk is lockless and the tags are
+    * looked up in bulk, one radix tree node at a
+    * time, so there is a sizable window for page
+    * reclaim to evict a page we saw tagged.
+    *
+    * Skip over it.
     */
-   BUG();
+   continue;
   }
 
   if (!page_cache_get_speculative(page))
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 15b0a9c..e175e01 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -6490,16 +6490,20 @@ static struct page *mc_handle_file_pte(struct vm_area_struct *vma,
   pgoff = pte_to_pgoff(ptent);
 
  /* page is moved even if it's not RSS of this task(page-faulted). */
- page = find_get_page(mapping, pgoff);
-
 #ifdef CONFIG_SWAP
  /* shmem/tmpfs may report page out on swap: account for that too. */
- if (radix_tree_exceptional_entry(page)) {
-  swp_entry_t swap = radix_to_swp_entry(page);
-  if (do_swap_account)
-   *entry = swap;
-  page = find_get_page(swap_address_space(swap), swap.val);
- }
+ if (shmem_mapping(mapping)) {
+  page = __find_get_page(mapping, pgoff);
+  if (radix_tree_exceptional_entry(page)) {
+   swp_entry_t swp = radix_to_swp_entry(page);
+   if (do_swap_account)
+    *entry = swp;
+   page = find_get_page(swap_address_space(swp), swp.val);
+  }
+ } else
+  page = find_get_page(mapping, pgoff);
+#else
+ page = find_get_page(mapping, pgoff);
 #endif
  return page;
 }
diff --git a/mm/truncate.c b/mm/truncate.c
index a292c23..bb5271c 100644
--- a/mm/truncate.c
+++ b/mm/truncate.c
@@ -437,14 +437,6 @@ unsigned long invalidate_mapping_pages(struct address_space *mapping,
  unsigned long count = 0;
  int i;
 
- /*
-  * Note: this function may get called on a shmem/tmpfs mapping:
-  * pagevec_lookup() might then return 0 prematurely (because it
-  * got a gangful of swap entries); but it's hardly worth worrying
-  * about - it can rarely have anything to free from such a mapping
-  * (most pages are dirty), and already skips over any difficulties.
-  */
-
  pagevec_init(&pvec, 0);
  while (index <= end && __pagevec_lookup(&pvec, mapping, index,
    min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,
-- 
1.7.1