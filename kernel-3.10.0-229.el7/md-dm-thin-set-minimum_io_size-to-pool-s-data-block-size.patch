From 1e18a7d97246d0b87edcfd83ebaa39bf85b0a098 Mon Sep 17 00:00:00 2001
From: Mike Snitzer <snitzer@redhat.com>
Date: Mon, 21 Jul 2014 13:17:15 -0400
Subject: [md] dm-thin: set minimum_io_size to pool's data block size

Message-id: <20140721131714.GA28259@redhat.com>
Patchwork-id: 86052
O-Subject: [RHEL7.1 PATCH 41/40] dm thin: set minimum_io_size to pool's data block size
Bugzilla: 1117872
RH-Acked-by: Heinz Mauelshagen <heinzm@redhat.com>
RH-Acked-by: Joe Thornber <thornber@redhat.com>

BZ: 1117872

Upstream linux-dm.git commit 8c8ac06bd9643a86ea32f8ec550b25bf14746d27
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Jul 18 17:59:43 2014 -0400

    dm thin: set minimum_io_size to pool's data block size

    Before, if the block layer's limit stacking didn't establish an
    optimal_io_size that was compatible with the thin-pool's data block size
    we'd set optimal_io_size to the data block size and minimum_io_size to 0
    (which the block layer adjusts to be physical_block_size).

    Update pool_io_hints() to set both minimum_io_size and optimal_io_size
    to the thin-pool's data block size.  This fixes an issue reported where
    mkfs.xfs would create more XFS Allocation Groups on thinp volumes than
    on a normal linear LV of comparable size, see:
    https://bugzilla.redhat.com/show_bug.cgi?id=1003227

    Reported-by: Chris Murphy <lists@colorremedies.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/drivers/md/dm-thin.c b/drivers/md/dm-thin.c
index 3a1f923..cbfd35e 100644
--- a/drivers/md/dm-thin.c
+++ b/drivers/md/dm-thin.c
@@ -3171,7 +3171,7 @@ static void pool_io_hints(struct dm_target *ti, struct queue_limits *limits)
   */
  if (io_opt_sectors < pool->sectors_per_block ||
      do_div(io_opt_sectors, pool->sectors_per_block)) {
-  blk_limits_io_min(limits, 0);
+  blk_limits_io_min(limits, pool->sectors_per_block << SECTOR_SHIFT);
   blk_limits_io_opt(limits, pool->sectors_per_block << SECTOR_SHIFT);
  }
 
-- 
1.7.1