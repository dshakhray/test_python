From a7d6a094e45503ba75ae722060a96dbc73b71e72 Mon Sep 17 00:00:00 2001
From: Paolo Bonzini <pbonzini@redhat.com>
Date: Tue, 22 Jul 2014 14:40:09 -0400
Subject: [virt] kvm/x86: Check for nested events if there is an injectable interrupt

Message-id: <1406040016-3289-215-git-send-email-pbonzini@redhat.com>
Patchwork-id: 86293
O-Subject: [RHEL7 PATCH v2 214/221] KVM: x86: Check for nested events if there is an injectable interrupt
Bugzilla: 1116936
RH-Acked-by: Radim Krcmar <rkrcmar@redhat.com>
RH-Acked-by: Bandan Das <bsd@redhat.com>
RH-Acked-by: Tony Camuso <tcamuso@redhat.com>

From: Bandan Das <bsd@redhat.com>

With commit b6b8a1451fc40412c57d1 that introduced
vmx_check_nested_events, checks for injectable interrupts happen
at different points in time for L1 and L2 that could potentially
cause a race. The regression occurs because KVM_REQ_EVENT is always
set when nested_run_pending is set even if there's no pending interrupt.
Consequently, there could be a small window when check_nested_events
returns without exiting to L1, but an interrupt comes through soon
after and it incorrectly, gets injected to L2 by inject_pending_event
Fix this by adding a call to check for nested events too when a check
for injectable interrupt returns true

Signed-off-by: Bandan Das <bsd@redhat.com>
Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 9242b5b60df8b13b469bc6b7be08ff6ebb551ad3)
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 454f96d..660a25c 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5878,6 +5878,19 @@ static int inject_pending_event(struct kvm_vcpu *vcpu, bool req_int_win)
    kvm_x86_ops->set_nmi(vcpu);
   }
  } else if (kvm_cpu_has_injectable_intr(vcpu)) {
+  /*
+   * TODO/FIXME: We are calling check_nested_events again
+   * here to avoid a race condition. We should really be
+   * setting KVM_REQ_EVENT only on certain events
+   * and not unconditionally.
+   * See https://lkml.org/lkml/2014/7/2/60 for discussion
+   * about this proposal and current concerns
+   */
+  if (is_guest_mode(vcpu) && kvm_x86_ops->check_nested_events) {
+   r = kvm_x86_ops->check_nested_events(vcpu, req_int_win);
+   if (r != 0)
+    return r;
+  }
   if (kvm_x86_ops->interrupt_allowed(vcpu)) {
    kvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu),
          false);
-- 
1.7.1