From 9b839deffaf77dc93298d62dffb7820a301a0de4 Mon Sep 17 00:00:00 2001
From: Mike Snitzer <snitzer@redhat.com>
Date: Fri, 13 Jun 2014 14:36:29 -0400
Subject: [block] blk-mq: move blk_mq_get_ctx_blk_mq_put_ctx to mq private header

Message-id: <1402670238-13916-102-git-send-email-snitzer@redhat.com>
Patchwork-id: 83825
O-Subject: [RHEL7.1 PATCH 101/150] blk-mq: move blk_mq_get_ctx_blk_mq_put_ctx to mq private header
Bugzilla: 1105204
RH-Acked-by: David Milburn <dmilburn@redhat.com>
RH-Acked-by: Jeff Moyer <jmoyer@redhat.com>

BZ: 1105204

Upstream commit 1aecfe4887713838c79bc52f774609a57db4f988
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Sun Jun 1 00:43:36 2014 +0800

    blk-mq: move blk_mq_get_ctx/blk_mq_put_ctx to mq private header

    The blk-mq tag code need these helpers.

    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>
---

Signed-off-by: Jarod Wilson <jarod@redhat.com>

diff --git a/block/blk-mq.c b/block/blk-mq.c
index 0f5879c..b9230c5 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -33,28 +33,6 @@ static LIST_HEAD(all_q_list);
 
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx);
 
-static struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
-        unsigned int cpu)
-{
- return per_cpu_ptr(q->queue_ctx, cpu);
-}
-
-/*
- * This assumes per-cpu software queueing queues. They could be per-node
- * as well, for instance. For now this is hardcoded as-is. Note that we don't
- * care about preemption, since we know the ctx's are persistent. This does
- * mean that we can't rely on ctx always matching the currently running CPU.
- */
-static struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
-{
- return __blk_mq_get_ctx(q, get_cpu());
-}
-
-static void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
-{
- put_cpu();
-}
-
 /*
  * Check if any of the ctx's have pending work in this hardware queue
  */
diff --git a/block/blk-mq.h b/block/blk-mq.h
index de7b3bb..57a7968 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -69,4 +69,26 @@ struct blk_align_bitmap {
  unsigned long depth;
 } ____cacheline_aligned_in_smp;
 
+static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
+        unsigned int cpu)
+{
+ return per_cpu_ptr(q->queue_ctx, cpu);
+}
+
+/*
+ * This assumes per-cpu software queueing queues. They could be per-node
+ * as well, for instance. For now this is hardcoded as-is. Note that we don't
+ * care about preemption, since we know the ctx's are persistent. This does
+ * mean that we can't rely on ctx always matching the currently running CPU.
+ */
+static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
+{
+ return __blk_mq_get_ctx(q, get_cpu());
+}
+
+static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
+{
+ put_cpu();
+}
+
 #endif
-- 
1.7.1